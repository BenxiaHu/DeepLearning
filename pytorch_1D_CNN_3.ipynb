{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5dOhfWPz0m2cNg0pL7W5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "a3706f61-8325-4f12-e669-68b4042fe68c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f87a00a1e50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "#print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "#print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "#print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "#print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "#print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 16\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=4, out_channels=64, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=64),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=128, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=128),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=128, out_channels=256, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=256),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.fc1 = nn.Linear(256*25, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc3(x)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "LR = 0.00001 \n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 3000              # train the training data n times, to save time, we just train 1 epoch\n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    j=0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        j+=1\n",
        "\n",
        "    print(f'the loss on each epoch: {epoch}, loss: {running_loss/j}')\n",
        "    train_losses.append(running_loss/j)\n",
        "print('Finished Training')\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Letâ€™s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zckte_qdea_1",
        "outputId": "01f03935-b5b8-4332-8b1f-2d8ac28f026d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n",
            "torch.Size([16, 64, 100])\n",
            "torch.Size([16, 128, 50])\n",
            "torch.Size([16, 256, 25])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-942f2d2b7267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(outputs,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test : {100 * correct // total} %')\n"
      ],
      "metadata": {
        "id": "XjjcoqJLoqDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}