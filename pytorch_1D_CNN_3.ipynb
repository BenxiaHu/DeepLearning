{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEZPBO+UY+F58k4MJPplXH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "354c9648-b114-414d-dbb3-c7316d0b51a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5cfc0974f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "#print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "#print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "#print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "#print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "#print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "test_x2 = Variable(x_test[:200,:,:]).type(torch.FloatTensor)\n",
        "test_y2 = y_test[:200]\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 16\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b69da99-5f24-4e39-bce3-a478498f5fd2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
              "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
              "        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
              "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
              "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
              "        1., 0.], device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#step3: define residual layer"
      ],
      "metadata": {
        "id": "ZZ-ZFQYdK7em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "aftZA2qSK6Uc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step3: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=4, out_channels=64, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=64),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.residual1 = ResidualBlock(in_channels=64, out_channels=64)\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=128, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=128),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.residual2 = ResidualBlock(in_channels=128, out_channels=128)\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=128, out_channels=256, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=256),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.residual3 = ResidualBlock(in_channels=256, out_channels=256)\n",
        "        self.fc1 = nn.Linear(256*25, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.residual1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.residual2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.residual3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc3(x)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "LR = 0.000001 \n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 1000              # train the training data n times, to save time, we just train 1 epoch\n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "validation_loss = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    j=0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        j+=1\n",
        "        with torch.no_grad():\n",
        "            val_outputs = cnn(test_x2)\n",
        "            test_y2 = test_y2.type((torch.LongTensor))\n",
        "            val_loss = loss_func(val_outputs, test_y2)\n",
        "    validation_loss.append(val_loss.item())\n",
        "    train_losses.append(running_loss/j)\n",
        "    if(epoch % 10 == 0):\n",
        "        print(f'the loss on each epoch: {epoch}, loss: {running_loss/j}')\n",
        "        print(f'the loss on each epoch: {epoch}, loss: {val_loss.item()}')\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Letâ€™s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(validation_loss, label = \"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"silencers\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zckte_qdea_1",
        "outputId": "be7ab6bb-5182-424a-909a-04ceef252229"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step5: apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"silencers\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(outputs,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test : {100 * correct // total} %')\n"
      ],
      "metadata": {
        "id": "XjjcoqJLoqDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}