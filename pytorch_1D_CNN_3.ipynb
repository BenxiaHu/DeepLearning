{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPGH1dXCJUOjjPu81CYtRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "465cba45-b007-4147-ab1c-9a169268786a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7d1c2a1c30>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "#print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "#print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "#print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "#print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "#print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 8\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=4, out_channels=64, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=64),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=128, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=128),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=128, out_channels=256, \n",
        "                                             kernel_size=3, padding=1),\n",
        "                     nn.BatchNorm1d(num_features=256),\n",
        "                     nn.ReLU(),\n",
        "                     nn.MaxPool1d(kernel_size=2))\n",
        "        self.fc1 = nn.Linear(256*25, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc3(x)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "LR = 0.000001 \n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 5000              # train the training data n times, to save time, we just train 1 epoch\n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    j=0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        j+=1\n",
        "    train_losses.append(running_loss/j)\n",
        "    if(epoch % 10 == 0):\n",
        "        print(f'the loss on each epoch: {epoch}, loss: {running_loss/j}')\n",
        "\n",
        "print('Finished Training')\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Let’s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zckte_qdea_1",
        "outputId": "a4f446d3-89ca-4cf0-f36f-6a2ccc2aed13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the loss on each epoch: 0, loss: 8.323020801544189\n",
            "the loss on each epoch: 10, loss: 8.280107321058\n",
            "the loss on each epoch: 20, loss: 8.221221551213945\n",
            "the loss on each epoch: 30, loss: 8.151440240996225\n",
            "the loss on each epoch: 40, loss: 8.122501395770481\n",
            "the loss on each epoch: 50, loss: 8.037294837406703\n",
            "the loss on each epoch: 60, loss: 7.968091628210885\n",
            "the loss on each epoch: 70, loss: 7.929764902251107\n",
            "the loss on each epoch: 80, loss: 7.879478244100298\n",
            "the loss on each epoch: 90, loss: 7.814994428498404\n",
            "the loss on each epoch: 100, loss: 7.788275049754551\n",
            "the loss on each epoch: 110, loss: 7.715059364182609\n",
            "the loss on each epoch: 120, loss: 7.67434613432203\n",
            "the loss on each epoch: 130, loss: 7.635455562046596\n",
            "the loss on each epoch: 140, loss: 7.568572897229876\n",
            "the loss on each epoch: 150, loss: 7.481878506796701\n",
            "the loss on each epoch: 160, loss: 7.398678842272077\n",
            "the loss on each epoch: 170, loss: 7.3885639497212\n",
            "the loss on each epoch: 180, loss: 7.3096476415225435\n",
            "the loss on each epoch: 190, loss: 7.26543027639389\n",
            "the loss on each epoch: 200, loss: 7.158427594729832\n",
            "the loss on each epoch: 210, loss: 7.117898908002036\n",
            "the loss on each epoch: 220, loss: 7.016817284311567\n",
            "the loss on each epoch: 230, loss: 6.963011486189706\n",
            "the loss on each epoch: 240, loss: 6.924098935127258\n",
            "the loss on each epoch: 250, loss: 6.8783434956414355\n",
            "the loss on each epoch: 260, loss: 6.790764239515577\n",
            "the loss on each epoch: 270, loss: 6.786430997167315\n",
            "the loss on each epoch: 280, loss: 6.617073038986751\n",
            "the loss on each epoch: 290, loss: 6.682477390084948\n",
            "the loss on each epoch: 300, loss: 6.528845058338983\n",
            "the loss on each epoch: 310, loss: 6.500715777873993\n",
            "the loss on each epoch: 320, loss: 6.4533069712775095\n",
            "the loss on each epoch: 330, loss: 6.438191945212228\n",
            "the loss on each epoch: 340, loss: 6.421488384349005\n",
            "the loss on each epoch: 350, loss: 6.348851931776319\n",
            "the loss on each epoch: 360, loss: 6.287112512929099\n",
            "the loss on each epoch: 370, loss: 6.287678676503045\n",
            "the loss on each epoch: 380, loss: 6.299090922900609\n",
            "the loss on each epoch: 390, loss: 6.204313452584403\n",
            "the loss on each epoch: 400, loss: 6.1904614322526115\n",
            "the loss on each epoch: 410, loss: 6.185695627076285\n",
            "the loss on each epoch: 420, loss: 6.186680142368589\n",
            "the loss on each epoch: 430, loss: 6.123041753002576\n",
            "the loss on each epoch: 440, loss: 6.095510164414133\n",
            "the loss on each epoch: 450, loss: 6.110387060514518\n",
            "the loss on each epoch: 460, loss: 6.0600806386130195\n",
            "the loss on each epoch: 470, loss: 6.089941984159606\n",
            "the loss on each epoch: 480, loss: 6.023282890149526\n",
            "the loss on each epoch: 490, loss: 5.972501734154565\n",
            "the loss on each epoch: 500, loss: 5.983116128189224\n",
            "the loss on each epoch: 510, loss: 6.008046739527157\n",
            "the loss on each epoch: 520, loss: 6.027352172732353\n",
            "the loss on each epoch: 530, loss: 5.9976636534929275\n",
            "the loss on each epoch: 540, loss: 5.932733790874481\n",
            "the loss on each epoch: 550, loss: 5.985214049518109\n",
            "the loss on each epoch: 560, loss: 5.941807320075376\n",
            "the loss on each epoch: 570, loss: 5.927603741671358\n",
            "the loss on each epoch: 580, loss: 5.897672795653343\n",
            "the loss on each epoch: 590, loss: 5.9535491945488115\n",
            "the loss on each epoch: 600, loss: 5.914452707788774\n",
            "the loss on each epoch: 610, loss: 5.923588068527835\n",
            "the loss on each epoch: 620, loss: 5.884980318801744\n",
            "the loss on each epoch: 630, loss: 5.896871595893588\n",
            "the loss on each epoch: 640, loss: 5.942612080659186\n",
            "the loss on each epoch: 650, loss: 5.894169419556857\n",
            "the loss on each epoch: 660, loss: 5.90048854259508\n",
            "the loss on each epoch: 670, loss: 5.956371290598597\n",
            "the loss on each epoch: 680, loss: 5.858546561013375\n",
            "the loss on each epoch: 690, loss: 5.848956087529659\n",
            "the loss on each epoch: 700, loss: 5.832921498811671\n",
            "the loss on each epoch: 710, loss: 5.8359797562126605\n",
            "the loss on each epoch: 720, loss: 5.802550756218178\n",
            "the loss on each epoch: 730, loss: 5.890310315718608\n",
            "the loss on each epoch: 740, loss: 5.847012756701027\n",
            "the loss on each epoch: 750, loss: 5.856780931896397\n",
            "the loss on each epoch: 760, loss: 5.813897374847105\n",
            "the loss on each epoch: 770, loss: 5.848546634529318\n",
            "the loss on each epoch: 780, loss: 5.828803184181452\n",
            "the loss on each epoch: 790, loss: 5.818605131059885\n",
            "the loss on each epoch: 800, loss: 5.837388130511556\n",
            "the loss on each epoch: 810, loss: 5.813787405480231\n",
            "the loss on each epoch: 820, loss: 5.796560454357947\n",
            "the loss on each epoch: 830, loss: 5.873202288326408\n",
            "the loss on each epoch: 840, loss: 5.811972211961235\n",
            "the loss on each epoch: 850, loss: 5.803118502843593\n",
            "the loss on each epoch: 860, loss: 5.785143214120929\n",
            "the loss on each epoch: 870, loss: 5.857055224566055\n",
            "the loss on each epoch: 880, loss: 5.835508133965943\n",
            "the loss on each epoch: 890, loss: 5.816463783175817\n",
            "the loss on each epoch: 900, loss: 5.799897800769124\n",
            "the loss on each epoch: 910, loss: 5.807008182942601\n",
            "the loss on each epoch: 920, loss: 5.784597496282576\n",
            "the loss on each epoch: 930, loss: 5.779632200657257\n",
            "the loss on each epoch: 940, loss: 5.804347116917904\n",
            "the loss on each epoch: 950, loss: 5.787166384628841\n",
            "the loss on each epoch: 960, loss: 5.775808303910972\n",
            "the loss on each epoch: 970, loss: 5.7699526616452\n",
            "the loss on each epoch: 980, loss: 5.763512637993054\n",
            "the loss on each epoch: 990, loss: 5.806804008392085\n",
            "the loss on each epoch: 1000, loss: 5.810960358594145\n",
            "the loss on each epoch: 1010, loss: 5.787077119539359\n",
            "the loss on each epoch: 1020, loss: 5.881092357803136\n",
            "the loss on each epoch: 1030, loss: 5.778916058916865\n",
            "the loss on each epoch: 1040, loss: 5.794605369305771\n",
            "the loss on each epoch: 1050, loss: 5.813451378481196\n",
            "the loss on each epoch: 1060, loss: 5.770465808465545\n",
            "the loss on each epoch: 1070, loss: 5.7949196231637945\n",
            "the loss on each epoch: 1080, loss: 5.793162008477375\n",
            "the loss on each epoch: 1090, loss: 5.736545421342099\n",
            "the loss on each epoch: 1100, loss: 5.80965351593082\n",
            "the loss on each epoch: 1110, loss: 5.7774721182004685\n",
            "the loss on each epoch: 1120, loss: 5.7934588345861995\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-50c6f4b9a071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(outputs,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test : {100 * correct // total} %')\n"
      ],
      "metadata": {
        "id": "XjjcoqJLoqDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}