{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJMWLb8wpiWDnyXgVL4Hrz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "4c904c31-43fa-4c57-f259-c9d5fa262be0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd0ec6f2dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506f2fdd-1143-496c-ceff-6cd6f37cf235"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\n",
            "        chr      start        end  silencer                     peakid\n",
            "0      chr1  113408175  113408375         1   chr1:113408175-113408375\n",
            "1     chr10    6107715    6107915         1      chr10:6107715-6107915\n",
            "2     chr10   15538675   15538875         1    chr10:15538675-15538875\n",
            "3     chr11  114339756  114339956         1  chr11:114339756-114339956\n",
            "4     chr17   41121376   41121576         1    chr17:41121376-41121576\n",
            "...     ...        ...        ...       ...                        ...\n",
            "7227   chrX   93929096   93929296         0     chrX:93929096-93929296\n",
            "7228   chrX  101946395  101946595         0   chrX:101946395-101946595\n",
            "7229   chrX  130859105  130859305         0   chrX:130859105-130859305\n",
            "7230   chrX  131424075  131424275         0   chrX:131424075-131424275\n",
            "7231   chrX  141256436  141256636         0   chrX:141256436-141256636\n",
            "\n",
            "[7232 rows x 5 columns]\n",
            "                         peakid  silencer\n",
            "0      chr1:113408175-113408375         1\n",
            "1         chr10:6107715-6107915         1\n",
            "2       chr10:15538675-15538875         1\n",
            "3     chr11:114339756-114339956         1\n",
            "4       chr17:41121376-41121576         1\n",
            "...                         ...       ...\n",
            "7227     chrX:93929096-93929296         0\n",
            "7228   chrX:101946395-101946595         0\n",
            "7229   chrX:130859105-130859305         0\n",
            "7230   chrX:131424075-131424275         0\n",
            "7231   chrX:141256436-141256636         0\n",
            "\n",
            "[7232 rows x 2 columns]\n",
            "                              0  \\\n",
            "0         chr19:8631775-8631975   \n",
            "1      chr1:113408175-113408375   \n",
            "2      chr2:234276376-234276576   \n",
            "3       chr17:47401936-47402136   \n",
            "4       chr17:41121376-41121576   \n",
            "...                         ...   \n",
            "1995  chr11:110708376-110708576   \n",
            "1996   chr5:177389655-177389855   \n",
            "1997    chr18:44601576-44601776   \n",
            "1998     chr1:19873475-19873675   \n",
            "1999     chr7:72770095-72770295   \n",
            "\n",
            "                                                      1  \n",
            "0     tcttgaactcctgaatttcaagtaatcctcttgcttcagcctatga...  \n",
            "1     ttacaggcatgagccaccgcactcggccTTGTTAAACTTTTTTGTT...  \n",
            "2     AATGCTCTGTGCTCTACCGCATCCCACTTGGAGGTGGGTGTGTCCT...  \n",
            "3     agatgatgtttcactatgttggccaggctggtctcgaactcctgac...  \n",
            "4     GTAAACCAACACTACTTTGTCATACATCATTAGTTTCAGGACtttt...  \n",
            "...                                                 ...  \n",
            "1995  GCTTCCTTTATCTCTCTCCAGGCTCCAGCTTGGATGGTGGCCAACA...  \n",
            "1996  AGGGAGTGTGGCAGAGGGAATGATGGCCATGGTGACACTGAGGAGG...  \n",
            "1997  TTTTACTCTGGGAGCATCTGAGGACTTTAAGTCTATAAGCTTGAAC...  \n",
            "1998  gctgcagacagattagcagctagtaacccatcaaaatggatacaaa...  \n",
            "1999  aaaaaaGTGAGTGTATATGTAACTTGTCCACAAAAAGAATTATACA...  \n",
            "\n",
            "[2000 rows x 2 columns]\n",
            "(2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 10\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(3200, 128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.size())\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc2(x)\n",
        "        #out = torch.flatten(x,1)\n",
        "        #print(out)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "LR = 0.001 \n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 100              # train the training data n times, to save time, we just train 1 epoch\n",
        "LR = 0.001 \n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        trainCorrect += (predsid == b_y).type(torch.float).sum().item()\n",
        "\n",
        "        if i % 10 == 0:    # print every 100 mini-batches\n",
        "            #print(i)\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/10}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Let’s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zckte_qdea_1",
        "outputId": "e5dc9de9-5b54-499d-a242-4c044c9f8ba8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 1.1379390716552735\n",
            "[1,    11] loss: 15.275481605529786\n",
            "[1,    21] loss: 12.461465549468993\n",
            "[1,    31] loss: 10.671319723129272\n",
            "[1,    41] loss: 12.296576595306396\n",
            "[1,    51] loss: 11.40517692565918\n",
            "[1,    61] loss: 10.652275598049163\n",
            "[1,    71] loss: 11.774669885635376\n",
            "[1,    81] loss: 11.415689182281493\n",
            "[1,    91] loss: 11.376946306228637\n",
            "[1,   101] loss: 11.071510362625123\n",
            "[1,   111] loss: 10.649957370758056\n",
            "[1,   121] loss: 12.837766647338867\n",
            "[1,   131] loss: 12.312679481506347\n",
            "[2,     1] loss: 1.6183879852294922\n",
            "[2,    11] loss: 11.282277393341065\n",
            "[2,    21] loss: 12.00034990310669\n",
            "[2,    31] loss: 11.871966695785522\n",
            "[2,    41] loss: 9.928722143173218\n",
            "[2,    51] loss: 11.909815740585326\n",
            "[2,    61] loss: 11.20185890197754\n",
            "[2,    71] loss: 9.563836002349854\n",
            "[2,    81] loss: 10.348117399215699\n",
            "[2,    91] loss: 13.184807920455933\n",
            "[2,   101] loss: 10.94971342086792\n",
            "[2,   111] loss: 12.294267606735229\n",
            "[2,   121] loss: 9.569276297092438\n",
            "[2,   131] loss: 12.000946235656738\n",
            "[3,     1] loss: 1.5725770950317384\n",
            "[3,    11] loss: 12.77874174118042\n",
            "[3,    21] loss: 10.208752822875976\n",
            "[3,    31] loss: 10.586588382720947\n",
            "[3,    41] loss: 12.480201196670532\n",
            "[3,    51] loss: 12.968821716308593\n",
            "[3,    61] loss: 12.989609909057616\n",
            "[3,    71] loss: 8.82052539587021\n",
            "[3,    81] loss: 10.341912078857423\n",
            "[3,    91] loss: 11.262472534179688\n",
            "[3,   101] loss: 11.433306026458741\n",
            "[3,   111] loss: 10.413255167007446\n",
            "[3,   121] loss: 10.539019536972045\n",
            "[3,   131] loss: 9.742681503295898\n",
            "[4,     1] loss: 2.09808464050293\n",
            "[4,    11] loss: 8.357250261306763\n",
            "[4,    21] loss: 11.825111055374146\n",
            "[4,    31] loss: 10.797996997833252\n",
            "[4,    41] loss: 10.73941946029663\n",
            "[4,    51] loss: 10.46405725479126\n",
            "[4,    61] loss: 11.78775405883789\n",
            "[4,    71] loss: 9.457845449447632\n",
            "[4,    81] loss: 12.004951667785644\n",
            "[4,    91] loss: 8.93424413204193\n",
            "[4,   101] loss: 9.818603086471558\n",
            "[4,   111] loss: 11.489594173431396\n",
            "[4,   121] loss: 10.189162969589233\n",
            "[4,   131] loss: 12.78630714416504\n",
            "[5,     1] loss: 1.1376628875732422\n",
            "[5,    11] loss: 11.664697647094727\n",
            "[5,    21] loss: 10.080486130714416\n",
            "[5,    31] loss: 9.333226490020753\n",
            "[5,    41] loss: 10.715004539489746\n",
            "[5,    51] loss: 10.811116361618042\n",
            "[5,    61] loss: 11.198496389389039\n",
            "[5,    71] loss: 10.964450311660766\n",
            "[5,    81] loss: 12.690002059936523\n",
            "[5,    91] loss: 9.86155037879944\n",
            "[5,   101] loss: 8.597556543350219\n",
            "[5,   111] loss: 9.33238525390625\n",
            "[5,   121] loss: 11.027546215057374\n",
            "[5,   131] loss: 9.727053022384643\n",
            "[6,     1] loss: 0.8100099563598633\n",
            "[6,    11] loss: 10.202407789230346\n",
            "[6,    21] loss: 9.704407739639283\n",
            "[6,    31] loss: 7.714938640594482\n",
            "[6,    41] loss: 12.014393281936645\n",
            "[6,    51] loss: 8.557883501052856\n",
            "[6,    61] loss: 12.091529083251952\n",
            "[6,    71] loss: 10.898185014724731\n",
            "[6,    81] loss: 10.739790391921996\n",
            "[6,    91] loss: 9.856725311279297\n",
            "[6,   101] loss: 8.873837232589722\n",
            "[6,   111] loss: 12.038235282897949\n",
            "[6,   121] loss: 9.596979141235352\n",
            "[6,   131] loss: 10.590888071060181\n",
            "[7,     1] loss: 1.0621479988098144\n",
            "[7,    11] loss: 10.941242504119874\n",
            "[7,    21] loss: 10.21021580696106\n",
            "[7,    31] loss: 10.038599920272826\n",
            "[7,    41] loss: 8.74168050289154\n",
            "[7,    51] loss: 12.043254327774047\n",
            "[7,    61] loss: 11.07689689397812\n",
            "[7,    71] loss: 10.0789044380188\n",
            "[7,    81] loss: 9.480323934555054\n",
            "[7,    91] loss: 11.493783259391785\n",
            "[7,   101] loss: 9.5058753490448\n",
            "[7,   111] loss: 10.83299560546875\n",
            "[7,   121] loss: 8.67020525932312\n",
            "[7,   131] loss: 9.179203176498413\n",
            "[8,     1] loss: 0.1379438281059265\n",
            "[8,    11] loss: 10.227335214614868\n",
            "[8,    21] loss: 10.394886112213134\n",
            "[8,    31] loss: 9.885325717926026\n",
            "[8,    41] loss: 10.47096643447876\n",
            "[8,    51] loss: 11.098053216934204\n",
            "[8,    61] loss: 8.312402415275574\n",
            "[8,    71] loss: 9.79718804359436\n",
            "[8,    81] loss: 10.669401550292969\n",
            "[8,    91] loss: 10.061758995056152\n",
            "[8,   101] loss: 9.681439876556396\n",
            "[8,   111] loss: 10.62923355102539\n",
            "[8,   121] loss: 8.735570812225342\n",
            "[8,   131] loss: 10.63967342376709\n",
            "[9,     1] loss: 0.9024591445922852\n",
            "[9,    11] loss: 9.835348320007324\n",
            "[9,    21] loss: 9.55345344543457\n",
            "[9,    31] loss: 8.855101561546325\n",
            "[9,    41] loss: 9.5674334526062\n",
            "[9,    51] loss: 9.336647558212281\n",
            "[9,    61] loss: 9.173516273498535\n",
            "[9,    71] loss: 7.19091272354126\n",
            "[9,    81] loss: 11.305490827560424\n",
            "[9,    91] loss: 10.56865963935852\n",
            "[9,   101] loss: 9.2654559135437\n",
            "[9,   111] loss: 10.362133455276489\n",
            "[9,   121] loss: 9.23634614944458\n",
            "[9,   131] loss: 9.711890625953675\n",
            "[10,     1] loss: 1.5040234565734862\n",
            "[10,    11] loss: 9.731152057647705\n",
            "[10,    21] loss: 11.505173063278198\n",
            "[10,    31] loss: 8.016593670845031\n",
            "[10,    41] loss: 10.512070608139037\n",
            "[10,    51] loss: 8.65385549068451\n",
            "[10,    61] loss: 9.2558358669281\n",
            "[10,    71] loss: 10.037529611587525\n",
            "[10,    81] loss: 10.033919858932496\n",
            "[10,    91] loss: 8.886061310768127\n",
            "[10,   101] loss: 8.820416021347047\n",
            "[10,   111] loss: 9.031230020523072\n",
            "[10,   121] loss: 9.408920907974244\n",
            "[10,   131] loss: 8.825138711929322\n",
            "[11,     1] loss: 1.006885051727295\n",
            "[11,    11] loss: 9.278280591964721\n",
            "[11,    21] loss: 9.176557111740113\n",
            "[11,    31] loss: 7.006584548950196\n",
            "[11,    41] loss: 9.285436248779297\n",
            "[11,    51] loss: 10.991663074493408\n",
            "[11,    61] loss: 9.181905221939086\n",
            "[11,    71] loss: 10.645302152633667\n",
            "[11,    81] loss: 9.575327825546264\n",
            "[11,    91] loss: 11.387394332885743\n",
            "[11,   101] loss: 6.463857984542846\n",
            "[11,   111] loss: 6.924300813674927\n",
            "[11,   121] loss: 10.137464904785157\n",
            "[11,   131] loss: 8.574189805984497\n",
            "[12,     1] loss: 0.655626630783081\n",
            "[12,    11] loss: 6.59141161441803\n",
            "[12,    21] loss: 10.735984516143798\n",
            "[12,    31] loss: 10.23832459449768\n",
            "[12,    41] loss: 9.13941822052002\n",
            "[12,    51] loss: 9.968249040842057\n",
            "[12,    61] loss: 9.464368438720703\n",
            "[12,    71] loss: 9.844312131404877\n",
            "[12,    81] loss: 9.868872928619385\n",
            "[12,    91] loss: 8.266952753067017\n",
            "[12,   101] loss: 6.728803339600563\n",
            "[12,   111] loss: 8.502354669570924\n",
            "[12,   121] loss: 9.249815797805786\n",
            "[12,   131] loss: 8.317784643173217\n",
            "[13,     1] loss: 0.7588154315948487\n",
            "[13,    11] loss: 8.440085172653198\n",
            "[13,    21] loss: 9.601273012161254\n",
            "[13,    31] loss: 7.762981367111206\n",
            "[13,    41] loss: 8.6342764377594\n",
            "[13,    51] loss: 10.482302618026733\n",
            "[13,    61] loss: 9.151616477966309\n",
            "[13,    71] loss: 8.189363360404968\n",
            "[13,    81] loss: 9.951898574829102\n",
            "[13,    91] loss: 8.22464895248413\n",
            "[13,   101] loss: 9.974182605743408\n",
            "[13,   111] loss: 10.283847856521607\n",
            "[13,   121] loss: 7.007269811630249\n",
            "[13,   131] loss: 8.319822597503663\n",
            "[14,     1] loss: 0.6603609561920166\n",
            "[14,    11] loss: 8.3135160446167\n",
            "[14,    21] loss: 9.616844511032104\n",
            "[14,    31] loss: 7.537057387828827\n",
            "[14,    41] loss: 10.625121927261352\n",
            "[14,    51] loss: 7.670145511627197\n",
            "[14,    61] loss: 9.311152410507201\n",
            "[14,    71] loss: 10.054066562652588\n",
            "[14,    81] loss: 9.688378190994262\n",
            "[14,    91] loss: 7.7648711681365965\n",
            "[14,   101] loss: 8.039603531360626\n",
            "[14,   111] loss: 7.40098911523819\n",
            "[14,   121] loss: 9.911570167541504\n",
            "[14,   131] loss: 13.027682638168335\n",
            "[15,     1] loss: 1.460423469543457\n",
            "[15,    11] loss: 7.97166075706482\n",
            "[15,    21] loss: 9.358810186386108\n",
            "[15,    31] loss: 8.305638659000397\n",
            "[15,    41] loss: 10.121332502365112\n",
            "[15,    51] loss: 8.195440793037415\n",
            "[15,    61] loss: 10.508166027069091\n",
            "[15,    71] loss: 8.835243320465088\n",
            "[15,    81] loss: 11.21684639453888\n",
            "[15,    91] loss: 10.369883823394776\n",
            "[15,   101] loss: 10.264701461791992\n",
            "[15,   111] loss: 10.488841581344605\n",
            "[15,   121] loss: 6.294434690475464\n",
            "[15,   131] loss: 5.202109003067017\n",
            "[16,     1] loss: 0.9432136535644531\n",
            "[16,    11] loss: 10.827316164970398\n",
            "[16,    21] loss: 10.318702340126038\n",
            "[16,    31] loss: 9.984473085403442\n",
            "[16,    41] loss: 7.905010223388672\n",
            "[16,    51] loss: 6.579152464866638\n",
            "[16,    61] loss: 10.070972526073456\n",
            "[16,    71] loss: 8.212406086921693\n",
            "[16,    81] loss: 9.513282966613769\n",
            "[16,    91] loss: 9.01732325553894\n",
            "[16,   101] loss: 8.500241446495057\n",
            "[16,   111] loss: 8.044858360290528\n",
            "[16,   121] loss: 9.955011987686158\n",
            "[16,   131] loss: 9.082320380210877\n",
            "[17,     1] loss: 1.7200719833374023\n",
            "[17,    11] loss: 7.798884344100952\n",
            "[17,    21] loss: 8.163428831100465\n",
            "[17,    31] loss: 8.201314646005631\n",
            "[17,    41] loss: 7.830809831619263\n",
            "[17,    51] loss: 9.312659406661988\n",
            "[17,    61] loss: 8.20180606842041\n",
            "[17,    71] loss: 8.427187538146972\n",
            "[17,    81] loss: 9.323353576660157\n",
            "[17,    91] loss: 10.26195969581604\n",
            "[17,   101] loss: 10.02637745141983\n",
            "[17,   111] loss: 9.735463714599609\n",
            "[17,   121] loss: 8.975992202758789\n",
            "[17,   131] loss: 9.173682689666748\n",
            "[18,     1] loss: 0.6369794368743896\n",
            "[18,    11] loss: 9.097988700866699\n",
            "[18,    21] loss: 8.855582094192505\n",
            "[18,    31] loss: 8.402254438400268\n",
            "[18,    41] loss: 9.048378419876098\n",
            "[18,    51] loss: 9.109469604492187\n",
            "[18,    61] loss: 8.57523078918457\n",
            "[18,    71] loss: 10.017209815979005\n",
            "[18,    81] loss: 7.813971400260925\n",
            "[18,    91] loss: 8.090925431251526\n",
            "[18,   101] loss: 8.306878232955933\n",
            "[18,   111] loss: 10.476041984558105\n",
            "[18,   121] loss: 8.887609693408013\n",
            "[18,   131] loss: 6.994941687583923\n",
            "[19,     1] loss: 0.9062058448791503\n",
            "[19,    11] loss: 9.750989818572998\n",
            "[19,    21] loss: 7.4573872804641725\n",
            "[19,    31] loss: 7.1415303468704225\n",
            "[19,    41] loss: 10.57676157951355\n",
            "[19,    51] loss: 9.106918716430664\n",
            "[19,    61] loss: 7.134206527471543\n",
            "[19,    71] loss: 7.81245698928833\n",
            "[19,    81] loss: 8.92782998085022\n",
            "[19,    91] loss: 6.356993770599365\n",
            "[19,   101] loss: 8.513758516311645\n",
            "[19,   111] loss: 9.168495416641235\n",
            "[19,   121] loss: 11.228173017501831\n",
            "[19,   131] loss: 8.712681198120118\n",
            "[20,     1] loss: 1.0822887420654297\n",
            "[20,    11] loss: 6.437746334075928\n",
            "[20,    21] loss: 7.980390167236328\n",
            "[20,    31] loss: 7.4638570785522464\n",
            "[20,    41] loss: 9.333266258239746\n",
            "[20,    51] loss: 9.112326458096504\n",
            "[20,    61] loss: 10.749219989776611\n",
            "[20,    71] loss: 9.603418970108033\n",
            "[20,    81] loss: 10.829300999641418\n",
            "[20,    91] loss: 8.293142342567444\n",
            "[20,   101] loss: 7.355667638778686\n",
            "[20,   111] loss: 8.229306387901307\n",
            "[20,   121] loss: 7.014667320251465\n",
            "[20,   131] loss: 8.822926902770996\n",
            "[21,     1] loss: 1.4258879661560058\n",
            "[21,    11] loss: 7.540070915222168\n",
            "[21,    21] loss: 7.17472128868103\n",
            "[21,    31] loss: 9.275324892997741\n",
            "[21,    41] loss: 7.8309612512588505\n",
            "[21,    51] loss: 9.006071281433105\n",
            "[21,    61] loss: 8.322579073905946\n",
            "[21,    71] loss: 7.809888696670532\n",
            "[21,    81] loss: 10.031027364730836\n",
            "[21,    91] loss: 8.492737126350402\n",
            "[21,   101] loss: 6.938963842391968\n",
            "[21,   111] loss: 8.664107275009155\n",
            "[21,   121] loss: 9.038002347946167\n",
            "[21,   131] loss: 9.500636100769043\n",
            "[22,     1] loss: 0.3952932596206665\n",
            "[22,    11] loss: 8.666410064697265\n",
            "[22,    21] loss: 7.438838648796081\n",
            "[22,    31] loss: 10.263477754592895\n",
            "[22,    41] loss: 9.782349014282227\n",
            "[22,    51] loss: 8.228090977668762\n",
            "[22,    61] loss: 9.696325874328613\n",
            "[22,    71] loss: 7.60024139881134\n",
            "[22,    81] loss: 10.004057598114013\n",
            "[22,    91] loss: 8.70640823841095\n",
            "[22,   101] loss: 8.399285268783569\n",
            "[22,   111] loss: 10.255320823192596\n",
            "[22,   121] loss: 9.196055126190185\n",
            "[22,   131] loss: 8.331473922729492\n",
            "[23,     1] loss: 0.8078533172607422\n",
            "[23,    11] loss: 10.54016363620758\n",
            "[23,    21] loss: 8.442815828323365\n",
            "[23,    31] loss: 9.280083703994752\n",
            "[23,    41] loss: 7.492778170108795\n",
            "[23,    51] loss: 9.540367603302002\n",
            "[23,    61] loss: 8.921851205825806\n",
            "[23,    71] loss: 9.098446774482728\n",
            "[23,    81] loss: 9.861880826950074\n",
            "[23,    91] loss: 7.466725707054138\n",
            "[23,   101] loss: 10.094902181625367\n",
            "[23,   111] loss: 8.92944065630436\n",
            "[23,   121] loss: 7.035147285461425\n",
            "[23,   131] loss: 9.480388593673705\n",
            "[24,     1] loss: 1.3786850929260255\n",
            "[24,    11] loss: 6.998209904134273\n",
            "[24,    21] loss: 7.578767049312591\n",
            "[24,    31] loss: 9.074752378463746\n",
            "[24,    41] loss: 9.353168535232545\n",
            "[24,    51] loss: 9.466505026817321\n",
            "[24,    61] loss: 9.57712917327881\n",
            "[24,    71] loss: 7.850902843475342\n",
            "[24,    81] loss: 10.303321075439452\n",
            "[24,    91] loss: 8.650973463058472\n",
            "[24,   101] loss: 8.879146504402161\n",
            "[24,   111] loss: 7.724299478530884\n",
            "[24,   121] loss: 7.5661367893219\n",
            "[24,   131] loss: 10.722604680061341\n",
            "[25,     1] loss: 0.44797601699829104\n",
            "[25,    11] loss: 9.216045904159547\n",
            "[25,    21] loss: 9.36260290145874\n",
            "[25,    31] loss: 9.742231369018555\n",
            "[25,    41] loss: 8.460057997703553\n",
            "[25,    51] loss: 7.423386779427529\n",
            "[25,    61] loss: 9.75809907913208\n",
            "[25,    71] loss: 11.00505256652832\n",
            "[25,    81] loss: 8.328033924102783\n",
            "[25,    91] loss: 7.773529386520385\n",
            "[25,   101] loss: 6.8310879945755\n",
            "[25,   111] loss: 9.355045604705811\n",
            "[25,   121] loss: 8.881965446472169\n",
            "[25,   131] loss: 10.409522294998169\n",
            "[26,     1] loss: 0.5680620193481445\n",
            "[26,    11] loss: 9.984196424484253\n",
            "[26,    21] loss: 8.878272557258606\n",
            "[26,    31] loss: 9.51878925561905\n",
            "[26,    41] loss: 6.0448805809021\n",
            "[26,    51] loss: 6.083216643333435\n",
            "[26,    61] loss: 8.136552882194518\n",
            "[26,    71] loss: 9.916305923461914\n",
            "[26,    81] loss: 7.57816219329834\n",
            "[26,    91] loss: 10.980659008026123\n",
            "[26,   101] loss: 9.759115505218507\n",
            "[26,   111] loss: 9.44601731300354\n",
            "[26,   121] loss: 7.057640743255615\n",
            "[26,   131] loss: 10.627618408203125\n",
            "[27,     1] loss: 0.07236138582229615\n",
            "[27,    11] loss: 7.812777948379517\n",
            "[27,    21] loss: 7.4917732119560245\n",
            "[27,    31] loss: 9.402224552631377\n",
            "[27,    41] loss: 9.452928483486176\n",
            "[27,    51] loss: 9.00647144317627\n",
            "[27,    61] loss: 9.446054172515868\n",
            "[27,    71] loss: 7.773220157623291\n",
            "[27,    81] loss: 8.295769524574279\n",
            "[27,    91] loss: 7.7849660277366635\n",
            "[27,   101] loss: 13.90505075454712\n",
            "[27,   111] loss: 9.745235633850097\n",
            "[27,   121] loss: 9.26328604221344\n",
            "[27,   131] loss: 6.603353273868561\n",
            "[28,     1] loss: 0.5558830261230469\n",
            "[28,    11] loss: 8.862311267852784\n",
            "[28,    21] loss: 8.331541967391967\n",
            "[28,    31] loss: 8.433511543273926\n",
            "[28,    41] loss: 7.554556822776794\n",
            "[28,    51] loss: 6.022243374586106\n",
            "[28,    61] loss: 8.220442390441894\n",
            "[28,    71] loss: 9.374523401260376\n",
            "[28,    81] loss: 10.383845996856689\n",
            "[28,    91] loss: 7.842540884017945\n",
            "[28,   101] loss: 9.362375211715698\n",
            "[28,   111] loss: 10.589092350006103\n",
            "[28,   121] loss: 8.483412027359009\n",
            "[28,   131] loss: 9.43767409324646\n",
            "[29,     1] loss: 0.5571462631225585\n",
            "[29,    11] loss: 8.842112731933593\n",
            "[29,    21] loss: 7.410305714607238\n",
            "[29,    31] loss: 8.877751445770263\n",
            "[29,    41] loss: 8.649462246894837\n",
            "[29,    51] loss: 8.36318210363388\n",
            "[29,    61] loss: 7.813153076171875\n",
            "[29,    71] loss: 9.21475772857666\n",
            "[29,    81] loss: 7.600336694717408\n",
            "[29,    91] loss: 8.947711706161499\n",
            "[29,   101] loss: 7.647939705848694\n",
            "[29,   111] loss: 9.910930728912353\n",
            "[29,   121] loss: 9.973657512664795\n",
            "[29,   131] loss: 8.63060051202774\n",
            "[30,     1] loss: 0.19695295095443727\n",
            "[30,    11] loss: 6.191098690032959\n",
            "[30,    21] loss: 8.323972487449646\n",
            "[30,    31] loss: 8.668953466415406\n",
            "[30,    41] loss: 10.691932153701782\n",
            "[30,    51] loss: 8.953014183044434\n",
            "[30,    61] loss: 8.711793923377991\n",
            "[30,    71] loss: 10.098227214813232\n",
            "[30,    81] loss: 7.407302089035511\n",
            "[30,    91] loss: 8.674960947036743\n",
            "[30,   101] loss: 8.607493257522583\n",
            "[30,   111] loss: 8.312163424491882\n",
            "[30,   121] loss: 10.824027395248413\n",
            "[30,   131] loss: 8.8934650182724\n",
            "[31,     1] loss: 0.8062089920043946\n",
            "[31,    11] loss: 8.562648439407349\n",
            "[31,    21] loss: 8.699282693862916\n",
            "[31,    31] loss: 9.638341927528382\n",
            "[31,    41] loss: 8.955758547782898\n",
            "[31,    51] loss: 8.334716391563415\n",
            "[31,    61] loss: 7.894012761116028\n",
            "[31,    71] loss: 9.743061995506286\n",
            "[31,    81] loss: 10.554791116714478\n",
            "[31,    91] loss: 7.02371129989624\n",
            "[31,   101] loss: 7.758961081504822\n",
            "[31,   111] loss: 7.401137280464172\n",
            "[31,   121] loss: 10.42695198059082\n",
            "[31,   131] loss: 9.757824087142945\n",
            "[32,     1] loss: 0.5561573505401611\n",
            "[32,    11] loss: 10.236451959609985\n",
            "[32,    21] loss: 8.895281314849854\n",
            "[32,    31] loss: 7.762549962103367\n",
            "[32,    41] loss: 9.357771599292755\n",
            "[32,    51] loss: 8.943896150588989\n",
            "[32,    61] loss: 8.514270687103272\n",
            "[32,    71] loss: 8.646801567077636\n",
            "[32,    81] loss: 7.316959714889526\n",
            "[32,    91] loss: 11.094708251953126\n",
            "[32,   101] loss: 6.663366580009461\n",
            "[32,   111] loss: 10.212931585311889\n",
            "[32,   121] loss: 7.877742409706116\n",
            "[32,   131] loss: 8.062289702892304\n",
            "[33,     1] loss: 1.3638318061828614\n",
            "[33,    11] loss: 5.488043248653412\n",
            "[33,    21] loss: 9.518083572387695\n",
            "[33,    31] loss: 9.586418771743775\n",
            "[33,    41] loss: 8.052328276634217\n",
            "[33,    51] loss: 7.901882314682007\n",
            "[33,    61] loss: 9.336108922958374\n",
            "[33,    71] loss: 7.244203281402588\n",
            "[33,    81] loss: 9.685787510871886\n",
            "[33,    91] loss: 9.041654467582703\n",
            "[33,   101] loss: 10.26738748550415\n",
            "[33,   111] loss: 7.1022762805223465\n",
            "[33,   121] loss: 7.868341231346131\n",
            "[33,   131] loss: 12.168097877502442\n",
            "[34,     1] loss: 1.0796728134155273\n",
            "[34,    11] loss: 7.851651620864868\n",
            "[34,    21] loss: 8.933451604843139\n",
            "[34,    31] loss: 9.321579027175904\n",
            "[34,    41] loss: 9.873994994163514\n",
            "[34,    51] loss: 9.00965723991394\n",
            "[34,    61] loss: 9.088575220108032\n",
            "[34,    71] loss: 8.7680344581604\n",
            "[34,    81] loss: 10.971711945533752\n",
            "[34,    91] loss: 8.443061041831971\n",
            "[34,   101] loss: 6.617599201202393\n",
            "[34,   111] loss: 8.679210901260376\n",
            "[34,   121] loss: 6.955438160896302\n",
            "[34,   131] loss: 9.608516621589661\n",
            "[35,     1] loss: 0.5570882797241211\n",
            "[35,    11] loss: 6.809774041175842\n",
            "[35,    21] loss: 10.449644088745117\n",
            "[35,    31] loss: 8.751256275177003\n",
            "[35,    41] loss: 8.265138053894043\n",
            "[35,    51] loss: 8.324870610237122\n",
            "[35,    61] loss: 7.698050236701965\n",
            "[35,    71] loss: 8.166020500659943\n",
            "[35,    81] loss: 8.55640618801117\n",
            "[35,    91] loss: 8.243664503097534\n",
            "[35,   101] loss: 10.566166067123413\n",
            "[35,   111] loss: 9.942739629745484\n",
            "[35,   121] loss: 9.879954290390014\n",
            "[35,   131] loss: 6.81059763431549\n",
            "[36,     1] loss: 0.6493353843688965\n",
            "[36,    11] loss: 8.550015187263488\n",
            "[36,    21] loss: 9.414928531646728\n",
            "[36,    31] loss: 9.252405524253845\n",
            "[36,    41] loss: 8.661241424083709\n",
            "[36,    51] loss: 6.936881351470947\n",
            "[36,    61] loss: 8.970403957366944\n",
            "[36,    71] loss: 7.262181830406189\n",
            "[36,    81] loss: 10.759724330902099\n",
            "[36,    91] loss: 8.632383155822755\n",
            "[36,   101] loss: 11.459373378753662\n",
            "[36,   111] loss: 6.772134637832641\n",
            "[36,   121] loss: 7.668089866638184\n",
            "[36,   131] loss: 8.441216766834259\n",
            "[37,     1] loss: 0.6151731014251709\n",
            "[37,    11] loss: 8.321478223800659\n",
            "[37,    21] loss: 7.08646240234375\n",
            "[37,    31] loss: 8.411047434806823\n",
            "[37,    41] loss: 8.66076946258545\n",
            "[37,    51] loss: 10.667812776565551\n",
            "[37,    61] loss: 9.728267478942872\n",
            "[37,    71] loss: 6.777673661708832\n",
            "[37,    81] loss: 9.860532522201538\n",
            "[37,    91] loss: 8.894275784492493\n",
            "[37,   101] loss: 8.287152767181396\n",
            "[37,   111] loss: 9.188724398612976\n",
            "[37,   121] loss: 8.37471549808979\n",
            "[37,   131] loss: 7.8654618740081785\n",
            "[38,     1] loss: 1.0774923324584962\n",
            "[38,    11] loss: 9.056304383277894\n",
            "[38,    21] loss: 8.981617450714111\n",
            "[38,    31] loss: 9.290610980987548\n",
            "[38,    41] loss: 9.343333625793457\n",
            "[38,    51] loss: 5.771328550577164\n",
            "[38,    61] loss: 10.420236790180207\n",
            "[38,    71] loss: 8.109899497032165\n",
            "[38,    81] loss: 7.245679008960724\n",
            "[38,    91] loss: 8.100363183021546\n",
            "[38,   101] loss: 6.716256499290466\n",
            "[38,   111] loss: 8.38334059715271\n",
            "[38,   121] loss: 9.584370815753937\n",
            "[38,   131] loss: 8.697018725145607\n",
            "[39,     1] loss: 0.5579599380493164\n",
            "[39,    11] loss: 7.0768474817276\n",
            "[39,    21] loss: 10.170937013626098\n",
            "[39,    31] loss: 8.412746286392212\n",
            "[39,    41] loss: 9.973899817466735\n",
            "[39,    51] loss: 10.580469799041747\n",
            "[39,    61] loss: 6.861046020966024\n",
            "[39,    71] loss: 8.572018432617188\n",
            "[39,    81] loss: 6.806727886199951\n",
            "[39,    91] loss: 10.769524765014648\n",
            "[39,   101] loss: 6.370544195175171\n",
            "[39,   111] loss: 7.369917749986053\n",
            "[39,   121] loss: 8.459539937973023\n",
            "[39,   131] loss: 10.992514753341675\n",
            "[40,     1] loss: 0.6427876472473144\n",
            "[40,    11] loss: 10.321072626113892\n",
            "[40,    21] loss: 7.1826643466949465\n",
            "[40,    31] loss: 8.032048380374908\n",
            "[40,    41] loss: 10.464304733276368\n",
            "[40,    51] loss: 7.792433023452759\n",
            "[40,    61] loss: 8.231679821014405\n",
            "[40,    71] loss: 9.405370688438415\n",
            "[40,    81] loss: 6.774605023860931\n",
            "[40,    91] loss: 6.928918814659118\n",
            "[40,   101] loss: 11.787418508529663\n",
            "[40,   111] loss: 8.16016731262207\n",
            "[40,   121] loss: 9.97093951702118\n",
            "[40,   131] loss: 10.862002968788147\n",
            "[41,     1] loss: 0.8117529869079589\n",
            "[41,    11] loss: 7.31552631855011\n",
            "[41,    21] loss: 8.74851326942444\n",
            "[41,    31] loss: 8.83909239768982\n",
            "[41,    41] loss: 6.785899269580841\n",
            "[41,    51] loss: 6.354480028152466\n",
            "[41,    61] loss: 9.4960267663002\n",
            "[41,    71] loss: 9.923168992996215\n",
            "[41,    81] loss: 8.506389617919922\n",
            "[41,    91] loss: 8.935351753234864\n",
            "[41,   101] loss: 7.309164714813233\n",
            "[41,   111] loss: 9.509358167648315\n",
            "[41,   121] loss: 11.160168957710265\n",
            "[41,   131] loss: 9.217563676834107\n",
            "[42,     1] loss: 0.351422119140625\n",
            "[42,    11] loss: 7.203953868150711\n",
            "[42,    21] loss: 8.063275170326232\n",
            "[42,    31] loss: 8.05227825641632\n",
            "[42,    41] loss: 9.110039234161377\n",
            "[42,    51] loss: 8.55395324230194\n",
            "[42,    61] loss: 8.914154148101806\n",
            "[42,    71] loss: 8.458908557891846\n",
            "[42,    81] loss: 10.098869895935058\n",
            "[42,    91] loss: 10.854840993881226\n",
            "[42,   101] loss: 8.413945722579957\n",
            "[42,   111] loss: 9.631757831573486\n",
            "[42,   121] loss: 6.752351021766662\n",
            "[42,   131] loss: 10.005833292007447\n",
            "[43,     1] loss: 1.1816688537597657\n",
            "[43,    11] loss: 8.32874960899353\n",
            "[43,    21] loss: 6.501456952095031\n",
            "[43,    31] loss: 10.232043600082397\n",
            "[43,    41] loss: 9.676024913787842\n",
            "[43,    51] loss: 10.672761487960816\n",
            "[43,    61] loss: 9.068756294250488\n",
            "[43,    71] loss: 7.4542879343032835\n",
            "[43,    81] loss: 9.150865912437439\n",
            "[43,    91] loss: 8.502332544326782\n",
            "[43,   101] loss: 9.297157406806946\n",
            "[43,   111] loss: 9.01892831325531\n",
            "[43,   121] loss: 9.51327052116394\n",
            "[43,   131] loss: 9.299092531204224\n",
            "[44,     1] loss: 0.8082343101501465\n",
            "[44,    11] loss: 9.805720138549805\n",
            "[44,    21] loss: 7.870044994354248\n",
            "[44,    31] loss: 8.108381652832032\n",
            "[44,    41] loss: 9.692445969581604\n",
            "[44,    51] loss: 9.139450550079346\n",
            "[44,    61] loss: 10.599106931686402\n",
            "[44,    71] loss: 7.780683016777038\n",
            "[44,    81] loss: 5.5507166385650635\n",
            "[44,    91] loss: 9.456295156478882\n",
            "[44,   101] loss: 8.972328209877015\n",
            "[44,   111] loss: 8.16197726726532\n",
            "[44,   121] loss: 9.965139150619507\n",
            "[44,   131] loss: 10.450046634674072\n",
            "[45,     1] loss: 0.8070550918579101\n",
            "[45,    11] loss: 9.308531475067138\n",
            "[45,    21] loss: 8.309915924072266\n",
            "[45,    31] loss: 8.573744320869446\n",
            "[45,    41] loss: 6.988280773162842\n",
            "[45,    51] loss: 9.072692584991454\n",
            "[45,    61] loss: 9.737828063964844\n",
            "[45,    71] loss: 8.024878358840942\n",
            "[45,    81] loss: 8.592627954483032\n",
            "[45,    91] loss: 8.849232029914855\n",
            "[45,   101] loss: 9.281412386894226\n",
            "[45,   111] loss: 9.133932971954346\n",
            "[45,   121] loss: 7.582702577114105\n",
            "[45,   131] loss: 6.5544313788414\n",
            "[46,     1] loss: 0.33428704738616943\n",
            "[46,    11] loss: 10.52391276359558\n",
            "[46,    21] loss: 10.454469841718673\n",
            "[46,    31] loss: 8.258954799175262\n",
            "[46,    41] loss: 7.137706136703491\n",
            "[46,    51] loss: 5.483689332008362\n",
            "[46,    61] loss: 8.773563289642334\n",
            "[46,    71] loss: 8.70567946434021\n",
            "[46,    81] loss: 9.587781524658203\n",
            "[46,    91] loss: 10.381972551345825\n",
            "[46,   101] loss: 7.6778864622116085\n",
            "[46,   111] loss: 8.482622838020324\n",
            "[46,   121] loss: 8.845048069953918\n",
            "[46,   131] loss: 8.193746185302734\n",
            "[47,     1] loss: 1.1674891471862794\n",
            "[47,    11] loss: 7.350199067592621\n",
            "[47,    21] loss: 9.573859906196594\n",
            "[47,    31] loss: 8.629486703872681\n",
            "[47,    41] loss: 7.904703108896501\n",
            "[47,    51] loss: 7.098234915733338\n",
            "[47,    61] loss: 9.158175760507584\n",
            "[47,    71] loss: 8.687732410430907\n",
            "[47,    81] loss: 8.273419499397278\n",
            "[47,    91] loss: 8.399091482162476\n",
            "[47,   101] loss: 7.372490739822387\n",
            "[47,   111] loss: 10.151833510398864\n",
            "[47,   121] loss: 9.064555144309997\n",
            "[47,   131] loss: 8.387418508529663\n",
            "[48,     1] loss: 0.0023144811391830446\n",
            "[48,    11] loss: 8.50638813972473\n",
            "[48,    21] loss: 9.588026428222657\n",
            "[48,    31] loss: 8.420368275046348\n",
            "[48,    41] loss: 7.985204124450684\n",
            "[48,    51] loss: 7.840159392356872\n",
            "[48,    61] loss: 9.45282244682312\n",
            "[48,    71] loss: 8.421238434314727\n",
            "[48,    81] loss: 9.646236991882324\n",
            "[48,    91] loss: 7.455517911911011\n",
            "[48,   101] loss: 8.657587575912476\n",
            "[48,   111] loss: 10.076059722900391\n",
            "[48,   121] loss: 7.436806106567383\n",
            "[48,   131] loss: 8.266214406490326\n",
            "[49,     1] loss: 0.8108134269714355\n",
            "[49,    11] loss: 10.671460390090942\n",
            "[49,    21] loss: 5.998904299736023\n",
            "[49,    31] loss: 8.946436476707458\n",
            "[49,    41] loss: 9.82896984219551\n",
            "[49,    51] loss: 9.48825136423111\n",
            "[49,    61] loss: 6.877955627441406\n",
            "[49,    71] loss: 9.159664630889893\n",
            "[49,    81] loss: 7.39834361076355\n",
            "[49,    91] loss: 10.326300048828125\n",
            "[49,   101] loss: 10.119904184341431\n",
            "[49,   111] loss: 7.194922256469726\n",
            "[49,   121] loss: 9.535695648193359\n",
            "[49,   131] loss: 8.487495493888854\n",
            "[50,     1] loss: 0.8068169593811035\n",
            "[50,    11] loss: 9.78332902789116\n",
            "[50,    21] loss: 10.789438176155091\n",
            "[50,    31] loss: 10.067807817459107\n",
            "[50,    41] loss: 6.979006754327566\n",
            "[50,    51] loss: 6.6234459400177\n",
            "[50,    61] loss: 8.50399363040924\n",
            "[50,    71] loss: 7.777893090248108\n",
            "[50,    81] loss: 10.041749548912048\n",
            "[50,    91] loss: 6.849983048439026\n",
            "[50,   101] loss: 8.698924601078033\n",
            "[50,   111] loss: 9.141317987442017\n",
            "[50,   121] loss: 9.52203369140625\n",
            "[50,   131] loss: 10.146991682052612\n",
            "[51,     1] loss: 1.076595401763916\n",
            "[51,    11] loss: 8.80593376159668\n",
            "[51,    21] loss: 7.02725328207016\n",
            "[51,    31] loss: 9.242891311645508\n",
            "[51,    41] loss: 7.190566039085388\n",
            "[51,    51] loss: 10.50129988193512\n",
            "[51,    61] loss: 7.818040227890014\n",
            "[51,    71] loss: 7.489782810211182\n",
            "[51,    81] loss: 10.599356925487518\n",
            "[51,    91] loss: 8.639846515655517\n",
            "[51,   101] loss: 8.584264445304871\n",
            "[51,   111] loss: 10.535513973236084\n",
            "[51,   121] loss: 7.8915381073951725\n",
            "[51,   131] loss: 7.274641752243042\n",
            "[52,     1] loss: 0.8658782005310058\n",
            "[52,    11] loss: 8.212277495861054\n",
            "[52,    21] loss: 8.07906939983368\n",
            "[52,    31] loss: 7.1270117756146645\n",
            "[52,    41] loss: 7.695395731925965\n",
            "[52,    51] loss: 8.836509466171265\n",
            "[52,    61] loss: 7.203285956382752\n",
            "[52,    71] loss: 11.30365447998047\n",
            "[52,    81] loss: 10.525356769561768\n",
            "[52,    91] loss: 9.27751636505127\n",
            "[52,   101] loss: 6.982908415794372\n",
            "[52,   111] loss: 9.668571329116821\n",
            "[52,   121] loss: 8.513198137283325\n",
            "[52,   131] loss: 8.564888095855713\n",
            "[53,     1] loss: 1.4136104583740234\n",
            "[53,    11] loss: 10.614071464538574\n",
            "[53,    21] loss: 6.388716769218445\n",
            "[53,    31] loss: 9.882078504562378\n",
            "[53,    41] loss: 8.60948829650879\n",
            "[53,    51] loss: 9.75181794166565\n",
            "[53,    61] loss: 7.0800323486328125\n",
            "[53,    71] loss: 8.054884147644042\n",
            "[53,    81] loss: 6.7890049576759335\n",
            "[53,    91] loss: 9.390984165668488\n",
            "[53,   101] loss: 8.61683918237686\n",
            "[53,   111] loss: 9.033765411376953\n",
            "[53,   121] loss: 7.327535152435303\n",
            "[53,   131] loss: 8.267078065872193\n",
            "[54,     1] loss: 0.8470480918884278\n",
            "[54,    11] loss: 5.790758419036865\n",
            "[54,    21] loss: 10.162674570083619\n",
            "[54,    31] loss: 6.973231399059296\n",
            "[54,    41] loss: 7.343706312356517\n",
            "[54,    51] loss: 6.269795942306518\n",
            "[54,    61] loss: 8.758207941055298\n",
            "[54,    71] loss: 9.604939770698547\n",
            "[54,    81] loss: 9.688295650482178\n",
            "[54,    91] loss: 8.343295741081239\n",
            "[54,   101] loss: 10.795220756530762\n",
            "[54,   111] loss: 8.352013897895812\n",
            "[54,   121] loss: 8.76645860671997\n",
            "[54,   131] loss: 10.202807331085205\n",
            "[55,     1] loss: 1.1694984436035156\n",
            "[55,    11] loss: 8.029325866699219\n",
            "[55,    21] loss: 9.006084525585175\n",
            "[55,    31] loss: 8.455537116527557\n",
            "[55,    41] loss: 8.756443452835082\n",
            "[55,    51] loss: 9.89253146648407\n",
            "[55,    61] loss: 9.042187166213989\n",
            "[55,    71] loss: 7.229562664031983\n",
            "[55,    81] loss: 9.131330871582032\n",
            "[55,    91] loss: 10.346053504943848\n",
            "[55,   101] loss: 8.010195501800627\n",
            "[55,   111] loss: 7.668520760536194\n",
            "[55,   121] loss: 7.010619854927063\n",
            "[55,   131] loss: 10.029838418960571\n",
            "[56,     1] loss: 0.32977092266082764\n",
            "[56,    11] loss: 10.053953433036805\n",
            "[56,    21] loss: 9.591461229324342\n",
            "[56,    31] loss: 8.39727599620819\n",
            "[56,    41] loss: 9.006572353839875\n",
            "[56,    51] loss: 7.186134231090546\n",
            "[56,    61] loss: 8.611357498168946\n",
            "[56,    71] loss: 9.142344570159912\n",
            "[56,    81] loss: 8.909401202201844\n",
            "[56,    91] loss: 8.622201585769654\n",
            "[56,   101] loss: 8.383133792877198\n",
            "[56,   111] loss: 9.603144097328187\n",
            "[56,   121] loss: 8.840798616409302\n",
            "[56,   131] loss: 8.206604075431823\n",
            "[57,     1] loss: 0.5577239036560059\n",
            "[57,    11] loss: 8.761153173446655\n",
            "[57,    21] loss: 7.502659571170807\n",
            "[57,    31] loss: 10.30960783958435\n",
            "[57,    41] loss: 8.953222227096557\n",
            "[57,    51] loss: 8.270020604133606\n",
            "[57,    61] loss: 10.072580289840698\n",
            "[57,    71] loss: 9.273217582702637\n",
            "[57,    81] loss: 6.322119891643524\n",
            "[57,    91] loss: 6.8041986733675\n",
            "[57,   101] loss: 10.407150745391846\n",
            "[57,   111] loss: 10.22429542541504\n",
            "[57,   121] loss: 5.775648856163025\n",
            "[57,   131] loss: 7.647525453567505\n",
            "[58,     1] loss: 1.3632575035095216\n",
            "[58,    11] loss: 7.528167080879212\n",
            "[58,    21] loss: 8.70441400706768\n",
            "[58,    31] loss: 10.78521761894226\n",
            "[58,    41] loss: 8.165418004989624\n",
            "[58,    51] loss: 12.204050970077514\n",
            "[58,    61] loss: 7.602547883987427\n",
            "[58,    71] loss: 9.822542738914489\n",
            "[58,    81] loss: 8.455225133895874\n",
            "[58,    91] loss: 8.173541414737702\n",
            "[58,   101] loss: 6.702589011192321\n",
            "[58,   111] loss: 9.112793064117431\n",
            "[58,   121] loss: 7.604520273208618\n",
            "[58,   131] loss: 7.3052469158545135\n",
            "[59,     1] loss: 1.0768149375915528\n",
            "[59,    11] loss: 8.591429853439331\n",
            "[59,    21] loss: 8.15139844417572\n",
            "[59,    31] loss: 8.364810943603516\n",
            "[59,    41] loss: 8.544762301445008\n",
            "[59,    51] loss: 9.11913765668869\n",
            "[59,    61] loss: 9.33892755508423\n",
            "[59,    71] loss: 9.57601752281189\n",
            "[59,    81] loss: 9.901403450965882\n",
            "[59,    91] loss: 7.881127524375915\n",
            "[59,   101] loss: 10.274141216278077\n",
            "[59,   111] loss: 8.237526512145996\n",
            "[59,   121] loss: 7.582896971702576\n",
            "[59,   131] loss: 9.863259845972062\n",
            "[60,     1] loss: 0.8777351379394531\n",
            "[60,    11] loss: 8.461217105388641\n",
            "[60,    21] loss: 10.519428300857545\n",
            "[60,    31] loss: 9.253207015991212\n",
            "[60,    41] loss: 10.404087162017822\n",
            "[60,    51] loss: 8.252255523204804\n",
            "[60,    61] loss: 9.071920037269592\n",
            "[60,    71] loss: 8.625282573699952\n",
            "[60,    81] loss: 8.106852197647095\n",
            "[60,    91] loss: 7.651998507976532\n",
            "[60,   101] loss: 8.377279806137086\n",
            "[60,   111] loss: 8.210069751739502\n",
            "[60,   121] loss: 7.960241985321045\n",
            "[60,   131] loss: 9.737818145751953\n",
            "[61,     1] loss: 0.605036449432373\n",
            "[61,    11] loss: 9.183040571212768\n",
            "[61,    21] loss: 8.762767791748047\n",
            "[61,    31] loss: 8.164415264129639\n",
            "[61,    41] loss: 9.953351283073426\n",
            "[61,    51] loss: 7.452886784076691\n",
            "[61,    61] loss: 8.089783430099487\n",
            "[61,    71] loss: 10.031732320785522\n",
            "[61,    81] loss: 10.286483764648438\n",
            "[61,    91] loss: 7.718671464920044\n",
            "[61,   101] loss: 8.994875049591064\n",
            "[61,   111] loss: 9.501694202423096\n",
            "[61,   121] loss: 8.587002515792847\n",
            "[61,   131] loss: 6.646329593658447\n",
            "[62,     1] loss: 0.6133075714111328\n",
            "[62,    11] loss: 7.172618722915649\n",
            "[62,    21] loss: 7.2377671003341675\n",
            "[62,    31] loss: 10.343745636940003\n",
            "[62,    41] loss: 10.137751007080078\n",
            "[62,    51] loss: 10.610396528244019\n",
            "[62,    61] loss: 9.774991273880005\n",
            "[62,    71] loss: 8.513447308540345\n",
            "[62,    81] loss: 8.886487519741058\n",
            "[62,    91] loss: 7.414481854438781\n",
            "[62,   101] loss: 5.784279775712639\n",
            "[62,   111] loss: 10.299521160125732\n",
            "[62,   121] loss: 7.664327096939087\n",
            "[62,   131] loss: 8.775004529953003\n",
            "[63,     1] loss: 0.3806262254714966\n",
            "[63,    11] loss: 8.633508098125457\n",
            "[63,    21] loss: 8.289957761764526\n",
            "[63,    31] loss: 10.44719500541687\n",
            "[63,    41] loss: 9.979201844334602\n",
            "[63,    51] loss: 9.890142631530761\n",
            "[63,    61] loss: 9.74695520401001\n",
            "[63,    71] loss: 7.1696113586425785\n",
            "[63,    81] loss: 7.675160384178161\n",
            "[63,    91] loss: 8.63362077474594\n",
            "[63,   101] loss: 7.708626127243042\n",
            "[63,   111] loss: 9.16435809135437\n",
            "[63,   121] loss: 8.690123796463013\n",
            "[63,   131] loss: 8.978930878639222\n",
            "[64,     1] loss: 0.8220890998840332\n",
            "[64,    11] loss: 7.212756979465484\n",
            "[64,    21] loss: 7.5245789051055905\n",
            "[64,    31] loss: 7.451446127891541\n",
            "[64,    41] loss: 6.543471704423427\n",
            "[64,    51] loss: 8.597542786598206\n",
            "[64,    61] loss: 8.9572744846344\n",
            "[64,    71] loss: 10.050066876411439\n",
            "[64,    81] loss: 10.082639753818512\n",
            "[64,    91] loss: 9.967425179481506\n",
            "[64,   101] loss: 7.921573829650879\n",
            "[64,   111] loss: 10.095175004005432\n",
            "[64,   121] loss: 8.203985285758971\n",
            "[64,   131] loss: 10.180874609947205\n",
            "[65,     1] loss: 0.8048929214477539\n",
            "[65,    11] loss: 7.190130233764648\n",
            "[65,    21] loss: 8.065733528137207\n",
            "[65,    31] loss: 7.827557754516602\n",
            "[65,    41] loss: 8.594095993041993\n",
            "[65,    51] loss: 8.815645265579224\n",
            "[65,    61] loss: 8.910765588283539\n",
            "[65,    71] loss: 6.953705531358719\n",
            "[65,    81] loss: 9.513304045051337\n",
            "[65,    91] loss: 8.38342833518982\n",
            "[65,   101] loss: 9.037962007522584\n",
            "[65,   111] loss: 9.975218892097473\n",
            "[65,   121] loss: 9.256437385082245\n",
            "[65,   131] loss: 7.545292687416077\n",
            "[66,     1] loss: 0.5554737091064453\n",
            "[66,    11] loss: 7.442168521881103\n",
            "[66,    21] loss: 7.522958257980645\n",
            "[66,    31] loss: 8.72262064218521\n",
            "[66,    41] loss: 8.111452995240688\n",
            "[66,    51] loss: 10.441458654403686\n",
            "[66,    61] loss: 11.08782320022583\n",
            "[66,    71] loss: 7.69584299325943\n",
            "[66,    81] loss: 7.944471144676209\n",
            "[66,    91] loss: 7.863812494277954\n",
            "[66,   101] loss: 6.763540315628052\n",
            "[66,   111] loss: 7.784904956817627\n",
            "[66,   121] loss: 7.250114234897774\n",
            "[66,   131] loss: 10.965211963653564\n",
            "[67,     1] loss: 1.0805413246154785\n",
            "[67,    11] loss: 5.124365651607514\n",
            "[67,    21] loss: 7.871360039710998\n",
            "[67,    31] loss: 9.91621799468994\n",
            "[67,    41] loss: 6.568707084655761\n",
            "[67,    51] loss: 8.647687435150146\n",
            "[67,    61] loss: 6.735754227638244\n",
            "[67,    71] loss: 11.408126926422119\n",
            "[67,    81] loss: 8.343153238296509\n",
            "[67,    91] loss: 8.16610827445984\n",
            "[67,   101] loss: 7.6490593433380125\n",
            "[67,   111] loss: 9.63849720954895\n",
            "[67,   121] loss: 9.690306758880615\n",
            "[67,   131] loss: 11.161090230941772\n",
            "[68,     1] loss: 1.668857955932617\n",
            "[68,    11] loss: 6.977422022819519\n",
            "[68,    21] loss: 8.359890246391297\n",
            "[68,    31] loss: 7.977024257183075\n",
            "[68,    41] loss: 10.901008105278015\n",
            "[68,    51] loss: 7.433735862374306\n",
            "[68,    61] loss: 9.517175352573394\n",
            "[68,    71] loss: 7.455674839019776\n",
            "[68,    81] loss: 9.096087741851807\n",
            "[68,    91] loss: 8.336697721481324\n",
            "[68,   101] loss: 8.007890224456787\n",
            "[68,   111] loss: 9.60240155160427\n",
            "[68,   121] loss: 12.224656295776366\n",
            "[68,   131] loss: 6.14461110830307\n",
            "[69,     1] loss: 1.1657649993896484\n",
            "[69,    11] loss: 7.861206817626953\n",
            "[69,    21] loss: 8.161632657051086\n",
            "[69,    31] loss: 9.243985462188721\n",
            "[69,    41] loss: 8.42709379196167\n",
            "[69,    51] loss: 8.22597770690918\n",
            "[69,    61] loss: 9.402591800689697\n",
            "[69,    71] loss: 10.381630325317383\n",
            "[69,    81] loss: 8.598017072677612\n",
            "[69,    91] loss: 9.94837064743042\n",
            "[69,   101] loss: 10.257749128341676\n",
            "[69,   111] loss: 6.581567858159542\n",
            "[69,   121] loss: 8.520127058029175\n",
            "[69,   131] loss: 7.168637132644653\n",
            "[70,     1] loss: 1.11500825881958\n",
            "[70,    11] loss: 7.308193325996399\n",
            "[70,    21] loss: 8.979874777793885\n",
            "[70,    31] loss: 8.557244443893433\n",
            "[70,    41] loss: 7.683280754089355\n",
            "[70,    51] loss: 7.966854524612427\n",
            "[70,    61] loss: 9.122721791267395\n",
            "[70,    71] loss: 9.977924728393555\n",
            "[70,    81] loss: 9.445002698898316\n",
            "[70,    91] loss: 6.8254575490951535\n",
            "[70,   101] loss: 6.559156179428101\n",
            "[70,   111] loss: 7.684970116615295\n",
            "[70,   121] loss: 10.029942512512207\n",
            "[70,   131] loss: 9.069940888881684\n",
            "[71,     1] loss: 1.1543032646179199\n",
            "[71,    11] loss: 8.408487820625306\n",
            "[71,    21] loss: 8.234279738180339\n",
            "[71,    31] loss: 8.495419764518738\n",
            "[71,    41] loss: 7.28143527507782\n",
            "[71,    51] loss: 7.995423173904419\n",
            "[71,    61] loss: 8.846005368232728\n",
            "[71,    71] loss: 9.237072944641113\n",
            "[71,    81] loss: 7.647525787353516\n",
            "[71,    91] loss: 8.81289267539978\n",
            "[71,   101] loss: 11.389735126495362\n",
            "[71,   111] loss: 8.288324069976806\n",
            "[71,   121] loss: 7.683999490737915\n",
            "[71,   131] loss: 9.245346450805664\n",
            "[72,     1] loss: 1.3628429412841796\n",
            "[72,    11] loss: 6.08147908449173\n",
            "[72,    21] loss: 8.7932834982872\n",
            "[72,    31] loss: 9.431063294410706\n",
            "[72,    41] loss: 9.805258417129517\n",
            "[72,    51] loss: 7.766389179229736\n",
            "[72,    61] loss: 8.711013126373292\n",
            "[72,    71] loss: 9.408987140655517\n",
            "[72,    81] loss: 8.574494862556458\n",
            "[72,    91] loss: 10.101762700080872\n",
            "[72,   101] loss: 10.282529592514038\n",
            "[72,   111] loss: 7.450432157516479\n",
            "[72,   121] loss: 7.936388826370239\n",
            "[72,   131] loss: 7.341820406913757\n",
            "[73,     1] loss: 0.6037095069885254\n",
            "[73,    11] loss: 8.006132221221923\n",
            "[73,    21] loss: 8.280926263332367\n",
            "[73,    31] loss: 7.626441240310669\n",
            "[73,    41] loss: 9.632050251960754\n",
            "[73,    51] loss: 9.151616883277892\n",
            "[73,    61] loss: 6.833602005243302\n",
            "[73,    71] loss: 7.44450159072876\n",
            "[73,    81] loss: 9.551036167144776\n",
            "[73,    91] loss: 7.441119730472565\n",
            "[73,   101] loss: 8.142100441455842\n",
            "[73,   111] loss: 8.426928734779358\n",
            "[73,   121] loss: 12.52300567626953\n",
            "[73,   131] loss: 8.563820481300354\n",
            "[74,     1] loss: 0.8135993003845214\n",
            "[74,    11] loss: 11.001986742019653\n",
            "[74,    21] loss: 9.365447878837585\n",
            "[74,    31] loss: 9.314667630195618\n",
            "[74,    41] loss: 9.046227049827575\n",
            "[74,    51] loss: 9.142207503318787\n",
            "[74,    61] loss: 7.4886651039123535\n",
            "[74,    71] loss: 9.04713523387909\n",
            "[74,    81] loss: 9.077204966545105\n",
            "[74,    91] loss: 7.737761163711548\n",
            "[74,   101] loss: 8.610625624656677\n",
            "[74,   111] loss: 7.777106595039368\n",
            "[74,   121] loss: 9.461645233631135\n",
            "[74,   131] loss: 7.869457173347473\n",
            "[75,     1] loss: 0.5588923454284668\n",
            "[75,    11] loss: 8.211762285232544\n",
            "[75,    21] loss: 10.621091413497926\n",
            "[75,    31] loss: 8.58161529302597\n",
            "[75,    41] loss: 8.48560290336609\n",
            "[75,    51] loss: 10.01208953857422\n",
            "[75,    61] loss: 8.25511634349823\n",
            "[75,    71] loss: 7.854605364799499\n",
            "[75,    81] loss: 7.170827782154083\n",
            "[75,    91] loss: 7.6623430252075195\n",
            "[75,   101] loss: 11.654493236541748\n",
            "[75,   111] loss: 10.906191539764404\n",
            "[75,   121] loss: 8.485747408866882\n",
            "[75,   131] loss: 6.966342163085938\n",
            "[76,     1] loss: 1.1226943969726562\n",
            "[76,    11] loss: 7.4210120677948\n",
            "[76,    21] loss: 9.478351163864136\n",
            "[76,    31] loss: 7.757170867919922\n",
            "[76,    41] loss: 9.046398544311524\n",
            "[76,    51] loss: 7.044585657119751\n",
            "[76,    61] loss: 9.323509407043456\n",
            "[76,    71] loss: 9.16563572883606\n",
            "[76,    81] loss: 8.157977402210236\n",
            "[76,    91] loss: 7.565515458583832\n",
            "[76,   101] loss: 9.971306586265564\n",
            "[76,   111] loss: 11.492343139648437\n",
            "[76,   121] loss: 8.781026017665862\n",
            "[76,   131] loss: 5.66939360499382\n",
            "[77,     1] loss: 0.5958292484283447\n",
            "[77,    11] loss: 10.132177472114563\n",
            "[77,    21] loss: 8.651757788658141\n",
            "[77,    31] loss: 8.745951986312866\n",
            "[77,    41] loss: 8.257900667190551\n",
            "[77,    51] loss: 6.19283013343811\n",
            "[77,    61] loss: 9.919255757331848\n",
            "[77,    71] loss: 7.411029386520386\n",
            "[77,    81] loss: 7.924990272521972\n",
            "[77,    91] loss: 9.733766436576843\n",
            "[77,   101] loss: 8.918948554992676\n",
            "[77,   111] loss: 9.134025573730469\n",
            "[77,   121] loss: 10.128110122680663\n",
            "[77,   131] loss: 8.086137771606445\n",
            "[78,     1] loss: 0.9346097946166992\n",
            "[78,    11] loss: 7.942353773117065\n",
            "[78,    21] loss: 8.99545783996582\n",
            "[78,    31] loss: 11.160336589813232\n",
            "[78,    41] loss: 7.358889436721801\n",
            "[78,    51] loss: 8.466325974464416\n",
            "[78,    61] loss: 6.6946409940719604\n",
            "[78,    71] loss: 9.847119998931884\n",
            "[78,    81] loss: 9.420195579528809\n",
            "[78,    91] loss: 10.60126452445984\n",
            "[78,   101] loss: 9.30968017578125\n",
            "[78,   111] loss: 6.603152322769165\n",
            "[78,   121] loss: 10.384845638275147\n",
            "[78,   131] loss: 9.377717590332031\n",
            "[79,     1] loss: 0.35445756912231446\n",
            "[79,    11] loss: 8.34338345527649\n",
            "[79,    21] loss: 8.296277046203613\n",
            "[79,    31] loss: 8.873741364479065\n",
            "[79,    41] loss: 9.229697608947754\n",
            "[79,    51] loss: 8.379083049297332\n",
            "[79,    61] loss: 8.59096565246582\n",
            "[79,    71] loss: 7.783220818499103\n",
            "[79,    81] loss: 10.61917884349823\n",
            "[79,    91] loss: 7.734337019920349\n",
            "[79,   101] loss: 8.21993613243103\n",
            "[79,   111] loss: 7.169889813056216\n",
            "[79,   121] loss: 11.2198016166687\n",
            "[79,   131] loss: 9.586208820343018\n",
            "[80,     1] loss: 0.6172390937805176\n",
            "[80,    11] loss: 9.142672729492187\n",
            "[80,    21] loss: 6.296585822105408\n",
            "[80,    31] loss: 6.589027228951454\n",
            "[80,    41] loss: 7.6592793464660645\n",
            "[80,    51] loss: 7.75760440826416\n",
            "[80,    61] loss: 10.527689027786256\n",
            "[80,    71] loss: 9.070850038528443\n",
            "[80,    81] loss: 8.800406098365784\n",
            "[80,    91] loss: 9.710621309280395\n",
            "[80,   101] loss: 7.541607785224914\n",
            "[80,   111] loss: 9.273124957084656\n",
            "[80,   121] loss: 11.680130338668823\n",
            "[80,   131] loss: 7.06518440246582\n",
            "[81,     1] loss: 0.8062375068664551\n",
            "[81,    11] loss: 9.402177143096925\n",
            "[81,    21] loss: 7.634857559204102\n",
            "[81,    31] loss: 8.750035500526428\n",
            "[81,    41] loss: 6.953107738494873\n",
            "[81,    51] loss: 10.04194552898407\n",
            "[81,    61] loss: 9.348542499542237\n",
            "[81,    71] loss: 9.277631807327271\n",
            "[81,    81] loss: 9.030395913310349\n",
            "[81,    91] loss: 9.177057528495789\n",
            "[81,   101] loss: 8.09277410507202\n",
            "[81,   111] loss: 6.04971559047699\n",
            "[81,   121] loss: 6.087666249275207\n",
            "[81,   131] loss: 10.908078527450561\n",
            "[82,     1] loss: 0.6585063934326172\n",
            "[82,    11] loss: 8.697692275047302\n",
            "[82,    21] loss: 6.810823917388916\n",
            "[82,    31] loss: 7.319405994564295\n",
            "[82,    41] loss: 11.355547857284545\n",
            "[82,    51] loss: 8.741759157180786\n",
            "[82,    61] loss: 8.016474103927612\n",
            "[82,    71] loss: 9.099769353866577\n",
            "[82,    81] loss: 9.272391319274902\n",
            "[82,    91] loss: 9.519133377075196\n",
            "[82,   101] loss: 7.055647301673889\n",
            "[82,   111] loss: 9.124493169784547\n",
            "[82,   121] loss: 9.82082395553589\n",
            "[82,   131] loss: 5.935338228940964\n",
            "[83,     1] loss: 1.3657915115356445\n",
            "[83,    11] loss: 10.429162120819091\n",
            "[83,    21] loss: 7.853492546081543\n",
            "[83,    31] loss: 7.600561499595642\n",
            "[83,    41] loss: 7.381538009643554\n",
            "[83,    51] loss: 9.45664439201355\n",
            "[83,    61] loss: 7.161909866333008\n",
            "[83,    71] loss: 8.355060648918151\n",
            "[83,    81] loss: 8.531329870223999\n",
            "[83,    91] loss: 10.526604342460633\n",
            "[83,   101] loss: 7.840782213211059\n",
            "[83,   111] loss: 7.65678619146347\n",
            "[83,   121] loss: 6.6836306810379025\n",
            "[83,   131] loss: 10.628684544563294\n",
            "[84,     1] loss: 0.8079008102416992\n",
            "[84,    11] loss: 9.162706995010376\n",
            "[84,    21] loss: 8.898736667633056\n",
            "[84,    31] loss: 7.744115233421326\n",
            "[84,    41] loss: 8.727655094861984\n",
            "[84,    51] loss: 8.385050439834595\n",
            "[84,    61] loss: 7.6351360321044925\n",
            "[84,    71] loss: 10.7066330909729\n",
            "[84,    81] loss: 8.559552937746048\n",
            "[84,    91] loss: 8.0956627368927\n",
            "[84,   101] loss: 7.50286762714386\n",
            "[84,   111] loss: 9.777479553222657\n",
            "[84,   121] loss: 7.991981822770322\n",
            "[84,   131] loss: 7.632558875821997\n",
            "[85,     1] loss: 0.33141722679138186\n",
            "[85,    11] loss: 7.974385356903076\n",
            "[85,    21] loss: 9.168608951568604\n",
            "[85,    31] loss: 7.140098249912262\n",
            "[85,    41] loss: 11.4304518699646\n",
            "[85,    51] loss: 11.516875505447388\n",
            "[85,    61] loss: 9.67669243812561\n",
            "[85,    71] loss: 9.294422006607055\n",
            "[85,    81] loss: 9.893467330932618\n",
            "[85,    91] loss: 7.279950645565987\n",
            "[85,   101] loss: 6.359387600421906\n",
            "[85,   111] loss: 7.6899990916252134\n",
            "[85,   121] loss: 6.666697001457214\n",
            "[85,   131] loss: 7.955729198455811\n",
            "[86,     1] loss: 1.0786518096923827\n",
            "[86,    11] loss: 10.055243110656738\n",
            "[86,    21] loss: 9.295132994651794\n",
            "[86,    31] loss: 9.796604108810424\n",
            "[86,    41] loss: 10.783416414260865\n",
            "[86,    51] loss: 7.406100630760193\n",
            "[86,    61] loss: 5.581368535757065\n",
            "[86,    71] loss: 7.432000362873078\n",
            "[86,    81] loss: 6.677757024765015\n",
            "[86,    91] loss: 8.423880028724671\n",
            "[86,   101] loss: 8.505994641780854\n",
            "[86,   111] loss: 9.928397750854492\n",
            "[86,   121] loss: 8.543874299526214\n",
            "[86,   131] loss: 7.48353157043457\n",
            "[87,     1] loss: 1.0761445045471192\n",
            "[87,    11] loss: 9.024112460017204\n",
            "[87,    21] loss: 7.520928120613098\n",
            "[87,    31] loss: 9.265271818637848\n",
            "[87,    41] loss: 7.59260311126709\n",
            "[87,    51] loss: 8.065869808197021\n",
            "[87,    61] loss: 7.903728246688843\n",
            "[87,    71] loss: 7.815899324417114\n",
            "[87,    81] loss: 6.783606696128845\n",
            "[87,    91] loss: 8.52819184064865\n",
            "[87,   101] loss: 9.227729320526123\n",
            "[87,   111] loss: 9.16779808998108\n",
            "[87,   121] loss: 10.707651042938233\n",
            "[87,   131] loss: 7.792737364768982\n",
            "[88,     1] loss: 1.076396369934082\n",
            "[88,    11] loss: 8.357971048355102\n",
            "[88,    21] loss: 7.56810364946723\n",
            "[88,    31] loss: 8.507731139659882\n",
            "[88,    41] loss: 9.447515225410461\n",
            "[88,    51] loss: 10.054392671585083\n",
            "[88,    61] loss: 8.415781497955322\n",
            "[88,    71] loss: 8.710777950286865\n",
            "[88,    81] loss: 7.007930755615234\n",
            "[88,    91] loss: 8.48482575416565\n",
            "[88,   101] loss: 8.011129879951477\n",
            "[88,   111] loss: 7.362671407684684\n",
            "[88,   121] loss: 7.826770329475403\n",
            "[88,   131] loss: 9.999316072463989\n",
            "[89,     1] loss: 0.8079751014709473\n",
            "[89,    11] loss: 8.619025254249573\n",
            "[89,    21] loss: 8.0193421959877\n",
            "[89,    31] loss: 9.297708630561829\n",
            "[89,    41] loss: 7.896993660926819\n",
            "[89,    51] loss: 6.6785061120986935\n",
            "[89,    61] loss: 6.927013850212097\n",
            "[89,    71] loss: 8.353278732299804\n",
            "[89,    81] loss: 9.89064195093233\n",
            "[89,    91] loss: 10.929337787628175\n",
            "[89,   101] loss: 10.78950023651123\n",
            "[89,   111] loss: 10.273422265052796\n",
            "[89,   121] loss: 9.503743958473205\n",
            "[89,   131] loss: 7.226808750629425\n",
            "[90,     1] loss: 0.07579477429389954\n",
            "[90,    11] loss: 9.655704855918884\n",
            "[90,    21] loss: 11.993261742591859\n",
            "[90,    31] loss: 6.628252410888672\n",
            "[90,    41] loss: 6.1265113115310665\n",
            "[90,    51] loss: 11.891457414627075\n",
            "[90,    61] loss: 7.277404344081878\n",
            "[90,    71] loss: 11.338837337493896\n",
            "[90,    81] loss: 8.220768260955811\n",
            "[90,    91] loss: 10.324153518676757\n",
            "[90,   101] loss: 8.232008075714111\n",
            "[90,   111] loss: 8.238376474380493\n",
            "[90,   121] loss: 8.630428576469422\n",
            "[90,   131] loss: 8.380631446838379\n",
            "[91,     1] loss: 1.1664596557617188\n",
            "[91,    11] loss: 9.219224691390991\n",
            "[91,    21] loss: 8.395532369613647\n",
            "[91,    31] loss: 9.604139804840088\n",
            "[91,    41] loss: 10.019063091278076\n",
            "[91,    51] loss: 5.141176629066467\n",
            "[91,    61] loss: 9.921346378326415\n",
            "[91,    71] loss: 5.418523478507995\n",
            "[91,    81] loss: 8.769304180145264\n",
            "[91,    91] loss: 10.239076614379883\n",
            "[91,   101] loss: 8.878240776062011\n",
            "[91,   111] loss: 10.329760909080505\n",
            "[91,   121] loss: 8.499045705795288\n",
            "[91,   131] loss: 6.710051953792572\n",
            "[92,     1] loss: 1.077256202697754\n",
            "[92,    11] loss: 5.110467466711998\n",
            "[92,    21] loss: 11.812908554077149\n",
            "[92,    31] loss: 8.709647798538208\n",
            "[92,    41] loss: 8.37839584350586\n",
            "[92,    51] loss: 7.907653021812439\n",
            "[92,    61] loss: 9.695879650115966\n",
            "[92,    71] loss: 8.128168642520905\n",
            "[92,    81] loss: 8.725593328475952\n",
            "[92,    91] loss: 6.038889373093843\n",
            "[92,   101] loss: 7.968538284301758\n",
            "[92,   111] loss: 11.399991178512574\n",
            "[92,   121] loss: 9.646263146400452\n",
            "[92,   131] loss: 10.092875385284424\n",
            "[93,     1] loss: 0.11287803649902343\n",
            "[93,    11] loss: 8.108241033554076\n",
            "[93,    21] loss: 7.108694124221802\n",
            "[93,    31] loss: 7.74741518497467\n",
            "[93,    41] loss: 8.747456789016724\n",
            "[93,    51] loss: 8.600746297836304\n",
            "[93,    61] loss: 11.160083627700805\n",
            "[93,    71] loss: 9.392373669147492\n",
            "[93,    81] loss: 6.7509832859039305\n",
            "[93,    91] loss: 8.313374996185303\n",
            "[93,   101] loss: 10.798188877105712\n",
            "[93,   111] loss: 10.542466640472412\n",
            "[93,   121] loss: 7.291582989692688\n",
            "[93,   131] loss: 7.4027865648269655\n",
            "[94,     1] loss: 1.3651133537292481\n",
            "[94,    11] loss: 6.5598094940185545\n",
            "[94,    21] loss: 8.445241832733155\n",
            "[94,    31] loss: 8.891194891929626\n",
            "[94,    41] loss: 7.49454734721221\n",
            "[94,    51] loss: 6.019103646278381\n",
            "[94,    61] loss: 7.728719997406006\n",
            "[94,    71] loss: 9.571994519233703\n",
            "[94,    81] loss: 7.411331725120545\n",
            "[94,    91] loss: 11.290339040756226\n",
            "[94,   101] loss: 9.814989280700683\n",
            "[94,   111] loss: 10.247189712524413\n",
            "[94,   121] loss: 8.247036170959472\n",
            "[94,   131] loss: 8.712707042694092\n",
            "[95,     1] loss: 0.6011532783508301\n",
            "[95,    11] loss: 8.45496644973755\n",
            "[95,    21] loss: 8.385103511810303\n",
            "[95,    31] loss: 9.854318952560424\n",
            "[95,    41] loss: 8.265647447109222\n",
            "[95,    51] loss: 8.114906191825867\n",
            "[95,    61] loss: 8.401689910888672\n",
            "[95,    71] loss: 8.649860000610351\n",
            "[95,    81] loss: 5.556880813837052\n",
            "[95,    91] loss: 9.41732702255249\n",
            "[95,   101] loss: 8.65735321044922\n",
            "[95,   111] loss: 8.084148383140564\n",
            "[95,   121] loss: 8.618555784225464\n",
            "[95,   131] loss: 8.401054763793946\n",
            "[96,     1] loss: 0.5564263343811036\n",
            "[96,    11] loss: 7.6779743194580075\n",
            "[96,    21] loss: 10.133376216888427\n",
            "[96,    31] loss: 10.097417402267457\n",
            "[96,    41] loss: 6.362638002634048\n",
            "[96,    51] loss: 8.475897932052613\n",
            "[96,    61] loss: 8.204991579055786\n",
            "[96,    71] loss: 9.58571412563324\n",
            "[96,    81] loss: 8.643230056762695\n",
            "[96,    91] loss: 8.785835075378419\n",
            "[96,   101] loss: 9.097333717346192\n",
            "[96,   111] loss: 9.628882598876952\n",
            "[96,   121] loss: 7.993366784229875\n",
            "[96,   131] loss: 6.890132716298103\n",
            "[97,     1] loss: 1.0754756927490234\n",
            "[97,    11] loss: 8.176009595394135\n",
            "[97,    21] loss: 8.272115898132324\n",
            "[97,    31] loss: 7.738138151168823\n",
            "[97,    41] loss: 8.678306126594544\n",
            "[97,    51] loss: 9.398550081253052\n",
            "[97,    61] loss: 6.755553436279297\n",
            "[97,    71] loss: 7.975377678871155\n",
            "[97,    81] loss: 10.340511798858643\n",
            "[97,    91] loss: 9.72208251953125\n",
            "[97,   101] loss: 8.853710889816284\n",
            "[97,   111] loss: 8.446167826652527\n",
            "[97,   121] loss: 8.055985081195832\n",
            "[97,   131] loss: 8.468235945701599\n",
            "[98,     1] loss: 0.5548888683319092\n",
            "[98,    11] loss: 8.165794372558594\n",
            "[98,    21] loss: 7.140228819847107\n",
            "[98,    31] loss: 8.726652979850769\n",
            "[98,    41] loss: 8.031201434135436\n",
            "[98,    51] loss: 8.90269169807434\n",
            "[98,    61] loss: 9.709943461418153\n",
            "[98,    71] loss: 8.63257393836975\n",
            "[98,    81] loss: 8.421638822555542\n",
            "[98,    91] loss: 11.283157920837402\n",
            "[98,   101] loss: 6.17220174074173\n",
            "[98,   111] loss: 9.193758344650268\n",
            "[98,   121] loss: 6.9852697144960985\n",
            "[98,   131] loss: 9.387491178512573\n",
            "[99,     1] loss: 0.33258984088897703\n",
            "[99,    11] loss: 8.633409547805787\n",
            "[99,    21] loss: 9.523130226135255\n",
            "[99,    31] loss: 8.133088970184327\n",
            "[99,    41] loss: 7.424109933525324\n",
            "[99,    51] loss: 10.597989797592163\n",
            "[99,    61] loss: 8.270252084732055\n",
            "[99,    71] loss: 5.932197988033295\n",
            "[99,    81] loss: 8.172801327705383\n",
            "[99,    91] loss: 7.984277791343629\n",
            "[99,   101] loss: 9.980157256126404\n",
            "[99,   111] loss: 9.356872606277467\n",
            "[99,   121] loss: 8.224591326713561\n",
            "[99,   131] loss: 10.176190757751465\n",
            "[100,     1] loss: 0.8073453903198242\n",
            "[100,    11] loss: 9.572447109222413\n",
            "[100,    21] loss: 7.577880585193634\n",
            "[100,    31] loss: 7.6225687026977536\n",
            "[100,    41] loss: 9.821833276748658\n",
            "[100,    51] loss: 10.231401801109314\n",
            "[100,    61] loss: 8.915116596221925\n",
            "[100,    71] loss: 7.7709526181221005\n",
            "[100,    81] loss: 7.765242004394532\n",
            "[100,    91] loss: 9.446647000312804\n",
            "[100,   101] loss: 6.6587073564529415\n",
            "[100,   111] loss: 8.148120393231512\n",
            "[100,   121] loss: 8.456685614585876\n",
            "[100,   131] loss: 8.583314251899719\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(outputs,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test : {100 * correct // total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjjcoqJLoqDC",
        "outputId": "30504dd2-b33f-4c2f-c3ce-42d01d255944"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test : 50 %\n"
          ]
        }
      ]
    }
  ]
}