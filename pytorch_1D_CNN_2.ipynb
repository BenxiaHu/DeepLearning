{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS8lC1sT/2x2qsegSdhq4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "4c904c31-43fa-4c57-f259-c9d5fa262be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd0ec6f2dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506f2fdd-1143-496c-ceff-6cd6f37cf235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\n",
            "        chr      start        end  silencer                     peakid\n",
            "0      chr1  113408175  113408375         1   chr1:113408175-113408375\n",
            "1     chr10    6107715    6107915         1      chr10:6107715-6107915\n",
            "2     chr10   15538675   15538875         1    chr10:15538675-15538875\n",
            "3     chr11  114339756  114339956         1  chr11:114339756-114339956\n",
            "4     chr17   41121376   41121576         1    chr17:41121376-41121576\n",
            "...     ...        ...        ...       ...                        ...\n",
            "7227   chrX   93929096   93929296         0     chrX:93929096-93929296\n",
            "7228   chrX  101946395  101946595         0   chrX:101946395-101946595\n",
            "7229   chrX  130859105  130859305         0   chrX:130859105-130859305\n",
            "7230   chrX  131424075  131424275         0   chrX:131424075-131424275\n",
            "7231   chrX  141256436  141256636         0   chrX:141256436-141256636\n",
            "\n",
            "[7232 rows x 5 columns]\n",
            "                         peakid  silencer\n",
            "0      chr1:113408175-113408375         1\n",
            "1         chr10:6107715-6107915         1\n",
            "2       chr10:15538675-15538875         1\n",
            "3     chr11:114339756-114339956         1\n",
            "4       chr17:41121376-41121576         1\n",
            "...                         ...       ...\n",
            "7227     chrX:93929096-93929296         0\n",
            "7228   chrX:101946395-101946595         0\n",
            "7229   chrX:130859105-130859305         0\n",
            "7230   chrX:131424075-131424275         0\n",
            "7231   chrX:141256436-141256636         0\n",
            "\n",
            "[7232 rows x 2 columns]\n",
            "                              0  \\\n",
            "0         chr19:8631775-8631975   \n",
            "1      chr1:113408175-113408375   \n",
            "2      chr2:234276376-234276576   \n",
            "3       chr17:47401936-47402136   \n",
            "4       chr17:41121376-41121576   \n",
            "...                         ...   \n",
            "1995  chr11:110708376-110708576   \n",
            "1996   chr5:177389655-177389855   \n",
            "1997    chr18:44601576-44601776   \n",
            "1998     chr1:19873475-19873675   \n",
            "1999     chr7:72770095-72770295   \n",
            "\n",
            "                                                      1  \n",
            "0     tcttgaactcctgaatttcaagtaatcctcttgcttcagcctatga...  \n",
            "1     ttacaggcatgagccaccgcactcggccTTGTTAAACTTTTTTGTT...  \n",
            "2     AATGCTCTGTGCTCTACCGCATCCCACTTGGAGGTGGGTGTGTCCT...  \n",
            "3     agatgatgtttcactatgttggccaggctggtctcgaactcctgac...  \n",
            "4     GTAAACCAACACTACTTTGTCATACATCATTAGTTTCAGGACtttt...  \n",
            "...                                                 ...  \n",
            "1995  GCTTCCTTTATCTCTCTCCAGGCTCCAGCTTGGATGGTGGCCAACA...  \n",
            "1996  AGGGAGTGTGGCAGAGGGAATGATGGCCATGGTGACACTGAGGAGG...  \n",
            "1997  TTTTACTCTGGGAGCATCTGAGGACTTTAAGTCTATAAGCTTGAAC...  \n",
            "1998  gctgcagacagattagcagctagtaacccatcaaaatggatacaaa...  \n",
            "1999  aaaaaaGTGAGTGTATATGTAACTTGTCCACAAAAAGAATTATACA...  \n",
            "\n",
            "[2000 rows x 2 columns]\n",
            "(2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 10\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(3200, 128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.size())\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc2(x)\n",
        "        #out = torch.flatten(x,1)\n",
        "        #print(out)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "LR = 0.001 \n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 100              # train the training data n times, to save time, we just train 1 epoch\n",
        "LR = 0.001 \n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        j+=1\n",
        "\n",
        "    print(f'the loss on each epoch: {epoch}, loss: {running_loss/j}')\n",
        "    train_losses.append(running_loss/j)\n",
        "print('Finished Training')\n",
        "\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Let’s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zckte_qdea_1",
        "outputId": "63d35a9a-5d57-4ff8-cf06-ddfceb4d141e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the loss on each epoch: 0, loss: 30.09368532725743\n",
            "the loss on each epoch: 1, loss: 29.878518322535925\n",
            "the loss on each epoch: 2, loss: 29.663701438903807\n",
            "the loss on each epoch: 3, loss: 29.578824397495815\n",
            "the loss on each epoch: 4, loss: 29.425483812604632\n",
            "the loss on each epoch: 5, loss: 29.30907816205706\n",
            "the loss on each epoch: 6, loss: 29.256988865988596\n",
            "the loss on each epoch: 7, loss: 29.10717236655099\n",
            "the loss on each epoch: 8, loss: 29.07653487069266\n",
            "the loss on each epoch: 9, loss: 28.997638470785958\n",
            "the loss on each epoch: 10, loss: 28.908516883850098\n",
            "the loss on each epoch: 11, loss: 28.829364776611328\n",
            "the loss on each epoch: 12, loss: 28.65573877607073\n",
            "the loss on each epoch: 13, loss: 28.596840722220286\n",
            "the loss on each epoch: 14, loss: 28.60241707393101\n",
            "the loss on each epoch: 15, loss: 28.51485469681876\n",
            "the loss on each epoch: 16, loss: 28.42991636821202\n",
            "the loss on each epoch: 17, loss: 28.454259055001394\n",
            "the loss on each epoch: 18, loss: 28.275601632254464\n",
            "the loss on each epoch: 19, loss: 28.145193726675853\n",
            "the loss on each epoch: 20, loss: 28.125227246965682\n",
            "the loss on each epoch: 21, loss: 27.997789887019565\n",
            "the loss on each epoch: 22, loss: 27.935374423435757\n",
            "the loss on each epoch: 23, loss: 27.93023772920881\n",
            "the loss on each epoch: 24, loss: 27.877794919695173\n",
            "the loss on each epoch: 25, loss: 27.847313690185548\n",
            "the loss on each epoch: 26, loss: 27.72239261354719\n",
            "the loss on each epoch: 27, loss: 27.643066215515137\n",
            "the loss on each epoch: 28, loss: 27.6386871610369\n",
            "the loss on each epoch: 29, loss: 27.534229360307965\n",
            "the loss on each epoch: 30, loss: 27.46203331266131\n",
            "the loss on each epoch: 31, loss: 27.467907306126186\n",
            "the loss on each epoch: 32, loss: 27.30690304892404\n",
            "the loss on each epoch: 33, loss: 27.276116493770054\n",
            "the loss on each epoch: 34, loss: 27.101534271240233\n",
            "the loss on each epoch: 35, loss: 27.104534408024378\n",
            "the loss on each epoch: 36, loss: 26.965737070356095\n",
            "the loss on each epoch: 37, loss: 27.115090349742346\n",
            "the loss on each epoch: 38, loss: 26.97625126157488\n",
            "the loss on each epoch: 39, loss: 27.017493752070834\n",
            "the loss on each epoch: 40, loss: 26.880408382415773\n",
            "the loss on each epoch: 41, loss: 26.827157061440605\n",
            "the loss on each epoch: 42, loss: 26.67087415967669\n",
            "the loss on each epoch: 43, loss: 26.7091705594744\n",
            "the loss on each epoch: 44, loss: 26.557549694606237\n",
            "the loss on each epoch: 45, loss: 26.66552663530622\n",
            "the loss on each epoch: 46, loss: 26.509334768567765\n",
            "the loss on each epoch: 47, loss: 26.43289534705026\n",
            "the loss on each epoch: 48, loss: 26.526996203831263\n",
            "the loss on each epoch: 49, loss: 26.385144383566722\n",
            "the loss on each epoch: 50, loss: 26.317602252960206\n",
            "the loss on each epoch: 51, loss: 26.248860372815813\n",
            "the loss on each epoch: 52, loss: 26.183489418029787\n",
            "the loss on each epoch: 53, loss: 26.211517797197615\n",
            "the loss on each epoch: 54, loss: 26.109360463278634\n",
            "the loss on each epoch: 55, loss: 26.101476110730854\n",
            "the loss on each epoch: 56, loss: 26.07490110397339\n",
            "the loss on each epoch: 57, loss: 26.054793861934115\n",
            "the loss on each epoch: 58, loss: 25.940699141366142\n",
            "the loss on each epoch: 59, loss: 25.8545305115836\n",
            "the loss on each epoch: 60, loss: 25.804192325047083\n",
            "the loss on each epoch: 61, loss: 25.72865836279733\n",
            "the loss on each epoch: 62, loss: 25.77488946914673\n",
            "the loss on each epoch: 63, loss: 25.655501501900808\n",
            "the loss on each epoch: 64, loss: 25.625744928632464\n",
            "the loss on each epoch: 65, loss: 25.720798669542585\n",
            "the loss on each epoch: 66, loss: 25.51447206224714\n",
            "the loss on each epoch: 67, loss: 25.566479396820068\n",
            "the loss on each epoch: 68, loss: 25.497941289629257\n",
            "the loss on each epoch: 69, loss: 25.503961045401436\n",
            "the loss on each epoch: 70, loss: 25.368172264099123\n",
            "the loss on each epoch: 71, loss: 25.424738448006767\n",
            "the loss on each epoch: 72, loss: 25.293326813834053\n",
            "the loss on each epoch: 73, loss: 25.298997456686838\n",
            "the loss on each epoch: 74, loss: 25.22296259743827\n",
            "the loss on each epoch: 75, loss: 25.302757726396834\n",
            "the loss on each epoch: 76, loss: 25.267526463099887\n",
            "the loss on each epoch: 77, loss: 25.162663718632288\n",
            "the loss on each epoch: 78, loss: 25.085529477255687\n",
            "the loss on each epoch: 79, loss: 25.109033870697022\n",
            "the loss on each epoch: 80, loss: 25.120396654946465\n",
            "the loss on each epoch: 81, loss: 25.062057113647462\n",
            "the loss on each epoch: 82, loss: 24.978878375462124\n",
            "the loss on each epoch: 83, loss: 24.966386563437325\n",
            "the loss on each epoch: 84, loss: 25.021865027291433\n",
            "the loss on each epoch: 85, loss: 24.96378496715001\n",
            "the loss on each epoch: 86, loss: 24.861379296439036\n",
            "the loss on each epoch: 87, loss: 24.881013890675135\n",
            "the loss on each epoch: 88, loss: 24.883150018964496\n",
            "the loss on each epoch: 89, loss: 24.82366830280849\n",
            "the loss on each epoch: 90, loss: 24.94674210548401\n",
            "the loss on each epoch: 91, loss: 24.871776403699602\n",
            "the loss on each epoch: 92, loss: 24.771388557979037\n",
            "the loss on each epoch: 93, loss: 24.827829429081508\n",
            "the loss on each epoch: 94, loss: 24.732136971609933\n",
            "the loss on each epoch: 95, loss: 24.76789048058646\n",
            "the loss on each epoch: 96, loss: 24.746585260118756\n",
            "the loss on each epoch: 97, loss: 24.70007107598441\n",
            "the loss on each epoch: 98, loss: 24.67944267817906\n",
            "the loss on each epoch: 99, loss: 24.68070842197963\n",
            "the loss on each epoch: 100, loss: 24.666020822525024\n",
            "the loss on each epoch: 101, loss: 24.614136954716272\n",
            "the loss on each epoch: 102, loss: 24.54432464327131\n",
            "the loss on each epoch: 103, loss: 24.57815305846078\n",
            "the loss on each epoch: 104, loss: 24.609516416277206\n",
            "the loss on each epoch: 105, loss: 24.603600406646727\n",
            "the loss on each epoch: 106, loss: 24.55021404538836\n",
            "the loss on each epoch: 107, loss: 24.470303467341832\n",
            "the loss on each epoch: 108, loss: 24.48044345038278\n",
            "the loss on each epoch: 109, loss: 24.522876453399657\n",
            "the loss on each epoch: 110, loss: 24.44499271937779\n",
            "the loss on each epoch: 111, loss: 24.400410883767265\n",
            "the loss on each epoch: 112, loss: 24.350890854426794\n",
            "the loss on each epoch: 113, loss: 24.51097218649728\n",
            "the loss on each epoch: 114, loss: 24.4084135055542\n",
            "the loss on each epoch: 115, loss: 24.450912945611137\n",
            "the loss on each epoch: 116, loss: 24.438148348672048\n",
            "the loss on each epoch: 117, loss: 24.368952928270613\n",
            "the loss on each epoch: 118, loss: 24.36457247052874\n",
            "the loss on each epoch: 119, loss: 24.289295305524554\n",
            "the loss on each epoch: 120, loss: 24.443155690601895\n",
            "the loss on each epoch: 121, loss: 24.315334183829172\n",
            "the loss on each epoch: 122, loss: 24.28245120729719\n",
            "the loss on each epoch: 123, loss: 24.318457984924315\n",
            "the loss on each epoch: 124, loss: 24.355839007241386\n",
            "the loss on each epoch: 125, loss: 24.265602125440324\n",
            "the loss on each epoch: 126, loss: 24.291304847172327\n",
            "the loss on each epoch: 127, loss: 24.21769365583147\n",
            "the loss on each epoch: 128, loss: 24.316960062299454\n",
            "the loss on each epoch: 129, loss: 24.121888419560022\n",
            "the loss on each epoch: 130, loss: 24.322594370160783\n",
            "the loss on each epoch: 131, loss: 24.206186839512416\n",
            "the loss on each epoch: 132, loss: 24.251388917650495\n",
            "the loss on each epoch: 133, loss: 24.22212336403983\n",
            "the loss on each epoch: 134, loss: 24.184237125941685\n",
            "the loss on each epoch: 135, loss: 24.162618473597934\n",
            "the loss on each epoch: 136, loss: 24.180722454616003\n",
            "the loss on each epoch: 137, loss: 24.26074798447745\n",
            "the loss on each epoch: 138, loss: 24.219285569872174\n",
            "the loss on each epoch: 139, loss: 24.163004575456892\n",
            "the loss on each epoch: 140, loss: 24.25911823000227\n",
            "the loss on each epoch: 141, loss: 24.09860951559884\n",
            "the loss on each epoch: 142, loss: 24.28240626198905\n",
            "the loss on each epoch: 143, loss: 24.070927238464357\n",
            "the loss on each epoch: 144, loss: 24.03329326084682\n",
            "the loss on each epoch: 145, loss: 24.08058782305036\n",
            "the loss on each epoch: 146, loss: 24.06472133908953\n",
            "the loss on each epoch: 147, loss: 24.084350681304933\n",
            "the loss on each epoch: 148, loss: 24.053474766867502\n",
            "the loss on each epoch: 149, loss: 24.075542422703336\n",
            "the loss on each epoch: 150, loss: 24.140840653010777\n",
            "the loss on each epoch: 151, loss: 24.042858614240373\n",
            "the loss on each epoch: 152, loss: 24.04172852379935\n",
            "the loss on each epoch: 153, loss: 24.04149947847639\n",
            "the loss on each epoch: 154, loss: 24.034839057922362\n",
            "the loss on each epoch: 155, loss: 23.97815239770072\n",
            "the loss on each epoch: 156, loss: 23.993223708016533\n",
            "the loss on each epoch: 157, loss: 24.062478249413626\n",
            "the loss on each epoch: 158, loss: 24.073683302743095\n",
            "the loss on each epoch: 159, loss: 24.001292746407646\n",
            "the loss on each epoch: 160, loss: 23.99412945338658\n",
            "the loss on each epoch: 161, loss: 24.019663402012416\n",
            "the loss on each epoch: 162, loss: 23.994471618107386\n",
            "the loss on each epoch: 163, loss: 23.96462114879063\n",
            "the loss on each epoch: 164, loss: 23.907487160818917\n",
            "the loss on each epoch: 165, loss: 24.08831558908735\n",
            "the loss on each epoch: 166, loss: 24.006229407446725\n",
            "the loss on each epoch: 167, loss: 24.01085968017578\n",
            "the loss on each epoch: 168, loss: 23.863866608483452\n",
            "the loss on each epoch: 169, loss: 23.94665539605277\n",
            "the loss on each epoch: 170, loss: 23.902755301339287\n",
            "the loss on each epoch: 171, loss: 23.962678841182164\n",
            "the loss on each epoch: 172, loss: 23.869511250087193\n",
            "the loss on each epoch: 173, loss: 23.908683245522635\n",
            "the loss on each epoch: 174, loss: 23.95186495780945\n",
            "the loss on each epoch: 175, loss: 23.898344925471715\n",
            "the loss on each epoch: 176, loss: 23.88445781980242\n",
            "the loss on each epoch: 177, loss: 23.874423326764788\n",
            "the loss on each epoch: 178, loss: 23.85144372667585\n",
            "the loss on each epoch: 179, loss: 23.882527038029263\n",
            "the loss on each epoch: 180, loss: 23.876428617749895\n",
            "the loss on each epoch: 181, loss: 23.813823686327254\n",
            "the loss on each epoch: 182, loss: 23.83993560246059\n",
            "the loss on each epoch: 183, loss: 23.846263572147915\n",
            "the loss on each epoch: 184, loss: 23.805186353410992\n",
            "the loss on each epoch: 185, loss: 23.77852945327759\n",
            "the loss on each epoch: 186, loss: 23.846411418914794\n",
            "the loss on each epoch: 187, loss: 23.87335067476545\n",
            "the loss on each epoch: 188, loss: 23.87706380571638\n",
            "the loss on each epoch: 189, loss: 23.86084442138672\n",
            "the loss on each epoch: 190, loss: 23.93164484841483\n",
            "the loss on each epoch: 191, loss: 23.842252097811016\n",
            "the loss on each epoch: 192, loss: 23.776379694257464\n",
            "the loss on each epoch: 193, loss: 23.782704053606306\n",
            "the loss on each epoch: 194, loss: 23.841640090942384\n",
            "the loss on each epoch: 195, loss: 23.851156616210936\n",
            "the loss on each epoch: 196, loss: 23.825983783176966\n",
            "the loss on each epoch: 197, loss: 23.779909569876533\n",
            "the loss on each epoch: 198, loss: 23.76196109226772\n",
            "the loss on each epoch: 199, loss: 23.806382874080114\n",
            "the loss on each epoch: 200, loss: 23.82319880894252\n",
            "the loss on each epoch: 201, loss: 23.819898114885603\n",
            "the loss on each epoch: 202, loss: 23.857929761069162\n",
            "the loss on each epoch: 203, loss: 23.754695578983853\n",
            "the loss on each epoch: 204, loss: 23.76869993209839\n",
            "the loss on each epoch: 205, loss: 23.754325457981654\n",
            "the loss on each epoch: 206, loss: 23.76121006011963\n",
            "the loss on each epoch: 207, loss: 23.809048461914063\n",
            "the loss on each epoch: 208, loss: 23.754416138785228\n",
            "the loss on each epoch: 209, loss: 23.726485157012938\n",
            "the loss on each epoch: 210, loss: 23.69670034136091\n",
            "the loss on each epoch: 211, loss: 23.734042753492083\n",
            "the loss on each epoch: 212, loss: 23.68337787900652\n",
            "the loss on each epoch: 213, loss: 23.732776239940097\n",
            "the loss on each epoch: 214, loss: 23.727472952433995\n",
            "the loss on each epoch: 215, loss: 23.688020719800676\n",
            "the loss on each epoch: 216, loss: 23.767210681097847\n",
            "the loss on each epoch: 217, loss: 23.738933038711547\n",
            "the loss on each epoch: 218, loss: 23.7413731438773\n",
            "the loss on each epoch: 219, loss: 23.706614862169538\n",
            "the loss on each epoch: 220, loss: 23.664641434805734\n",
            "the loss on each epoch: 221, loss: 23.6225576536996\n",
            "the loss on each epoch: 222, loss: 23.630948625292095\n",
            "the loss on each epoch: 223, loss: 23.612974916185653\n",
            "the loss on each epoch: 224, loss: 23.625893783569335\n",
            "the loss on each epoch: 225, loss: 23.69158230509077\n",
            "the loss on each epoch: 226, loss: 23.616749164036342\n",
            "the loss on each epoch: 227, loss: 23.63520372254508\n",
            "the loss on each epoch: 228, loss: 23.687587806156703\n",
            "the loss on each epoch: 229, loss: 23.661090591975622\n",
            "the loss on each epoch: 230, loss: 23.619072954995293\n",
            "the loss on each epoch: 231, loss: 23.565074716295516\n",
            "the loss on each epoch: 232, loss: 23.560029138837542\n",
            "the loss on each epoch: 233, loss: 23.74183259350913\n",
            "the loss on each epoch: 234, loss: 23.687485698291233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(outputs,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test : {100 * correct // total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjjcoqJLoqDC",
        "outputId": "30504dd2-b33f-4c2f-c3ce-42d01d255944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test : 50 %\n"
          ]
        }
      ]
    }
  ]
}