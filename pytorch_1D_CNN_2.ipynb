{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP62jXxCbEwbEdIYEX+Y8m6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "c22d5419-dcae-49a9-d905-4f98a18eadfd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6e1828cdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2681b959-7f28-4a28-bcbb-dfa4c03781da"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 4, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "BATCH_SIZE = 10\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_features=128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(3200, 128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.size())\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc2(x)\n",
        "        #out = torch.flatten(x,1)\n",
        "        #print(out)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "EPOCH = 100              # train the training data n times, to save time, we just train 1 epoch\n",
        "LR = 0.001 \n",
        "\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        trainCorrect += (predsid == b_y).type(torch.float).sum().item()\n",
        "\n",
        "        if i % 10 == 0:    # print every 100 mini-batches\n",
        "            #print(i)\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/10}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Letâ€™s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zckte_qdea_1",
        "outputId": "ecde5059-cda4-4822-8e43-cfbdf500a3bd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 1.1379390716552735\n",
            "[1,    11] loss: 1089.0797958374023\n",
            "[1,    21] loss: 13.374147415161133\n",
            "[1,    31] loss: 21.2954692363739\n",
            "[1,    41] loss: 14.096915149688721\n",
            "[1,    51] loss: 12.186572074890137\n",
            "[1,    61] loss: 10.758504009246826\n",
            "[1,    71] loss: 11.78459062576294\n",
            "[1,    81] loss: 11.154080772399903\n",
            "[1,    91] loss: 11.299857234954834\n",
            "[1,   101] loss: 11.056188321113586\n",
            "[1,   111] loss: 10.591457223892212\n",
            "[1,   121] loss: 12.90058650970459\n",
            "[1,   131] loss: 12.421175670623779\n",
            "[2,     1] loss: 1.6303050994873047\n",
            "[2,    11] loss: 11.569576930999755\n",
            "[2,    21] loss: 12.43480725288391\n",
            "[2,    31] loss: 11.96131353378296\n",
            "[2,    41] loss: 10.357911205291748\n",
            "[2,    51] loss: 12.017787408828735\n",
            "[2,    61] loss: 11.51189079284668\n",
            "[2,    71] loss: 9.898406887054444\n",
            "[2,    81] loss: 10.594257974624634\n",
            "[2,    91] loss: 13.358060693740844\n",
            "[2,   101] loss: 11.282667541503907\n",
            "[2,   111] loss: 12.437638902664185\n",
            "[2,   121] loss: 9.901116466522216\n",
            "[2,   131] loss: 12.204429531097412\n",
            "[3,     1] loss: 1.6118097305297852\n",
            "[3,    11] loss: 13.355514097213746\n",
            "[3,    21] loss: 10.822110795974732\n",
            "[3,    31] loss: 11.052409029006958\n",
            "[3,    41] loss: 12.894477558135986\n",
            "[3,    51] loss: 13.354994583129884\n",
            "[3,    61] loss: 13.354994583129884\n",
            "[3,    71] loss: 9.901116371154785\n",
            "[3,    81] loss: 10.131584072113037\n",
            "[3,    91] loss: 11.743184757232665\n",
            "[3,   101] loss: 11.742561626434327\n",
            "[3,   111] loss: 10.82215051651001\n",
            "[3,   121] loss: 11.282667636871338\n",
            "[3,   131] loss: 10.361633491516113\n",
            "[4,     1] loss: 2.0723268508911135\n",
            "[4,    11] loss: 9.67017683982849\n",
            "[4,    21] loss: 12.433960390090942\n",
            "[4,    31] loss: 11.512926292419433\n",
            "[4,    41] loss: 11.743377923965454\n",
            "[4,    51] loss: 11.052409029006958\n",
            "[4,    61] loss: 12.66421890258789\n",
            "[4,    71] loss: 10.361578130722046\n",
            "[4,    81] loss: 12.203701829910278\n",
            "[4,    91] loss: 9.671592664718627\n",
            "[4,   101] loss: 10.822150611877442\n",
            "[4,   111] loss: 12.203701782226563\n",
            "[4,   121] loss: 10.822150564193725\n",
            "[4,   131] loss: 13.124736118316651\n",
            "[5,     1] loss: 1.151292610168457\n",
            "[5,    11] loss: 12.894477462768554\n",
            "[5,    21] loss: 10.591892099380493\n",
            "[5,    31] loss: 10.361633443832398\n",
            "[5,    41] loss: 11.512926149368287\n",
            "[5,    51] loss: 11.97344331741333\n",
            "[5,    61] loss: 12.433960437774658\n",
            "[5,    71] loss: 11.973443222045898\n",
            "[5,    81] loss: 13.354994630813598\n",
            "[5,    91] loss: 10.822150421142577\n",
            "[5,   101] loss: 9.210340785980225\n",
            "[5,   111] loss: 11.052408981323243\n",
            "[5,   121] loss: 11.743184661865234\n",
            "[5,   131] loss: 10.82215051651001\n",
            "[6,     1] loss: 0.9210340499877929\n",
            "[6,    11] loss: 11.743184804916382\n",
            "[6,    21] loss: 10.82215051651001\n",
            "[6,    31] loss: 8.98008222579956\n",
            "[6,    41] loss: 13.585253143310547\n",
            "[6,    51] loss: 9.670857906341553\n",
            "[6,    61] loss: 13.354994583129884\n",
            "[6,    71] loss: 11.743125200271606\n",
            "[6,    81] loss: 11.97344331741333\n",
            "[6,    91] loss: 11.282667636871338\n",
            "[6,   101] loss: 10.131374979019165\n",
            "[6,   111] loss: 12.43396062850952\n",
            "[6,   121] loss: 10.822150564193725\n",
            "[6,   131] loss: 11.512926197052002\n",
            "[7,     1] loss: 1.151292610168457\n",
            "[7,    11] loss: 12.203480577468872\n",
            "[7,    21] loss: 11.511542129516602\n",
            "[7,    31] loss: 11.2847749710083\n",
            "[7,    41] loss: 9.901116466522216\n",
            "[7,    51] loss: 13.124736070632935\n",
            "[7,    61] loss: 11.75957703590393\n",
            "[7,    71] loss: 11.282667779922486\n",
            "[7,    81] loss: 11.282667589187621\n",
            "[7,    91] loss: 12.66421914100647\n",
            "[7,   101] loss: 11.282667636871338\n",
            "[7,   111] loss: 12.203701782226563\n",
            "[7,   121] loss: 10.361633491516113\n",
            "[7,   131] loss: 10.591891956329345\n",
            "[8,     1] loss: 0.23025851249694823\n",
            "[8,    11] loss: 11.743184852600098\n",
            "[8,    21] loss: 11.97344331741333\n",
            "[8,    31] loss: 11.512926149368287\n",
            "[8,    41] loss: 12.203701829910278\n",
            "[8,    51] loss: 12.664218950271607\n",
            "[8,    61] loss: 10.13137502670288\n",
            "[8,    71] loss: 11.512926149368287\n",
            "[8,    81] loss: 12.203701877593994\n",
            "[8,    91] loss: 11.512926244735718\n",
            "[8,   101] loss: 11.28266773223877\n",
            "[8,   111] loss: 12.203701782226563\n",
            "[8,   121] loss: 10.13137502670288\n",
            "[8,   131] loss: 12.433960390090942\n",
            "[9,     1] loss: 1.151292610168457\n",
            "[9,    11] loss: 11.743184804916382\n",
            "[9,    21] loss: 11.282667589187621\n",
            "[9,    31] loss: 10.822150611877442\n",
            "[9,    41] loss: 11.74318470954895\n",
            "[9,    51] loss: 11.51292610168457\n",
            "[9,    61] loss: 11.052409029006958\n",
            "[9,    71] loss: 9.210340738296509\n",
            "[9,    81] loss: 12.89447751045227\n",
            "[9,    91] loss: 12.433960437774658\n",
            "[9,   101] loss: 11.51292610168457\n",
            "[9,   111] loss: 12.203701782226563\n",
            "[9,   121] loss: 11.282667684555054\n",
            "[9,   131] loss: 11.052409172058105\n",
            "[10,     1] loss: 1.6118097305297852\n",
            "[10,    11] loss: 11.512926244735718\n",
            "[10,    21] loss: 13.354994630813598\n",
            "[10,    31] loss: 9.901116323471069\n",
            "[10,    41] loss: 12.664218950271607\n",
            "[10,    51] loss: 10.591892051696778\n",
            "[10,    61] loss: 11.282667636871338\n",
            "[10,    71] loss: 12.203701782226563\n",
            "[10,    81] loss: 11.973443365097046\n",
            "[10,    91] loss: 10.591892051696778\n",
            "[10,   101] loss: 11.052409029006958\n",
            "[10,   111] loss: 11.052409219741822\n",
            "[10,   121] loss: 11.052409124374389\n",
            "[10,   131] loss: 10.82215051651001\n",
            "[11,     1] loss: 1.151292610168457\n",
            "[11,    11] loss: 11.282667636871338\n",
            "[11,    21] loss: 11.51292610168457\n",
            "[11,    31] loss: 9.901116275787354\n",
            "[11,    41] loss: 11.74318470954895\n",
            "[11,    51] loss: 13.354994583129884\n",
            "[11,    61] loss: 11.74318470954895\n",
            "[11,    71] loss: 12.89447751045227\n",
            "[11,    81] loss: 11.743184757232665\n",
            "[11,    91] loss: 13.354994583129884\n",
            "[11,   101] loss: 9.210340595245361\n",
            "[11,   111] loss: 9.440599298477172\n",
            "[11,   121] loss: 12.20370192527771\n",
            "[11,   131] loss: 10.822150611877442\n",
            "[12,     1] loss: 0.9210340499877929\n",
            "[12,    11] loss: 9.440599250793458\n",
            "[12,    21] loss: 13.354994583129884\n",
            "[12,    31] loss: 12.89447751045227\n",
            "[12,    41] loss: 11.743184757232665\n",
            "[12,    51] loss: 12.433960485458375\n",
            "[12,    61] loss: 11.973443269729614\n",
            "[12,    71] loss: 11.74318494796753\n",
            "[12,    81] loss: 12.203701877593994\n",
            "[12,    91] loss: 10.822150468826294\n",
            "[12,   101] loss: 9.21034083366394\n",
            "[12,   111] loss: 10.591892004013062\n",
            "[12,   121] loss: 11.973443222045898\n",
            "[12,   131] loss: 10.822150564193725\n",
            "[13,     1] loss: 0.9210340499877929\n",
            "[13,    11] loss: 11.052409076690674\n",
            "[13,    21] loss: 12.203701829910278\n",
            "[13,    31] loss: 10.591892051696778\n",
            "[13,    41] loss: 11.282667541503907\n",
            "[13,    51] loss: 12.89447751045227\n",
            "[13,    61] loss: 11.282667636871338\n",
            "[13,    71] loss: 10.822150659561157\n",
            "[13,    81] loss: 11.973443365097046\n",
            "[13,    91] loss: 10.591892004013062\n",
            "[13,   101] loss: 12.433960342407227\n",
            "[13,   111] loss: 12.664218997955322\n",
            "[13,   121] loss: 9.670857763290405\n",
            "[13,   131] loss: 11.052409076690674\n",
            "[14,     1] loss: 0.9210340499877929\n",
            "[14,    11] loss: 10.591892051696778\n",
            "[14,    21] loss: 12.433960342407227\n",
            "[14,    31] loss: 10.13137502670288\n",
            "[14,    41] loss: 12.664218997955322\n",
            "[14,    51] loss: 10.131374883651734\n",
            "[14,    61] loss: 11.282667779922486\n",
            "[14,    71] loss: 12.664218950271607\n",
            "[14,    81] loss: 11.973443222045898\n",
            "[14,    91] loss: 10.361633396148681\n",
            "[14,   101] loss: 10.822150564193725\n",
            "[14,   111] loss: 9.901116466522216\n",
            "[14,   121] loss: 12.433960437774658\n",
            "[14,   131] loss: 14.736546039581299\n",
            "[15,     1] loss: 1.6118097305297852\n",
            "[15,    11] loss: 10.822150564193725\n",
            "[15,    21] loss: 11.512926244735718\n",
            "[15,    31] loss: 10.822150564193725\n",
            "[15,    41] loss: 12.433960485458375\n",
            "[15,    51] loss: 10.361633682250977\n",
            "[15,    61] loss: 12.89447751045227\n",
            "[15,    71] loss: 11.052409076690674\n",
            "[15,    81] loss: 13.585253190994262\n",
            "[15,    91] loss: 12.894477462768554\n",
            "[15,   101] loss: 12.433960342407227\n",
            "[15,   111] loss: 12.894477605819702\n",
            "[15,   121] loss: 8.98008222579956\n",
            "[15,   131] loss: 8.059048080444336\n",
            "[16,     1] loss: 1.151292610168457\n",
            "[16,    11] loss: 13.124736118316651\n",
            "[16,    21] loss: 12.89447751045227\n",
            "[16,    31] loss: 12.66421890258789\n",
            "[16,    41] loss: 10.822150421142577\n",
            "[16,    51] loss: 9.440599298477172\n",
            "[16,    61] loss: 12.43396053314209\n",
            "[16,    71] loss: 11.282667636871338\n",
            "[16,    81] loss: 11.512926149368287\n",
            "[16,    91] loss: 11.512926149368287\n",
            "[16,   101] loss: 11.052409124374389\n",
            "[16,   111] loss: 10.591892051696778\n",
            "[16,   121] loss: 11.743184852600098\n",
            "[16,   131] loss: 11.74318470954895\n",
            "[17,     1] loss: 1.8420682907104493\n",
            "[17,    11] loss: 10.591891956329345\n",
            "[17,    21] loss: 10.82215051651001\n",
            "[17,    31] loss: 10.591892099380493\n",
            "[17,    41] loss: 10.361633491516113\n",
            "[17,    51] loss: 12.203701877593994\n",
            "[17,    61] loss: 10.822150564193725\n",
            "[17,    71] loss: 11.052409076690674\n",
            "[17,    81] loss: 11.97344331741333\n",
            "[17,    91] loss: 12.433960485458375\n",
            "[17,   101] loss: 12.203702020645142\n",
            "[17,   111] loss: 12.433960390090942\n",
            "[17,   121] loss: 11.74318470954895\n",
            "[17,   131] loss: 11.743184757232665\n",
            "[18,     1] loss: 0.9210340499877929\n",
            "[18,    11] loss: 11.74318470954895\n",
            "[18,    21] loss: 11.743184661865234\n",
            "[18,    31] loss: 11.052409076690674\n",
            "[18,    41] loss: 11.973443222045898\n",
            "[18,    51] loss: 11.973443269729614\n",
            "[18,    61] loss: 11.512926149368287\n",
            "[18,    71] loss: 12.664218950271607\n",
            "[18,    81] loss: 10.591892004013062\n",
            "[18,    91] loss: 11.052409172058105\n",
            "[18,   101] loss: 11.052409029006958\n",
            "[18,   111] loss: 12.894477558135986\n",
            "[18,   121] loss: 11.512926244735718\n",
            "[18,   131] loss: 9.901116371154785\n",
            "[19,     1] loss: 1.151292610168457\n",
            "[19,    11] loss: 12.433960342407227\n",
            "[19,    21] loss: 10.591892004013062\n",
            "[19,    31] loss: 10.131374931335449\n",
            "[19,    41] loss: 13.354994630813598\n",
            "[19,    51] loss: 11.743184757232665\n",
            "[19,    61] loss: 9.670857906341553\n",
            "[19,    71] loss: 10.822150468826294\n",
            "[19,    81] loss: 11.743184804916382\n",
            "[19,    91] loss: 9.440599298477172\n",
            "[19,   101] loss: 11.512926197052002\n",
            "[19,   111] loss: 11.973443269729614\n",
            "[19,   121] loss: 13.815511703491211\n",
            "[19,   131] loss: 11.512926197052002\n",
            "[20,     1] loss: 1.381551170349121\n",
            "[20,    11] loss: 9.440599298477172\n",
            "[20,    21] loss: 10.822150611877442\n",
            "[20,    31] loss: 10.591892004013062\n",
            "[20,    41] loss: 11.973443269729614\n",
            "[20,    51] loss: 11.743184900283813\n",
            "[20,    61] loss: 13.354994630813598\n",
            "[20,    71] loss: 12.203701782226563\n",
            "[20,    81] loss: 13.585253190994262\n",
            "[20,    91] loss: 11.052409267425537\n",
            "[20,   101] loss: 10.361633443832398\n",
            "[20,   111] loss: 11.282667636871338\n",
            "[20,   121] loss: 10.131374883651734\n",
            "[20,   131] loss: 11.973443222045898\n",
            "[21,     1] loss: 1.6118097305297852\n",
            "[21,    11] loss: 10.591891956329345\n",
            "[21,    21] loss: 10.361633348464967\n",
            "[21,    31] loss: 11.97344331741333\n",
            "[21,    41] loss: 10.822150564193725\n",
            "[21,    51] loss: 11.973443269729614\n",
            "[21,    61] loss: 11.282667636871338\n",
            "[21,    71] loss: 10.82215051651001\n",
            "[21,    81] loss: 12.433960390090942\n",
            "[21,    91] loss: 11.282667684555054\n",
            "[21,   101] loss: 9.901116371154785\n",
            "[21,   111] loss: 11.743184661865234\n",
            "[21,   121] loss: 11.743184757232665\n",
            "[21,   131] loss: 12.433960342407227\n",
            "[22,     1] loss: 0.6907755374908447\n",
            "[22,    11] loss: 11.512926244735718\n",
            "[22,    21] loss: 10.361633586883546\n",
            "[22,    31] loss: 12.664218950271607\n",
            "[22,    41] loss: 12.433960342407227\n",
            "[22,    51] loss: 11.282667684555054\n",
            "[22,    61] loss: 12.433960342407227\n",
            "[22,    71] loss: 10.131375074386597\n",
            "[22,    81] loss: 12.433960437774658\n",
            "[22,    91] loss: 11.282667636871338\n",
            "[22,   101] loss: 11.052409172058105\n",
            "[22,   111] loss: 12.664218997955322\n",
            "[22,   121] loss: 11.743184661865234\n",
            "[22,   131] loss: 10.82215051651001\n",
            "[23,     1] loss: 1.151292610168457\n",
            "[23,    11] loss: 12.66421914100647\n",
            "[23,    21] loss: 11.282667636871338\n",
            "[23,    31] loss: 12.203701782226563\n",
            "[23,    41] loss: 10.361633396148681\n",
            "[23,    51] loss: 11.973443365097046\n",
            "[23,    61] loss: 11.973443222045898\n",
            "[23,    71] loss: 11.743184804916382\n",
            "[23,    81] loss: 12.433960437774658\n",
            "[23,    91] loss: 10.361633491516113\n",
            "[23,   101] loss: 12.664218997955322\n",
            "[23,   111] loss: 11.512926292419433\n",
            "[23,   121] loss: 10.131374835968018\n",
            "[23,   131] loss: 11.973443412780762\n",
            "[24,     1] loss: 1.6118097305297852\n",
            "[24,    11] loss: 10.131374979019165\n",
            "[24,    21] loss: 10.591892051696778\n",
            "[24,    31] loss: 11.97344331741333\n",
            "[24,    41] loss: 12.203701782226563\n",
            "[24,    51] loss: 12.20370192527771\n",
            "[24,    61] loss: 12.433960342407227\n",
            "[24,    71] loss: 10.59189214706421\n",
            "[24,    81] loss: 12.664218950271607\n",
            "[24,    91] loss: 11.052409029006958\n",
            "[24,   101] loss: 11.512926244735718\n",
            "[24,   111] loss: 10.591891956329345\n",
            "[24,   121] loss: 10.361633396148681\n",
            "[24,   131] loss: 11.97344331741333\n",
            "[25,     1] loss: 0.6907755374908447\n",
            "[25,    11] loss: 11.512926292419433\n",
            "[25,    21] loss: 11.743184804916382\n",
            "[25,    31] loss: 12.203701782226563\n",
            "[25,    41] loss: 11.052409124374389\n",
            "[25,    51] loss: 10.131374931335449\n",
            "[25,    61] loss: 12.433960390090942\n",
            "[25,    71] loss: 13.124736070632935\n",
            "[25,    81] loss: 10.822150564193725\n",
            "[25,    91] loss: 10.591891956329345\n",
            "[25,   101] loss: 9.901116323471069\n",
            "[25,   111] loss: 11.052409076690674\n",
            "[25,   121] loss: 11.512926244735718\n",
            "[25,   131] loss: 12.894477462768554\n",
            "[26,     1] loss: 0.9210340499877929\n",
            "[26,    11] loss: 12.433960485458375\n",
            "[26,    21] loss: 11.512926149368287\n",
            "[26,    31] loss: 12.203701877593994\n",
            "[26,    41] loss: 9.210340738296509\n",
            "[26,    51] loss: 9.212023448944091\n",
            "[26,    61] loss: 11.282667589187621\n",
            "[26,    71] loss: 12.894477462768554\n",
            "[26,    81] loss: 10.361633443832398\n",
            "[26,    91] loss: 13.585253190994262\n",
            "[26,   101] loss: 12.20370192527771\n",
            "[26,   111] loss: 12.203701829910278\n",
            "[26,   121] loss: 10.361633396148681\n",
            "[26,   131] loss: 13.124736118316651\n",
            "[27,     1] loss: 0.23025851249694823\n",
            "[27,    11] loss: 10.822150421142577\n",
            "[27,    21] loss: 10.591892051696778\n",
            "[27,    31] loss: 12.20370192527771\n",
            "[27,    41] loss: 11.973443508148193\n",
            "[27,    51] loss: 11.743184852600098\n",
            "[27,    61] loss: 12.203701782226563\n",
            "[27,    71] loss: 10.822150468826294\n",
            "[27,    81] loss: 11.282667636871338\n",
            "[27,    91] loss: 10.13137502670288\n",
            "[27,   101] loss: 15.887838745117188\n",
            "[27,   111] loss: 11.973443269729614\n",
            "[27,   121] loss: 11.97344331741333\n",
            "[27,   131] loss: 9.21034083366394\n",
            "[28,     1] loss: 0.9210340499877929\n",
            "[28,    11] loss: 11.743184757232665\n",
            "[28,    21] loss: 11.282667636871338\n",
            "[28,    31] loss: 11.512926197052002\n",
            "[28,    41] loss: 10.591892004013062\n",
            "[28,    51] loss: 8.980082368850708\n",
            "[28,    61] loss: 11.052409076690674\n",
            "[28,    71] loss: 11.973443365097046\n",
            "[28,    81] loss: 13.124736022949218\n",
            "[28,    91] loss: 10.591892099380493\n",
            "[28,   101] loss: 11.973443222045898\n",
            "[28,   111] loss: 13.124736022949218\n",
            "[28,   121] loss: 11.282667636871338\n",
            "[28,   131] loss: 11.743184852600098\n",
            "[29,     1] loss: 0.9210340499877929\n",
            "[29,    11] loss: 11.743184661865234\n",
            "[29,    21] loss: 10.591892051696778\n",
            "[29,    31] loss: 11.743184757232665\n",
            "[29,    41] loss: 11.74318470954895\n",
            "[29,    51] loss: 11.282667684555054\n",
            "[29,    61] loss: 10.361633396148681\n",
            "[29,    71] loss: 12.203701829910278\n",
            "[29,    81] loss: 10.59189190864563\n",
            "[29,    91] loss: 11.74318470954895\n",
            "[29,   101] loss: 10.591892004013062\n",
            "[29,   111] loss: 12.664218950271607\n",
            "[29,   121] loss: 12.664218950271607\n",
            "[29,   131] loss: 11.052409172058105\n",
            "[30,     1] loss: 0.46051702499389646\n",
            "[30,    11] loss: 9.210340690612792\n",
            "[30,    21] loss: 11.052409172058105\n",
            "[30,    31] loss: 11.512926149368287\n",
            "[30,    41] loss: 13.354994583129884\n",
            "[30,    51] loss: 11.512926244735718\n",
            "[30,    61] loss: 11.512926149368287\n",
            "[30,    71] loss: 12.664218997955322\n",
            "[30,    81] loss: 10.361633491516113\n",
            "[30,    91] loss: 11.512926149368287\n",
            "[30,   101] loss: 11.51292610168457\n",
            "[30,   111] loss: 11.282667589187621\n",
            "[30,   121] loss: 13.354994583129884\n",
            "[30,   131] loss: 11.973443269729614\n",
            "[31,     1] loss: 1.151292610168457\n",
            "[31,    11] loss: 11.512926244735718\n",
            "[31,    21] loss: 11.74318470954895\n",
            "[31,    31] loss: 12.203701829910278\n",
            "[31,    41] loss: 11.743184757232665\n",
            "[31,    51] loss: 11.282667636871338\n",
            "[31,    61] loss: 10.591892051696778\n",
            "[31,    71] loss: 12.433960437774658\n",
            "[31,    81] loss: 13.124736118316651\n",
            "[31,    91] loss: 9.901116418838502\n",
            "[31,   101] loss: 10.361633443832398\n",
            "[31,   111] loss: 10.361633491516113\n",
            "[31,   121] loss: 12.894477462768554\n",
            "[31,   131] loss: 12.433960485458375\n",
            "[32,     1] loss: 0.9210340499877929\n",
            "[32,    11] loss: 12.664218950271607\n",
            "[32,    21] loss: 11.74318470954895\n",
            "[32,    31] loss: 10.131375122070313\n",
            "[32,    41] loss: 11.973443460464477\n",
            "[32,    51] loss: 11.743184757232665\n",
            "[32,    61] loss: 11.282667779922486\n",
            "[32,    71] loss: 11.512926244735718\n",
            "[32,    81] loss: 10.361633396148681\n",
            "[32,    91] loss: 13.815511703491211\n",
            "[32,   101] loss: 9.67085795402527\n",
            "[32,   111] loss: 12.89447751045227\n",
            "[32,   121] loss: 11.052409124374389\n",
            "[32,   131] loss: 10.822150611877442\n",
            "[33,     1] loss: 1.6118097305297852\n",
            "[33,    11] loss: 8.289306592941283\n",
            "[33,    21] loss: 12.433960342407227\n",
            "[33,    31] loss: 12.203701782226563\n",
            "[33,    41] loss: 10.822150611877442\n",
            "[33,    51] loss: 11.052409124374389\n",
            "[33,    61] loss: 11.97344331741333\n",
            "[33,    71] loss: 10.361633396148681\n",
            "[33,    81] loss: 12.203702068328857\n",
            "[33,    91] loss: 11.512926292419433\n",
            "[33,   101] loss: 12.664218950271607\n",
            "[33,   111] loss: 9.901116418838502\n",
            "[33,   121] loss: 10.131374931335449\n",
            "[33,   131] loss: 14.736545944213868\n",
            "[34,     1] loss: 1.381551170349121\n",
            "[34,    11] loss: 10.591892004013062\n",
            "[34,    21] loss: 11.512926197052002\n",
            "[34,    31] loss: 11.973443222045898\n",
            "[34,    41] loss: 12.664218997955322\n",
            "[34,    51] loss: 11.973443222045898\n",
            "[34,    61] loss: 11.973443269729614\n",
            "[34,    71] loss: 11.74318470954895\n",
            "[34,    81] loss: 13.124736070632935\n",
            "[34,    91] loss: 11.052409124374389\n",
            "[34,   101] loss: 9.21034083366394\n",
            "[34,   111] loss: 11.282667684555054\n",
            "[34,   121] loss: 9.670857810974121\n",
            "[34,   131] loss: 12.433960437774658\n",
            "[35,     1] loss: 0.9210340499877929\n",
            "[35,    11] loss: 9.670857810974121\n",
            "[35,    21] loss: 13.354994583129884\n",
            "[35,    31] loss: 11.512926244735718\n",
            "[35,    41] loss: 11.282667684555054\n",
            "[35,    51] loss: 11.512926197052002\n",
            "[35,    61] loss: 10.591892004013062\n",
            "[35,    71] loss: 10.822150611877442\n",
            "[35,    81] loss: 11.282667684555054\n",
            "[35,    91] loss: 11.052409172058105\n",
            "[35,   101] loss: 13.124736070632935\n",
            "[35,   111] loss: 12.664218950271607\n",
            "[35,   121] loss: 12.664218950271607\n",
            "[35,   131] loss: 9.901116418838502\n",
            "[36,     1] loss: 0.9210340499877929\n",
            "[36,    11] loss: 11.512926197052002\n",
            "[36,    21] loss: 12.203701782226563\n",
            "[36,    31] loss: 11.97344331741333\n",
            "[36,    41] loss: 11.052409267425537\n",
            "[36,    51] loss: 9.670857906341553\n",
            "[36,    61] loss: 11.512926244735718\n",
            "[36,    71] loss: 10.361633443832398\n",
            "[36,    81] loss: 13.354994630813598\n",
            "[36,    91] loss: 11.282667589187621\n",
            "[36,   101] loss: 13.815511751174927\n",
            "[36,   111] loss: 9.901116323471069\n",
            "[36,   121] loss: 10.822150611877442\n",
            "[36,   131] loss: 11.512926197052002\n",
            "[37,     1] loss: 0.9210340499877929\n",
            "[37,    11] loss: 11.512926197052002\n",
            "[37,    21] loss: 10.361633443832398\n",
            "[37,    31] loss: 11.512926197052002\n",
            "[37,    41] loss: 11.512926197052002\n",
            "[37,    51] loss: 13.585253143310547\n",
            "[37,    61] loss: 12.66421890258789\n",
            "[37,    71] loss: 9.901116371154785\n",
            "[37,    81] loss: 12.433960437774658\n",
            "[37,    91] loss: 11.74318470954895\n",
            "[37,   101] loss: 11.282667589187621\n",
            "[37,   111] loss: 11.973443365097046\n",
            "[37,   121] loss: 11.052409219741822\n",
            "[37,   131] loss: 10.822150468826294\n",
            "[38,     1] loss: 1.381551170349121\n",
            "[38,    11] loss: 11.743184757232665\n",
            "[38,    21] loss: 11.973443269729614\n",
            "[38,    31] loss: 12.203701877593994\n",
            "[38,    41] loss: 11.973443365097046\n",
            "[38,    51] loss: 8.749823713302613\n",
            "[38,    61] loss: 12.894477558135986\n",
            "[38,    71] loss: 10.822150611877442\n",
            "[38,    81] loss: 9.901116418838502\n",
            "[38,    91] loss: 10.822150611877442\n",
            "[38,   101] loss: 9.901116371154785\n",
            "[38,   111] loss: 11.51292610168457\n",
            "[38,   121] loss: 12.433960485458375\n",
            "[38,   131] loss: 11.28266773223877\n",
            "[39,     1] loss: 0.9210340499877929\n",
            "[39,    11] loss: 10.361633396148681\n",
            "[39,    21] loss: 12.89447751045227\n",
            "[39,    31] loss: 11.51292610168457\n",
            "[39,    41] loss: 12.664219045639038\n",
            "[39,    51] loss: 13.354994630813598\n",
            "[39,    61] loss: 9.901116418838502\n",
            "[39,    71] loss: 11.512926197052002\n",
            "[39,    81] loss: 9.901116323471069\n",
            "[39,    91] loss: 13.585253190994262\n",
            "[39,   101] loss: 9.440599250793458\n",
            "[39,   111] loss: 10.361633539199829\n",
            "[39,   121] loss: 11.052409076690674\n",
            "[39,   131] loss: 13.585253143310547\n",
            "[40,     1] loss: 0.9210340499877929\n",
            "[40,    11] loss: 12.894477558135986\n",
            "[40,    21] loss: 9.901116371154785\n",
            "[40,    31] loss: 10.822150659561157\n",
            "[40,    41] loss: 12.89447751045227\n",
            "[40,    51] loss: 10.822150564193725\n",
            "[40,    61] loss: 10.82215051651001\n",
            "[40,    71] loss: 11.973443365097046\n",
            "[40,    81] loss: 9.670857858657836\n",
            "[40,    91] loss: 9.901116466522216\n",
            "[40,   101] loss: 14.045770311355591\n",
            "[40,   111] loss: 10.822150564193725\n",
            "[40,   121] loss: 12.433960390090942\n",
            "[40,   131] loss: 13.124736118316651\n",
            "[41,     1] loss: 1.151292610168457\n",
            "[41,    11] loss: 10.361633443832398\n",
            "[41,    21] loss: 11.74318470954895\n",
            "[41,    31] loss: 11.743184661865234\n",
            "[41,    41] loss: 9.210340738296509\n",
            "[41,    51] loss: 9.440599203109741\n",
            "[41,    61] loss: 12.203701877593994\n",
            "[41,    71] loss: 12.433960390090942\n",
            "[41,    81] loss: 11.51292610168457\n",
            "[41,    91] loss: 11.743184804916382\n",
            "[41,   101] loss: 10.361633396148681\n",
            "[41,   111] loss: 12.433960390090942\n",
            "[41,   121] loss: 13.815511751174927\n",
            "[41,   131] loss: 12.203701829910278\n",
            "[42,     1] loss: 0.6907755374908447\n",
            "[42,    11] loss: 10.131374979019165\n",
            "[42,    21] loss: 11.052409124374389\n",
            "[42,    31] loss: 11.052409124374389\n",
            "[42,    41] loss: 12.203701829910278\n",
            "[42,    51] loss: 11.052409172058105\n",
            "[42,    61] loss: 11.282667636871338\n",
            "[42,    71] loss: 11.51292610168457\n",
            "[42,    81] loss: 12.89447751045227\n",
            "[42,    91] loss: 13.124736022949218\n",
            "[42,   101] loss: 10.822150611877442\n",
            "[42,   111] loss: 12.203701829910278\n",
            "[42,   121] loss: 9.440599346160889\n",
            "[42,   131] loss: 12.203701877593994\n",
            "[43,     1] loss: 1.381551170349121\n",
            "[43,    11] loss: 10.591892051696778\n",
            "[43,    21] loss: 9.210340738296509\n",
            "[43,    31] loss: 12.664218997955322\n",
            "[43,    41] loss: 12.203701877593994\n",
            "[43,    51] loss: 13.124736070632935\n",
            "[43,    61] loss: 10.822150564193725\n",
            "[43,    71] loss: 9.901116418838502\n",
            "[43,    81] loss: 11.743184804916382\n",
            "[43,    91] loss: 11.512926149368287\n",
            "[43,   101] loss: 11.97344331741333\n",
            "[43,   111] loss: 11.512926197052002\n",
            "[43,   121] loss: 11.973443269729614\n",
            "[43,   131] loss: 11.743184804916382\n",
            "[44,     1] loss: 1.151292610168457\n",
            "[44,    11] loss: 11.97344331741333\n",
            "[44,    21] loss: 10.822150564193725\n",
            "[44,    31] loss: 10.591892051696778\n",
            "[44,    41] loss: 11.743184852600098\n",
            "[44,    51] loss: 11.743184757232665\n",
            "[44,    61] loss: 13.124736022949218\n",
            "[44,    71] loss: 10.822150564193725\n",
            "[44,    81] loss: 8.519565057754516\n",
            "[44,    91] loss: 12.203701829910278\n",
            "[44,   101] loss: 11.512926197052002\n",
            "[44,   111] loss: 10.82215051651001\n",
            "[44,   121] loss: 12.66421890258789\n",
            "[44,   131] loss: 13.124736022949218\n",
            "[45,     1] loss: 1.151292610168457\n",
            "[45,    11] loss: 12.433960342407227\n",
            "[45,    21] loss: 11.052409172058105\n",
            "[45,    31] loss: 11.512926244735718\n",
            "[45,    41] loss: 9.901116418838502\n",
            "[45,    51] loss: 11.74318470954895\n",
            "[45,    61] loss: 12.433960390090942\n",
            "[45,    71] loss: 10.822150564193725\n",
            "[45,    81] loss: 11.512926244735718\n",
            "[45,    91] loss: 11.512926197052002\n",
            "[45,   101] loss: 12.203701877593994\n",
            "[45,   111] loss: 11.97344331741333\n",
            "[45,   121] loss: 10.591892051696778\n",
            "[45,   131] loss: 9.670857858657836\n",
            "[46,     1] loss: 0.6907755374908447\n",
            "[46,    11] loss: 12.894477462768554\n",
            "[46,    21] loss: 12.894477605819702\n",
            "[46,    31] loss: 11.052409124374389\n",
            "[46,    41] loss: 10.361633491516113\n",
            "[46,    51] loss: 8.980082130432129\n",
            "[46,    61] loss: 11.743184757232665\n",
            "[46,    71] loss: 11.743184661865234\n",
            "[46,    81] loss: 12.66421890258789\n",
            "[46,    91] loss: 12.89447751045227\n",
            "[46,   101] loss: 10.82215051651001\n",
            "[46,   111] loss: 11.282667589187621\n",
            "[46,   121] loss: 11.743184757232665\n",
            "[46,   131] loss: 11.282667684555054\n",
            "[47,     1] loss: 1.381551170349121\n",
            "[47,    11] loss: 10.361633443832398\n",
            "[47,    21] loss: 12.433960390090942\n",
            "[47,    31] loss: 11.512926149368287\n",
            "[47,    41] loss: 10.822150659561157\n",
            "[47,    51] loss: 10.131374883651734\n",
            "[47,    61] loss: 11.973443365097046\n",
            "[47,    71] loss: 11.973443222045898\n",
            "[47,    81] loss: 11.282667589187621\n",
            "[47,    91] loss: 11.512926149368287\n",
            "[47,   101] loss: 10.361633491516113\n",
            "[47,   111] loss: 12.894477558135986\n",
            "[47,   121] loss: 11.97344331741333\n",
            "[47,   131] loss: 11.512926149368287\n",
            "[48,     1] loss: 0.23025851249694823\n",
            "[48,    11] loss: 11.282667589187621\n",
            "[48,    21] loss: 12.433960342407227\n",
            "[48,    31] loss: 11.052409172058105\n",
            "[48,    41] loss: 11.282667541503907\n",
            "[48,    51] loss: 10.59189214706421\n",
            "[48,    61] loss: 12.203701877593994\n",
            "[48,    71] loss: 11.052409219741822\n",
            "[48,    81] loss: 12.664218950271607\n",
            "[48,    91] loss: 10.591892004013062\n",
            "[48,   101] loss: 11.74318470954895\n",
            "[48,   111] loss: 12.89447751045227\n",
            "[48,   121] loss: 10.591892004013062\n",
            "[48,   131] loss: 11.052409172058105\n",
            "[49,     1] loss: 1.151292610168457\n",
            "[49,    11] loss: 13.124736118316651\n",
            "[49,    21] loss: 9.210340690612792\n",
            "[49,    31] loss: 11.512926292419433\n",
            "[49,    41] loss: 12.203702020645142\n",
            "[49,    51] loss: 12.20370192527771\n",
            "[49,    61] loss: 10.131374883651734\n",
            "[49,    71] loss: 11.973443269729614\n",
            "[49,    81] loss: 10.59189190864563\n",
            "[49,    91] loss: 12.89447751045227\n",
            "[49,   101] loss: 12.894477462768554\n",
            "[49,   111] loss: 10.131374931335449\n",
            "[49,   121] loss: 12.433960390090942\n",
            "[49,   131] loss: 11.512926197052002\n",
            "[50,     1] loss: 1.151292610168457\n",
            "[50,    11] loss: 12.203701972961426\n",
            "[50,    21] loss: 13.354994630813598\n",
            "[50,    31] loss: 12.894477558135986\n",
            "[50,    41] loss: 9.670857858657836\n",
            "[50,    51] loss: 9.670857858657836\n",
            "[50,    61] loss: 11.512926197052002\n",
            "[50,    71] loss: 10.591892004013062\n",
            "[50,    81] loss: 12.203701972961426\n",
            "[50,    91] loss: 9.670857858657836\n",
            "[50,   101] loss: 11.282667684555054\n",
            "[50,   111] loss: 11.973443269729614\n",
            "[50,   121] loss: 11.97344331741333\n",
            "[50,   131] loss: 12.664218950271607\n",
            "[51,     1] loss: 1.381551170349121\n",
            "[51,    11] loss: 11.282667875289917\n",
            "[51,    21] loss: 10.361633443832398\n",
            "[51,    31] loss: 12.203701829910278\n",
            "[51,    41] loss: 10.361633396148681\n",
            "[51,    51] loss: 13.354994630813598\n",
            "[51,    61] loss: 10.82215051651001\n",
            "[51,    71] loss: 10.591891956329345\n",
            "[51,    81] loss: 13.124736166000366\n",
            "[51,    91] loss: 11.512926292419433\n",
            "[51,   101] loss: 11.512926197052002\n",
            "[51,   111] loss: 12.894477558135986\n",
            "[51,   121] loss: 10.59189214706421\n",
            "[51,   131] loss: 10.591891956329345\n",
            "[52,     1] loss: 1.151292610168457\n",
            "[52,    11] loss: 11.282667684555054\n",
            "[52,    21] loss: 11.052409172058105\n",
            "[52,    31] loss: 10.131374883651734\n",
            "[52,    41] loss: 10.82215051651001\n",
            "[52,    51] loss: 11.743184757232665\n",
            "[52,    61] loss: 10.361633443832398\n",
            "[52,    71] loss: 13.815511703491211\n",
            "[52,    81] loss: 13.354994583129884\n",
            "[52,    91] loss: 12.203701829910278\n",
            "[52,   101] loss: 10.131374883651734\n",
            "[52,   111] loss: 12.433960342407227\n",
            "[52,   121] loss: 11.512926197052002\n",
            "[52,   131] loss: 11.28266773223877\n",
            "[53,     1] loss: 1.6118097305297852\n",
            "[53,    11] loss: 13.354994630813598\n",
            "[53,    21] loss: 9.440599250793458\n",
            "[53,    31] loss: 12.66421890258789\n",
            "[53,    41] loss: 11.74318470954895\n",
            "[53,    51] loss: 12.894477462768554\n",
            "[53,    61] loss: 10.131375074386597\n",
            "[53,    71] loss: 11.282667589187621\n",
            "[53,    81] loss: 9.670858001708984\n",
            "[53,    91] loss: 11.973443412780762\n",
            "[53,   101] loss: 11.512926244735718\n",
            "[53,   111] loss: 11.973443365097046\n",
            "[53,   121] loss: 10.361633443832398\n",
            "[53,   131] loss: 11.51292610168457\n",
            "[54,     1] loss: 1.151292610168457\n",
            "[54,    11] loss: 8.980082178115845\n",
            "[54,    21] loss: 13.124736022949218\n",
            "[54,    31] loss: 10.13137502670288\n",
            "[54,    41] loss: 10.131374979019165\n",
            "[54,    51] loss: 9.210340785980225\n",
            "[54,    61] loss: 11.743184757232665\n",
            "[54,    71] loss: 12.664218950271607\n",
            "[54,    81] loss: 12.433960485458375\n",
            "[54,    91] loss: 11.512926149368287\n",
            "[54,   101] loss: 13.354994583129884\n",
            "[54,   111] loss: 11.282667684555054\n",
            "[54,   121] loss: 11.512926149368287\n",
            "[54,   131] loss: 12.894477462768554\n",
            "[55,     1] loss: 1.381551170349121\n",
            "[55,    11] loss: 11.282667541503907\n",
            "[55,    21] loss: 11.743184804916382\n",
            "[55,    31] loss: 11.512926244735718\n",
            "[55,    41] loss: 11.743184804916382\n",
            "[55,    51] loss: 12.203701877593994\n",
            "[55,    61] loss: 11.97344331741333\n",
            "[55,    71] loss: 10.361633443832398\n",
            "[55,    81] loss: 11.97344331741333\n",
            "[55,    91] loss: 13.124736118316651\n",
            "[55,   101] loss: 10.822150564193725\n",
            "[55,   111] loss: 11.052409076690674\n",
            "[55,   121] loss: 10.131374835968018\n",
            "[55,   131] loss: 12.89447751045227\n",
            "[56,     1] loss: 0.6907755374908447\n",
            "[56,    11] loss: 12.894477558135986\n",
            "[56,    21] loss: 12.203701877593994\n",
            "[56,    31] loss: 11.052409267425537\n",
            "[56,    41] loss: 11.973443365097046\n",
            "[56,    51] loss: 10.361633443832398\n",
            "[56,    61] loss: 11.512926197052002\n",
            "[56,    71] loss: 12.203701782226563\n",
            "[56,    81] loss: 11.74318470954895\n",
            "[56,    91] loss: 11.282667636871338\n",
            "[56,   101] loss: 10.822150611877442\n",
            "[56,   111] loss: 12.433960437774658\n",
            "[56,   121] loss: 11.973443269729614\n",
            "[56,   131] loss: 11.512926149368287\n",
            "[57,     1] loss: 0.9210340499877929\n",
            "[57,    11] loss: 11.743184757232665\n",
            "[57,    21] loss: 10.591892051696778\n",
            "[57,    31] loss: 12.89447751045227\n",
            "[57,    41] loss: 11.512926197052002\n",
            "[57,    51] loss: 11.512926149368287\n",
            "[57,    61] loss: 12.894477558135986\n",
            "[57,    71] loss: 12.203701877593994\n",
            "[57,    81] loss: 9.440599298477172\n",
            "[57,    91] loss: 9.440599298477172\n",
            "[57,   101] loss: 13.124736022949218\n",
            "[57,   111] loss: 12.894477558135986\n",
            "[57,   121] loss: 8.749823760986327\n",
            "[57,   131] loss: 10.82215051651001\n",
            "[58,     1] loss: 1.6118097305297852\n",
            "[58,    11] loss: 10.591892004013062\n",
            "[58,    21] loss: 11.28266773223877\n",
            "[58,    31] loss: 13.124736022949218\n",
            "[58,    41] loss: 11.052409029006958\n",
            "[58,    51] loss: 14.506287384033204\n",
            "[58,    61] loss: 10.822150421142577\n",
            "[58,    71] loss: 12.664218950271607\n",
            "[58,    81] loss: 11.51292610168457\n",
            "[58,    91] loss: 11.052409172058105\n",
            "[58,   101] loss: 9.901116371154785\n",
            "[58,   111] loss: 11.743184852600098\n",
            "[58,   121] loss: 10.591892051696778\n",
            "[58,   131] loss: 10.131375074386597\n",
            "[59,     1] loss: 1.381551170349121\n",
            "[59,    11] loss: 11.282667636871338\n",
            "[59,    21] loss: 11.052409124374389\n",
            "[59,    31] loss: 11.052409029006958\n",
            "[59,    41] loss: 11.28266773223877\n",
            "[59,    51] loss: 11.743184852600098\n",
            "[59,    61] loss: 11.973443269729614\n",
            "[59,    71] loss: 12.433960342407227\n",
            "[59,    81] loss: 12.203702020645142\n",
            "[59,    91] loss: 10.591892004013062\n",
            "[59,   101] loss: 12.89447751045227\n",
            "[59,   111] loss: 11.282667541503907\n",
            "[59,   121] loss: 10.131374883651734\n",
            "[59,   131] loss: 11.973443365097046\n",
            "[60,     1] loss: 1.151292610168457\n",
            "[60,    11] loss: 10.822150564193725\n",
            "[60,    21] loss: 12.203701972961426\n",
            "[60,    31] loss: 11.74318470954895\n",
            "[60,    41] loss: 12.66421890258789\n",
            "[60,    51] loss: 10.822150659561157\n",
            "[60,    61] loss: 11.743184804916382\n",
            "[60,    71] loss: 11.512926197052002\n",
            "[60,    81] loss: 11.282667541503907\n",
            "[60,    91] loss: 10.36163363456726\n",
            "[60,   101] loss: 11.28266773223877\n",
            "[60,   111] loss: 10.822150611877442\n",
            "[60,   121] loss: 10.82215051651001\n",
            "[60,   131] loss: 12.664218950271607\n",
            "[61,     1] loss: 0.9210340499877929\n",
            "[61,    11] loss: 11.973443269729614\n",
            "[61,    21] loss: 11.512926197052002\n",
            "[61,    31] loss: 11.052409029006958\n",
            "[61,    41] loss: 12.664218950271607\n",
            "[61,    51] loss: 10.361633491516113\n",
            "[61,    61] loss: 10.822150564193725\n",
            "[61,    71] loss: 12.664218950271607\n",
            "[61,    81] loss: 13.124736070632935\n",
            "[61,    91] loss: 10.82215051651001\n",
            "[61,   101] loss: 11.97344331741333\n",
            "[61,   111] loss: 12.203701877593994\n",
            "[61,   121] loss: 11.512926244735718\n",
            "[61,   131] loss: 9.670857810974121\n",
            "[62,     1] loss: 0.9210340499877929\n",
            "[62,    11] loss: 10.361633396148681\n",
            "[62,    21] loss: 10.361633539199829\n",
            "[62,    31] loss: 13.124736070632935\n",
            "[62,    41] loss: 12.89447751045227\n",
            "[62,    51] loss: 13.354994583129884\n",
            "[62,    61] loss: 12.433960437774658\n",
            "[62,    71] loss: 11.282667684555054\n",
            "[62,    81] loss: 11.512926244735718\n",
            "[62,    91] loss: 10.361633539199829\n",
            "[62,   101] loss: 8.98008222579956\n",
            "[62,   111] loss: 13.124736022949218\n",
            "[62,   121] loss: 10.82215051651001\n",
            "[62,   131] loss: 11.74318470954895\n",
            "[63,     1] loss: 0.6907755374908447\n",
            "[63,    11] loss: 11.512926244735718\n",
            "[63,    21] loss: 10.822150659561157\n",
            "[63,    31] loss: 13.124736022949218\n",
            "[63,    41] loss: 12.664219045639038\n",
            "[63,    51] loss: 12.664218950271607\n",
            "[63,    61] loss: 12.433960390090942\n",
            "[63,    71] loss: 10.131374931335449\n",
            "[63,    81] loss: 10.82215051651001\n",
            "[63,    91] loss: 11.512926244735718\n",
            "[63,   101] loss: 10.822150564193725\n",
            "[63,   111] loss: 11.97344331741333\n",
            "[63,   121] loss: 11.282667636871338\n",
            "[63,   131] loss: 11.97344331741333\n",
            "[64,     1] loss: 1.151292610168457\n",
            "[64,    11] loss: 10.131374979019165\n",
            "[64,    21] loss: 10.822150421142577\n",
            "[64,    31] loss: 10.822150468826294\n",
            "[64,    41] loss: 9.670857906341553\n",
            "[64,    51] loss: 11.282667684555054\n",
            "[64,    61] loss: 11.973443269729614\n",
            "[64,    71] loss: 12.664218997955322\n",
            "[64,    81] loss: 12.433960580825806\n",
            "[64,    91] loss: 12.433960437774658\n",
            "[64,   101] loss: 10.591892099380493\n",
            "[64,   111] loss: 12.89447751045227\n",
            "[64,   121] loss: 11.282667589187621\n",
            "[64,   131] loss: 12.89447751045227\n",
            "[65,     1] loss: 1.151292610168457\n",
            "[65,    11] loss: 10.361633396148681\n",
            "[65,    21] loss: 11.052409029006958\n",
            "[65,    31] loss: 10.822150564193725\n",
            "[65,    41] loss: 11.28266773223877\n",
            "[65,    51] loss: 11.512926149368287\n",
            "[65,    61] loss: 11.97344331741333\n",
            "[65,    71] loss: 9.901116418838502\n",
            "[65,    81] loss: 11.973443412780762\n",
            "[65,    91] loss: 11.052409172058105\n",
            "[65,   101] loss: 11.97344331741333\n",
            "[65,   111] loss: 12.89447751045227\n",
            "[65,   121] loss: 12.20370192527771\n",
            "[65,   131] loss: 10.82215051651001\n",
            "[66,     1] loss: 0.9210340499877929\n",
            "[66,    11] loss: 10.822150421142577\n",
            "[66,    21] loss: 10.591892004013062\n",
            "[66,    31] loss: 11.512926244735718\n",
            "[66,    41] loss: 11.052409219741822\n",
            "[66,    51] loss: 13.354994583129884\n",
            "[66,    61] loss: 13.585253238677979\n",
            "[66,    71] loss: 10.59189214706421\n",
            "[66,    81] loss: 11.052409124374389\n",
            "[66,    91] loss: 11.052409029006958\n",
            "[66,   101] loss: 9.440599489212037\n",
            "[66,   111] loss: 11.052409076690674\n",
            "[66,   121] loss: 10.131374931335449\n",
            "[66,   131] loss: 13.585253190994262\n",
            "[67,     1] loss: 1.381551170349121\n",
            "[67,    11] loss: 7.8287896633148195\n",
            "[67,    21] loss: 10.822150659561157\n",
            "[67,    31] loss: 12.89447751045227\n",
            "[67,    41] loss: 9.901116323471069\n",
            "[67,    51] loss: 11.743184661865234\n",
            "[67,    61] loss: 10.131374883651734\n",
            "[67,    71] loss: 14.276028823852538\n",
            "[67,    81] loss: 11.282667636871338\n",
            "[67,    91] loss: 11.282667589187621\n",
            "[67,   101] loss: 10.591892004013062\n",
            "[67,   111] loss: 12.66421890258789\n",
            "[67,   121] loss: 12.433960390090942\n",
            "[67,   131] loss: 13.585253238677979\n",
            "[68,     1] loss: 1.8420682907104493\n",
            "[68,    11] loss: 10.131374883651734\n",
            "[68,    21] loss: 11.282667684555054\n",
            "[68,    31] loss: 11.052409076690674\n",
            "[68,    41] loss: 13.585253190994262\n",
            "[68,    51] loss: 10.361633539199829\n",
            "[68,    61] loss: 12.433960437774658\n",
            "[68,    71] loss: 10.361633539199829\n",
            "[68,    81] loss: 11.97344331741333\n",
            "[68,    91] loss: 11.282667636871338\n",
            "[68,   101] loss: 10.822150659561157\n",
            "[68,   111] loss: 12.20370192527771\n",
            "[68,   121] loss: 14.506287384033204\n",
            "[68,   131] loss: 8.749823713302613\n",
            "[69,     1] loss: 1.381551170349121\n",
            "[69,    11] loss: 11.052409029006958\n",
            "[69,    21] loss: 11.282667636871338\n",
            "[69,    31] loss: 12.203701877593994\n",
            "[69,    41] loss: 11.512926149368287\n",
            "[69,    51] loss: 11.052409172058105\n",
            "[69,    61] loss: 12.433960342407227\n",
            "[69,    71] loss: 12.894477558135986\n",
            "[69,    81] loss: 11.512926244735718\n",
            "[69,    91] loss: 12.66421890258789\n",
            "[69,   101] loss: 12.894477462768554\n",
            "[69,   111] loss: 9.670857906341553\n",
            "[69,   121] loss: 11.512926149368287\n",
            "[69,   131] loss: 10.361633396148681\n",
            "[70,     1] loss: 1.381551170349121\n",
            "[70,    11] loss: 10.59189190864563\n",
            "[70,    21] loss: 11.51292634010315\n",
            "[70,    31] loss: 11.512926197052002\n",
            "[70,    41] loss: 10.82215051651001\n",
            "[70,    51] loss: 11.052409029006958\n",
            "[70,    61] loss: 11.973443365097046\n",
            "[70,    71] loss: 12.894477462768554\n",
            "[70,    81] loss: 12.433960342407227\n",
            "[70,    91] loss: 10.131374883651734\n",
            "[70,   101] loss: 9.67085771560669\n",
            "[70,   111] loss: 10.591892051696778\n",
            "[70,   121] loss: 12.89447751045227\n",
            "[70,   131] loss: 11.973443365097046\n",
            "[71,     1] loss: 1.381551170349121\n",
            "[71,    11] loss: 11.282667636871338\n",
            "[71,    21] loss: 11.052409124374389\n",
            "[71,    31] loss: 11.512926292419433\n",
            "[71,    41] loss: 10.361633348464967\n",
            "[71,    51] loss: 11.052409172058105\n",
            "[71,    61] loss: 11.512926149368287\n",
            "[71,    71] loss: 12.203701877593994\n",
            "[71,    81] loss: 10.822150468826294\n",
            "[71,    91] loss: 11.743184661865234\n",
            "[71,   101] loss: 14.045770263671875\n",
            "[71,   111] loss: 11.282667589187621\n",
            "[71,   121] loss: 10.59189214706421\n",
            "[71,   131] loss: 12.203701877593994\n",
            "[72,     1] loss: 1.6118097305297852\n",
            "[72,    11] loss: 9.210340785980225\n",
            "[72,    21] loss: 11.282667779922486\n",
            "[72,    31] loss: 12.203701972961426\n",
            "[72,    41] loss: 12.664218950271607\n",
            "[72,    51] loss: 10.822150611877442\n",
            "[72,    61] loss: 11.512926292419433\n",
            "[72,    71] loss: 12.203701829910278\n",
            "[72,    81] loss: 11.28266773223877\n",
            "[72,    91] loss: 12.664219045639038\n",
            "[72,   101] loss: 12.89447751045227\n",
            "[72,   111] loss: 10.361633443832398\n",
            "[72,   121] loss: 11.052409124374389\n",
            "[72,   131] loss: 10.361633443832398\n",
            "[73,     1] loss: 0.9210340499877929\n",
            "[73,    11] loss: 11.282667589187621\n",
            "[73,    21] loss: 11.052409267425537\n",
            "[73,    31] loss: 10.591892004013062\n",
            "[73,    41] loss: 12.433960437774658\n",
            "[73,    51] loss: 12.203701829910278\n",
            "[73,    61] loss: 9.901116371154785\n",
            "[73,    71] loss: 10.591891956329345\n",
            "[73,    81] loss: 12.433960342407227\n",
            "[73,    91] loss: 10.591892099380493\n",
            "[73,   101] loss: 11.052409219741822\n",
            "[73,   111] loss: 10.822150564193725\n",
            "[73,   121] loss: 14.736545944213868\n",
            "[73,   131] loss: 11.052409124374389\n",
            "[74,     1] loss: 1.151292610168457\n",
            "[74,    11] loss: 13.124736070632935\n",
            "[74,    21] loss: 11.743184804916382\n",
            "[74,    31] loss: 11.973443365097046\n",
            "[74,    41] loss: 11.512926197052002\n",
            "[74,    51] loss: 11.97344331741333\n",
            "[74,    61] loss: 10.361633443832398\n",
            "[74,    71] loss: 11.512926244735718\n",
            "[74,    81] loss: 11.512926197052002\n",
            "[74,    91] loss: 10.591891956329345\n",
            "[74,   101] loss: 11.512926292419433\n",
            "[74,   111] loss: 10.82215051651001\n",
            "[74,   121] loss: 11.973443365097046\n",
            "[74,   131] loss: 10.822150468826294\n",
            "[75,     1] loss: 0.9210340499877929\n",
            "[75,    11] loss: 11.282667684555054\n",
            "[75,    21] loss: 13.354994583129884\n",
            "[75,    31] loss: 11.282667779922486\n",
            "[75,    41] loss: 11.282667779922486\n",
            "[75,    51] loss: 12.66421890258789\n",
            "[75,    61] loss: 11.052409172058105\n",
            "[75,    71] loss: 11.052409029006958\n",
            "[75,    81] loss: 10.131374979019165\n",
            "[75,    91] loss: 10.82215051651001\n",
            "[75,   101] loss: 14.045770263671875\n",
            "[75,   111] loss: 13.354994583129884\n",
            "[75,   121] loss: 11.282667636871338\n",
            "[75,   131] loss: 9.901116371154785\n",
            "[76,     1] loss: 1.381551170349121\n",
            "[76,    11] loss: 10.822150421142577\n",
            "[76,    21] loss: 12.20370192527771\n",
            "[76,    31] loss: 10.591892051696778\n",
            "[76,    41] loss: 11.973443269729614\n",
            "[76,    51] loss: 10.131374740600586\n",
            "[76,    61] loss: 11.973443365097046\n",
            "[76,    71] loss: 12.203701782226563\n",
            "[76,    81] loss: 11.282667636871338\n",
            "[76,    91] loss: 10.361633586883546\n",
            "[76,   101] loss: 12.89447751045227\n",
            "[76,   111] loss: 14.045770263671875\n",
            "[76,   121] loss: 11.743184804916382\n",
            "[76,   131] loss: 8.289306640625\n",
            "[77,     1] loss: 0.9210340499877929\n",
            "[77,    11] loss: 12.433960390090942\n",
            "[77,    21] loss: 11.282667684555054\n",
            "[77,    31] loss: 11.743184757232665\n",
            "[77,    41] loss: 11.052409172058105\n",
            "[77,    51] loss: 9.67085771560669\n",
            "[77,    61] loss: 12.664218997955322\n",
            "[77,    71] loss: 10.131374931335449\n",
            "[77,    81] loss: 10.82215051651001\n",
            "[77,    91] loss: 12.664218950271607\n",
            "[77,   101] loss: 11.97344331741333\n",
            "[77,   111] loss: 11.28266773223877\n",
            "[77,   121] loss: 12.66421890258789\n",
            "[77,   131] loss: 11.052409029006958\n",
            "[78,     1] loss: 1.151292610168457\n",
            "[78,    11] loss: 10.822150564193725\n",
            "[78,    21] loss: 11.74318470954895\n",
            "[78,    31] loss: 13.124736022949218\n",
            "[78,    41] loss: 9.901116466522216\n",
            "[78,    51] loss: 11.282667684555054\n",
            "[78,    61] loss: 9.670857906341553\n",
            "[78,    71] loss: 12.203701782226563\n",
            "[78,    81] loss: 11.74318470954895\n",
            "[78,    91] loss: 12.664218997955322\n",
            "[78,   101] loss: 11.973443269729614\n",
            "[78,   111] loss: 9.440599298477172\n",
            "[78,   121] loss: 12.66421890258789\n",
            "[78,   131] loss: 12.203701829910278\n",
            "[79,     1] loss: 0.6907755374908447\n",
            "[79,    11] loss: 11.052409172058105\n",
            "[79,    21] loss: 11.052409124374389\n",
            "[79,    31] loss: 11.512926292419433\n",
            "[79,    41] loss: 12.203701782226563\n",
            "[79,    51] loss: 11.052409172058105\n",
            "[79,    61] loss: 11.512926149368287\n",
            "[79,    71] loss: 10.591892099380493\n",
            "[79,    81] loss: 13.124736118316651\n",
            "[79,    91] loss: 10.591892004013062\n",
            "[79,   101] loss: 11.052409076690674\n",
            "[79,   111] loss: 9.901116514205933\n",
            "[79,   121] loss: 13.585253238677979\n",
            "[79,   131] loss: 12.66421890258789\n",
            "[80,     1] loss: 0.9210340499877929\n",
            "[80,    11] loss: 12.203701829910278\n",
            "[80,    21] loss: 9.440599298477172\n",
            "[80,    31] loss: 9.440599346160889\n",
            "[80,    41] loss: 10.822150611877442\n",
            "[80,    51] loss: 11.052408981323243\n",
            "[80,    61] loss: 13.124736118316651\n",
            "[80,    71] loss: 11.743184661865234\n",
            "[80,    81] loss: 11.51292634010315\n",
            "[80,    91] loss: 12.664218950271607\n",
            "[80,   101] loss: 10.361633539199829\n",
            "[80,   111] loss: 11.97344331741333\n",
            "[80,   121] loss: 14.276028823852538\n",
            "[80,   131] loss: 9.901116323471069\n",
            "[81,     1] loss: 1.151292610168457\n",
            "[81,    11] loss: 12.433960390090942\n",
            "[81,    21] loss: 10.822150468826294\n",
            "[81,    31] loss: 11.512926244735718\n",
            "[81,    41] loss: 9.901116371154785\n",
            "[81,    51] loss: 12.664218997955322\n",
            "[81,    61] loss: 11.973443460464477\n",
            "[81,    71] loss: 12.203701829910278\n",
            "[81,    81] loss: 11.743184852600098\n",
            "[81,    91] loss: 11.97344331741333\n",
            "[81,   101] loss: 11.052409029006958\n",
            "[81,   111] loss: 9.440599203109741\n",
            "[81,   121] loss: 9.440599203109741\n",
            "[81,   131] loss: 13.585253143310547\n",
            "[82,     1] loss: 0.9210340499877929\n",
            "[82,    11] loss: 11.512926197052002\n",
            "[82,    21] loss: 9.901116275787354\n",
            "[82,    31] loss: 10.361633491516113\n",
            "[82,    41] loss: 14.045770263671875\n",
            "[82,    51] loss: 11.282667684555054\n",
            "[82,    61] loss: 11.052409124374389\n",
            "[82,    71] loss: 11.973443269729614\n",
            "[82,    81] loss: 12.203701782226563\n",
            "[82,    91] loss: 12.433960390090942\n",
            "[82,   101] loss: 10.131374835968018\n",
            "[82,   111] loss: 12.203701782226563\n",
            "[82,   121] loss: 12.664218950271607\n",
            "[82,   131] loss: 8.98008222579956\n",
            "[83,     1] loss: 1.6118097305297852\n",
            "[83,    11] loss: 13.124736070632935\n",
            "[83,    21] loss: 11.052409124374389\n",
            "[83,    31] loss: 10.82215051651001\n",
            "[83,    41] loss: 10.591891956329345\n",
            "[83,    51] loss: 12.433960342407227\n",
            "[83,    61] loss: 10.361633396148681\n",
            "[83,    71] loss: 11.512926149368287\n",
            "[83,    81] loss: 11.28266773223877\n",
            "[83,    91] loss: 13.124736166000366\n",
            "[83,   101] loss: 10.822150611877442\n",
            "[83,   111] loss: 10.361633682250977\n",
            "[83,   121] loss: 9.901116371154785\n",
            "[83,   131] loss: 12.894477558135986\n",
            "[84,     1] loss: 1.151292610168457\n",
            "[84,    11] loss: 12.203701829910278\n",
            "[84,    21] loss: 11.512926149368287\n",
            "[84,    31] loss: 10.82215051651001\n",
            "[84,    41] loss: 11.512926244735718\n",
            "[84,    51] loss: 11.512926197052002\n",
            "[84,    61] loss: 10.822150564193725\n",
            "[84,    71] loss: 13.354994583129884\n",
            "[84,    81] loss: 11.28266773223877\n",
            "[84,    91] loss: 11.282667636871338\n",
            "[84,   101] loss: 10.591892051696778\n",
            "[84,   111] loss: 12.894477462768554\n",
            "[84,   121] loss: 10.822150564193725\n",
            "[84,   131] loss: 10.822150564193725\n",
            "[85,     1] loss: 0.6907755374908447\n",
            "[85,    11] loss: 11.052409172058105\n",
            "[85,    21] loss: 11.74318470954895\n",
            "[85,    31] loss: 10.13137502670288\n",
            "[85,    41] loss: 14.045770263671875\n",
            "[85,    51] loss: 14.045770263671875\n",
            "[85,    61] loss: 12.66421890258789\n",
            "[85,    71] loss: 12.20370192527771\n",
            "[85,    81] loss: 12.66421890258789\n",
            "[85,    91] loss: 10.13137502670288\n",
            "[85,   101] loss: 9.670857763290405\n",
            "[85,   111] loss: 10.591892099380493\n",
            "[85,   121] loss: 9.67085795402527\n",
            "[85,   131] loss: 11.052409076690674\n",
            "[86,     1] loss: 1.381551170349121\n",
            "[86,    11] loss: 12.894477462768554\n",
            "[86,    21] loss: 11.973443365097046\n",
            "[86,    31] loss: 12.203702020645142\n",
            "[86,    41] loss: 13.585253190994262\n",
            "[86,    51] loss: 10.591892051696778\n",
            "[86,    61] loss: 8.749823665618896\n",
            "[86,    71] loss: 10.591892004013062\n",
            "[86,    81] loss: 9.440599393844604\n",
            "[86,    91] loss: 11.512926197052002\n",
            "[86,   101] loss: 11.052409267425537\n",
            "[86,   111] loss: 12.894477462768554\n",
            "[86,   121] loss: 11.512926244735718\n",
            "[86,   131] loss: 10.82215051651001\n",
            "[87,     1] loss: 1.381551170349121\n",
            "[87,    11] loss: 11.973443412780762\n",
            "[87,    21] loss: 10.591892004013062\n",
            "[87,    31] loss: 11.973443412780762\n",
            "[87,    41] loss: 10.591892099380493\n",
            "[87,    51] loss: 11.052409029006958\n",
            "[87,    61] loss: 10.82215051651001\n",
            "[87,    71] loss: 10.82215051651001\n",
            "[87,    81] loss: 10.131374883651734\n",
            "[87,    91] loss: 11.28266773223877\n",
            "[87,   101] loss: 12.203701782226563\n",
            "[87,   111] loss: 12.203701782226563\n",
            "[87,   121] loss: 13.585253143310547\n",
            "[87,   131] loss: 10.822150564193725\n",
            "[88,     1] loss: 1.381551170349121\n",
            "[88,    11] loss: 11.282667589187621\n",
            "[88,    21] loss: 10.361633539199829\n",
            "[88,    31] loss: 11.28266773223877\n",
            "[88,    41] loss: 12.20370192527771\n",
            "[88,    51] loss: 12.664219093322753\n",
            "[88,    61] loss: 11.51292610168457\n",
            "[88,    71] loss: 11.743184757232665\n",
            "[88,    81] loss: 10.131374931335449\n",
            "[88,    91] loss: 11.512926197052002\n",
            "[88,   101] loss: 11.052409219741822\n",
            "[88,   111] loss: 10.131375074386597\n",
            "[88,   121] loss: 10.82215051651001\n",
            "[88,   131] loss: 12.894477462768554\n",
            "[89,     1] loss: 1.151292610168457\n",
            "[89,    11] loss: 11.512926244735718\n",
            "[89,    21] loss: 10.822150611877442\n",
            "[89,    31] loss: 11.973443412780762\n",
            "[89,    41] loss: 10.82215051651001\n",
            "[89,    51] loss: 9.901116418838502\n",
            "[89,    61] loss: 9.901116323471069\n",
            "[89,    71] loss: 11.512926149368287\n",
            "[89,    81] loss: 11.97344355583191\n",
            "[89,    91] loss: 13.815511703491211\n",
            "[89,   101] loss: 13.124736022949218\n",
            "[89,   111] loss: 12.664218997955322\n",
            "[89,   121] loss: 11.973443365097046\n",
            "[89,   131] loss: 10.361633396148681\n",
            "[90,     1] loss: 0.23025851249694823\n",
            "[90,    11] loss: 12.433960437774658\n",
            "[90,    21] loss: 14.276028871536255\n",
            "[90,    31] loss: 9.440599250793458\n",
            "[90,    41] loss: 8.980082130432129\n",
            "[90,    51] loss: 13.585253190994262\n",
            "[90,    61] loss: 9.901116418838502\n",
            "[90,    71] loss: 13.815511703491211\n",
            "[90,    81] loss: 10.82215051651001\n",
            "[90,    91] loss: 12.664218997955322\n",
            "[90,   101] loss: 10.822150611877442\n",
            "[90,   111] loss: 11.052409076690674\n",
            "[90,   121] loss: 11.282667684555054\n",
            "[90,   131] loss: 11.282667684555054\n",
            "[91,     1] loss: 1.381551170349121\n",
            "[91,    11] loss: 11.973443269729614\n",
            "[91,    21] loss: 11.512926149368287\n",
            "[91,    31] loss: 11.97344331741333\n",
            "[91,    41] loss: 12.89447751045227\n",
            "[91,    51] loss: 8.289306545257569\n",
            "[91,    61] loss: 12.66421890258789\n",
            "[91,    71] loss: 8.519565153121949\n",
            "[91,    81] loss: 11.743184757232665\n",
            "[91,    91] loss: 11.973443460464477\n",
            "[91,   101] loss: 11.973443222045898\n",
            "[91,   111] loss: 13.124736070632935\n",
            "[91,   121] loss: 11.282667636871338\n",
            "[91,   131] loss: 9.670857810974121\n",
            "[92,     1] loss: 1.381551170349121\n",
            "[92,    11] loss: 8.059048175811768\n",
            "[92,    21] loss: 13.815511703491211\n",
            "[92,    31] loss: 11.512926244735718\n",
            "[92,    41] loss: 11.512926149368287\n",
            "[92,    51] loss: 10.82215051651001\n",
            "[92,    61] loss: 12.664218950271607\n",
            "[92,    71] loss: 11.052409124374389\n",
            "[92,    81] loss: 11.512926197052002\n",
            "[92,    91] loss: 9.210340785980225\n",
            "[92,   101] loss: 11.052409076690674\n",
            "[92,   111] loss: 13.585253190994262\n",
            "[92,   121] loss: 12.433960390090942\n",
            "[92,   131] loss: 12.89447751045227\n",
            "[93,     1] loss: 0.23025851249694823\n",
            "[93,    11] loss: 11.052409124374389\n",
            "[93,    21] loss: 10.131374931335449\n",
            "[93,    31] loss: 10.82215051651001\n",
            "[93,    41] loss: 11.743184757232665\n",
            "[93,    51] loss: 11.282667541503907\n",
            "[93,    61] loss: 13.815511703491211\n",
            "[93,    71] loss: 12.203701877593994\n",
            "[93,    81] loss: 9.901116418838502\n",
            "[93,    91] loss: 11.052409076690674\n",
            "[93,   101] loss: 13.585253143310547\n",
            "[93,   111] loss: 13.354994583129884\n",
            "[93,   121] loss: 10.591892004013062\n",
            "[93,   131] loss: 10.591892004013062\n",
            "[94,     1] loss: 1.6118097305297852\n",
            "[94,    11] loss: 9.901116323471069\n",
            "[94,    21] loss: 11.512926149368287\n",
            "[94,    31] loss: 11.973443269729614\n",
            "[94,    41] loss: 10.361633491516113\n",
            "[94,    51] loss: 8.980082321166993\n",
            "[94,    61] loss: 10.822150611877442\n",
            "[94,    71] loss: 12.20370192527771\n",
            "[94,    81] loss: 10.591891956329345\n",
            "[94,    91] loss: 13.815511751174927\n",
            "[94,   101] loss: 12.433960437774658\n",
            "[94,   111] loss: 13.124736070632935\n",
            "[94,   121] loss: 11.052409076690674\n",
            "[94,   131] loss: 11.743184757232665\n",
            "[95,     1] loss: 0.9210340499877929\n",
            "[95,    11] loss: 11.512926149368287\n",
            "[95,    21] loss: 11.512926149368287\n",
            "[95,    31] loss: 12.664218950271607\n",
            "[95,    41] loss: 11.282667684555054\n",
            "[95,    51] loss: 11.282667636871338\n",
            "[95,    61] loss: 11.282667636871338\n",
            "[95,    71] loss: 11.51292610168457\n",
            "[95,    81] loss: 8.289306688308717\n",
            "[95,    91] loss: 12.433960342407227\n",
            "[95,   101] loss: 11.74318470954895\n",
            "[95,   111] loss: 11.052409172058105\n",
            "[95,   121] loss: 11.512926197052002\n",
            "[95,   131] loss: 11.512926149368287\n",
            "[96,     1] loss: 0.9210340499877929\n",
            "[96,    11] loss: 10.82215051651001\n",
            "[96,    21] loss: 12.894477462768554\n",
            "[96,    31] loss: 12.89447751045227\n",
            "[96,    41] loss: 9.440599298477172\n",
            "[96,    51] loss: 11.512926149368287\n",
            "[96,    61] loss: 11.052409124374389\n",
            "[96,    71] loss: 12.20370192527771\n",
            "[96,    81] loss: 11.743184661865234\n",
            "[96,    91] loss: 11.743184757232665\n",
            "[96,   101] loss: 11.973443365097046\n",
            "[96,   111] loss: 12.66421890258789\n",
            "[96,   121] loss: 10.822150659561157\n",
            "[96,   131] loss: 9.901116466522216\n",
            "[97,     1] loss: 1.381551170349121\n",
            "[97,    11] loss: 11.282667636871338\n",
            "[97,    21] loss: 11.28266773223877\n",
            "[97,    31] loss: 11.052409029006958\n",
            "[97,    41] loss: 11.743184757232665\n",
            "[97,    51] loss: 12.203701829910278\n",
            "[97,    61] loss: 9.901116418838502\n",
            "[97,    71] loss: 10.822150611877442\n",
            "[97,    81] loss: 12.894477558135986\n",
            "[97,    91] loss: 12.664218950271607\n",
            "[97,   101] loss: 11.512926197052002\n",
            "[97,   111] loss: 11.282667684555054\n",
            "[97,   121] loss: 10.59189224243164\n",
            "[97,   131] loss: 11.28266773223877\n",
            "[98,     1] loss: 0.9210340499877929\n",
            "[98,    11] loss: 11.282667589187621\n",
            "[98,    21] loss: 10.131374931335449\n",
            "[98,    31] loss: 11.512926197052002\n",
            "[98,    41] loss: 11.052409029006958\n",
            "[98,    51] loss: 11.74318470954895\n",
            "[98,    61] loss: 12.433960437774658\n",
            "[98,    71] loss: 11.743184757232665\n",
            "[98,    81] loss: 11.512926149368287\n",
            "[98,    91] loss: 13.815511703491211\n",
            "[98,   101] loss: 9.21034083366394\n",
            "[98,   111] loss: 11.973443269729614\n",
            "[98,   121] loss: 9.67085795402527\n",
            "[98,   131] loss: 11.973443365097046\n",
            "[99,     1] loss: 0.6907755374908447\n",
            "[99,    11] loss: 11.512926197052002\n",
            "[99,    21] loss: 12.203701877593994\n",
            "[99,    31] loss: 11.052409076690674\n",
            "[99,    41] loss: 10.361633539199829\n",
            "[99,    51] loss: 13.354994630813598\n",
            "[99,    61] loss: 11.51292610168457\n",
            "[99,    71] loss: 9.210340738296509\n",
            "[99,    81] loss: 11.282667636871338\n",
            "[99,    91] loss: 10.822150707244873\n",
            "[99,   101] loss: 12.664219093322753\n",
            "[99,   111] loss: 12.203701877593994\n",
            "[99,   121] loss: 11.282667684555054\n",
            "[99,   131] loss: 13.124736022949218\n",
            "[100,     1] loss: 1.151292610168457\n",
            "[100,    11] loss: 12.433960390090942\n",
            "[100,    21] loss: 10.822150564193725\n",
            "[100,    31] loss: 10.59189190864563\n",
            "[100,    41] loss: 12.433960390090942\n",
            "[100,    51] loss: 13.124736070632935\n",
            "[100,    61] loss: 11.973443222045898\n",
            "[100,    71] loss: 10.822150564193725\n",
            "[100,    81] loss: 10.82215051651001\n",
            "[100,    91] loss: 12.203701877593994\n",
            "[100,   101] loss: 9.901116371154785\n",
            "[100,   111] loss: 10.822150707244873\n",
            "[100,   121] loss: 11.512926149368287\n",
            "[100,   131] loss: 11.512926149368287\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        #print(outputs[0].shape)\n",
        "        #print(outputs[1].shape)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        preds, predsid = torch.max(output,1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predsid == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "XjjcoqJLoqDC",
        "outputId": "7a8582ca-7e58-4a3b-971e-4dccfe0c402e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-62cd6c10aeaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredsid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy of the network on the 10000 test images: {100 * correct // total} %'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    }
  ]
}