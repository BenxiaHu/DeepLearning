{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvK952ofkbMAkXnc8eKiHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenxiaHu/DeepLearning/blob/main/pytorch_1D_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dJBlpMEuYtuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "\n",
        "Dependencies:\n",
        "* torch: 0.1.11\n",
        "* matplotlib"
      ],
      "metadata": {
        "id": "xt7e8o5FYwDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step0: import python packages"
      ],
      "metadata": {
        "id": "ZiTV7aUDZq2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fun\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import requests\n",
        "from torch.nn import LogSoftmax\n",
        "torch.manual_seed(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJn2LrnY_Oz",
        "outputId": "f4c6db0c-4cc3-4ad7-fcf2-a09b1086fc8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efe0c777c70>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step1: import data"
      ],
      "metadata": {
        "id": "7bEJ9iCYZxe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\"\n",
        "print(seqid)\n",
        "seqid = pd.read_csv(seqid,header=0,sep=\"\\t\")\n",
        "seqid[\"peakid\"] = seqid['chr'] +\":\"+ seqid[\"start\"].astype(str) +\"-\"+ seqid[\"end\"].astype(str)\n",
        "print(seqid)\n",
        "seqid = seqid[[\"peakid\",\"silencer\"]]\n",
        "print(seqid)\n",
        "\n",
        "fastaid=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.fasta\"\n",
        "fastaid = pd.read_table(fastaid,header=None,sep=\"\\t\")\n",
        "print(fastaid)\n",
        "fastaid.rename(columns={0:\"peakid\",1:\"fasta\"},inplace=True)\n",
        "\n",
        "result = pd.merge(fastaid, seqid, how='inner', on=['peakid', 'peakid'])\n",
        "\n",
        "result[[\"fasta\"]] = result[[\"fasta\"]].apply(lambda x: x.astype(str).str.upper())\n",
        "\n",
        "result = result[[\"fasta\",\"silencer\"]]\n",
        "\n",
        "#label=\"https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/labels.txt\"\n",
        "#label = pd.read_table(labelid,header=None)\n",
        "input = result[[\"fasta\"]]\n",
        "print(input.shape)\n",
        "label = result[[\"silencer\"]]\n",
        "\n",
        "# Convert input to one-hot code\n",
        "#def one_hot_encoding(seq):\n",
        "#    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
        "#    one_hot = [mapping[nuc] for nuc in seq]\n",
        "#    return np.array(one_hot)\n",
        "\n",
        "#one_hot_input = np.array([one_hot_encoding(seq) for seq in input[0]])\n",
        "#\n",
        "#one_hot_input = torch.from_numpy(one_hot_input)\n",
        "#one_hot_input = one_hot_input.permute(0, 2, 1)\n",
        "#print(one_hot_input.shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "# Split the data into training, testing, and prediction sets\n",
        "#x_train, x_test, y_train, y_test = train_test_split(one_hot_input, label, test_size=0.3, random_state=42)\n",
        "#x_test, x_pred, y_test, y_pred = train_test_split(x_test, y_test, test_size=0.67, random_state=42)\n",
        "#print(x_train.shape)\n",
        "#print(x_test.shape)\n",
        "\n",
        "\n",
        "DNA = np.zeros(shape=(len(input),len(input[\"fasta\"][0]),4))\n",
        "labelid = np.zeros(shape=(len(input),))\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "for i in range(input.shape[0]):\n",
        "    seq_array = array(list(input[\"fasta\"][i]))\n",
        "    #integer encode the sequence\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    #one hot the sequence\n",
        "    onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    #reshape because that's what OneHotEncoder likes\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "    #print(integer_encoded_seq.shape)\n",
        "    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "    #print(onehot_encoded_seq.shape)\n",
        "    #print(len(onehot_encoded_seq))\n",
        "    DNA[i] = onehot_encoded_seq\n",
        "    #DNA[i] = onehot_encoded_seq.reshape(4,len(onehot_encoded_seq))\n",
        "    labelid[i] = label[\"silencer\"][i]\n",
        "\n",
        "#print(DNA.shape)\n",
        "#print(labelid.shape)\n",
        "\n",
        "DNA = torch.tensor(DNA)\n",
        "DNA = DNA.permute(0, 2, 1)\n",
        "labelid =  torch.tensor(labelid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    DNA = DNA.to(\"cuda:0\")\n",
        "    labelid =  labelid.to(\"cuda:0\")\n",
        "#print(DNA.is_cuda)\n",
        "\n",
        "#print(np.shape(DNA))\n",
        "#embed_x = embed_x.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "hcsO9VnIZ5Sa",
        "outputId": "4a2c9f50-272c-4bc6-944a-23a6872f8ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://raw.githubusercontent.com/BenxiaHu/DeepLearning/main/K562_MPRA_Silencers.txt\n",
            "        chr      start        end  silencer                     peakid\n",
            "0      chr1  113408175  113408375         1   chr1:113408175-113408375\n",
            "1     chr10    6107715    6107915         1      chr10:6107715-6107915\n",
            "2     chr10   15538675   15538875         1    chr10:15538675-15538875\n",
            "3     chr11  114339756  114339956         1  chr11:114339756-114339956\n",
            "4     chr17   41121376   41121576         1    chr17:41121376-41121576\n",
            "...     ...        ...        ...       ...                        ...\n",
            "7227   chrX   93929096   93929296         0     chrX:93929096-93929296\n",
            "7228   chrX  101946395  101946595         0   chrX:101946395-101946595\n",
            "7229   chrX  130859105  130859305         0   chrX:130859105-130859305\n",
            "7230   chrX  131424075  131424275         0   chrX:131424075-131424275\n",
            "7231   chrX  141256436  141256636         0   chrX:141256436-141256636\n",
            "\n",
            "[7232 rows x 5 columns]\n",
            "                         peakid  silencer\n",
            "0      chr1:113408175-113408375         1\n",
            "1         chr10:6107715-6107915         1\n",
            "2       chr10:15538675-15538875         1\n",
            "3     chr11:114339756-114339956         1\n",
            "4       chr17:41121376-41121576         1\n",
            "...                         ...       ...\n",
            "7227     chrX:93929096-93929296         0\n",
            "7228   chrX:101946395-101946595         0\n",
            "7229   chrX:130859105-130859305         0\n",
            "7230   chrX:131424075-131424275         0\n",
            "7231   chrX:141256436-141256636         0\n",
            "\n",
            "[7232 rows x 2 columns]\n",
            "                              0  \\\n",
            "0      chr1:113408175-113408375   \n",
            "1         chr10:6107715-6107915   \n",
            "2       chr10:15538675-15538875   \n",
            "3     chr11:114339756-114339956   \n",
            "4       chr17:41121376-41121576   \n",
            "...                         ...   \n",
            "7227     chrX:93929096-93929296   \n",
            "7228   chrX:101946395-101946595   \n",
            "7229   chrX:130859105-130859305   \n",
            "7230   chrX:131424075-131424275   \n",
            "7231   chrX:141256436-141256636   \n",
            "\n",
            "                                                      1  \n",
            "0     ttacaggcatgagccaccgcactcggccTTGTTAAACTTTTTTGTT...  \n",
            "1     AATATAACTAGCATATACTAAAGCAGTTTCAGACAAATTCTTTGTG...  \n",
            "2     tgagtagctgggataacaggtgtgcagcactataacaggaaatttt...  \n",
            "3     ctaaatagtataaccctgaaatatataaaacaaaatctgttacaat...  \n",
            "4     GTAAACCAACACTACTTTGTCATACATCATTAGTTTCAGGACtttt...  \n",
            "...                                                 ...  \n",
            "7227  aaggggaagaggaagcaaggcacatcttacagtatggcaggagaga...  \n",
            "7228  tgcccccagaggtggagtctacagaggcaggcaggcctccttgagc...  \n",
            "7229  tctttgttccccattttaaagtttaacttcctcattctcctcgtct...  \n",
            "7230  gtttggctatgccctgcccccagaggtggaggctacagaggcaggc...  \n",
            "7231  TTGAAATGTCACTATCCAAAGTGAATAGTCACATCGAAGGGCTGTT...  \n",
            "\n",
            "[7232 rows x 2 columns]\n",
            "(7232, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step2: split the data into training, test and prediction data"
      ],
      "metadata": {
        "id": "mda3oRSQiziW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_tensor = DNA\n",
        "#label_tensor = labelid\n",
        "# pick 1400 samples as training data\n",
        "#print(input_tensor[0:1400,:,:].shape)\n",
        "#print(label_tensor[0:1400].shape)\n",
        "\n",
        "# Split the data into training, test, and prediction sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(DNA, labelid, test_size=0.3, random_state=42)\n",
        "\n",
        "#print(x_train.shape)\n",
        "# Hyper Parameters\n",
        "EPOCH = 10              # train the training data n times, to save time, we just train 1 epoch\n",
        "BATCH_SIZE = 10\n",
        "LR = 0.1                 # learning rate\n",
        "torch_dataset = Data.TensorDataset(x_train, y_train)\n",
        "train_loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "torch_dataset2 = Data.TensorDataset(x_test, y_test)\n",
        "test_loader = Data.DataLoader(dataset=torch_dataset2, shuffle=True)\n",
        "              \n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (100, 1, 50, 4)\n",
        "\n",
        "#for i, j in enumerate(train_loader):\n",
        "    #x, y = j\n",
        "    #print('batch:{0} x:{1}  y: {2}'.format(i, x, y))\n",
        "    #print(i)\n",
        "    #print(x.shape)\n",
        "    #print(y.shape)\n",
        "\n",
        "# pick 400 samples as prediction data\n",
        "#test_x2 = Variable(input_tensor[1600:2000,:,:]).type(torch.FloatTensor)\n",
        "#test_y2 = label_tensor[1600:2000]\n"
      ],
      "metadata": {
        "id": "lIn4WnALi6Bc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step4: build the 1D-CNN model"
      ],
      "metadata": {
        "id": "2ipkZI_vvddy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         # input shape (50,4,200)\n",
        "            nn.Conv1d(\n",
        "                in_channels=4,              # input height\n",
        "                out_channels=1,            # n_filters\n",
        "                kernel_size=21,             # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=0                   # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
        "            ),                              # output shape (32, 1, 190)\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                  # choose max value in 1x2 area, output shape (32, 1, 178)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         # input shape (32, 1, 178)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,             # input height\n",
        "                out_channels=1,            # n_filters\n",
        "                kernel_size=21,             # filter size\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=0                   # output shape (32, 1, 158)\n",
        "            ),\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (32, 1, 156)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(       # input shape (32, 1, 156)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,           # input height\n",
        "                out_channels=1,          # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (16, 1, 136)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (16, 1, 134)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(       # input shape (16, 1, 134)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (16, 1, 114)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (16, 1, 112)\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(       # input shape (16, 1, 112)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (16, 1, 92)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (16, 1, 90)\n",
        "        )\n",
        "        self.conv6 = nn.Sequential(       # input shape (16, 1, 90)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (16, 1, 70)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (8, 1, 68)\n",
        "        )\n",
        "        self.conv7 = nn.Sequential(       # input shape (8, 1, 68)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (8, 1, 48)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (8, 1, 46)\n",
        "        )\n",
        "        self.conv8 = nn.Sequential(       # input shape (8, 1, 46)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (8, 1, 26)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (8, 1, 24)\n",
        "        )\n",
        "        self.conv9 = nn.Sequential(       # input shape (8, 1, 24)\n",
        "            nn.Conv1d(\n",
        "                in_channels=1,            # input height\n",
        "                out_channels=1,           # n_filters\n",
        "                kernel_size=21,           # filter size\n",
        "                stride=1,                 # filter movement/step\n",
        "                padding=0                 # output shape (8, 1, 30)\n",
        "            ),\n",
        "            nn.ReLU(),                    # activation\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0)                # output shape (4, 1, 20)\n",
        "        )\n",
        "        #self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(1*24, 10)     # fully connected layer\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 2)       # fully connected layer\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #print(x.size())            # torch.Size([50, 32, 38])\n",
        "        x = self.conv2(x)\n",
        "        #print(x.size())            # torch.Size([50, 16, 26])\n",
        "        x = self.conv3(x)\n",
        "        #print(x.size())            # torch.Size([50, 8, 14])\n",
        "        x = self.conv4(x)\n",
        "        #print(x.size())            # torch.Size([50, 4, 2])\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        #print(x.size())\n",
        "        #x = self.conv9(x)\n",
        "        x = torch.flatten(x,1)  # flatten the output to 2 dimensions from 3 dimensions\n",
        "        x = self.fc1(x)\n",
        "        #print(x.size())            # torch.Size([50, 10])\n",
        "        x = self.relu(x)\n",
        "        #print(x.size())            # torch.Size([50, 10])\n",
        "        x = self.dropout(x) \n",
        "        #x = x.view(x.size(0), -1)   # flatten the output 1 dimension from 2 dimensions\n",
        "        x = self.fc2(x)\n",
        "        #print(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        out = self.softmax(x)\n",
        "        return out              # return x for visualization\n",
        "\n",
        "\n",
        "cnn = CNN()\n",
        "#print(cnn)  # net architecture\n",
        "\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
      ],
      "metadata": {
        "id": "clAfv5DQvc5c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# running the CNN model"
      ],
      "metadata": {
        "id": "avagtSaveVqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing\n",
        "optimizer.zero_grad()           # clear gradients for this training step\n",
        "train_losses = []\n",
        "trainCorrect = 0\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    for i, (b_x, b_y) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
        "        #print(b_x.shape)\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        b_y = b_y.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        output = cnn(b_x)               # cnn output\n",
        "        preds, predsid = torch.max(output,1) \n",
        "        loss = loss_func(preds, b_y)   # cross entropy loss\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        train_losses.append(loss.item())\n",
        "        trainCorrect += (predsid == b_y).type(torch.float).sum().item()\n",
        "\n",
        "        if i % 10 == 0:    # print every 100 mini-batches\n",
        "            #print(i)\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
        "# calculate the average training and validation loss\n",
        "#avgTrainLoss = sum(train_losses) / trainSteps\n",
        "# calculate the training and validation accuracy\n",
        "#trainCorrect = trainCorrect / trainSteps\n",
        "\n",
        "# Letâ€™s quickly save our trained model:\n",
        "# Plot the training and test losses over time\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"# of epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "!pwd\n",
        "torch.save(cnn.state_dict(), \"./content\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zckte_qdea_1",
        "outputId": "c8972263-dbab-4e7f-fb99-582e20a25460"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 4.605170249938965\n",
            "[1,    11] loss: 89.80082273483276\n",
            "[1,    21] loss: 105.91892004013062\n",
            "[1,    31] loss: 94.40599250793457\n",
            "[1,    41] loss: 115.12926149368286\n",
            "[1,    51] loss: 94.40599489212036\n",
            "[1,    61] loss: 101.31374883651733\n",
            "[1,    71] loss: 94.40599203109741\n",
            "[1,    81] loss: 87.49823665618896\n",
            "[1,    91] loss: 101.31374883651733\n",
            "[1,   101] loss: 89.80082368850708\n",
            "[1,   111] loss: 85.19565153121948\n",
            "[1,   121] loss: 94.40599298477173\n",
            "[1,   131] loss: 80.59048175811768\n",
            "[1,   141] loss: 96.70857858657837\n",
            "[1,   151] loss: 103.61633539199829\n",
            "[1,   161] loss: 96.70857954025269\n",
            "[1,   171] loss: 96.70857810974121\n",
            "[1,   181] loss: 75.98531103134155\n",
            "[1,   191] loss: 99.01116514205933\n",
            "[1,   201] loss: 108.22150564193726\n",
            "[1,   211] loss: 105.91892051696777\n",
            "[1,   221] loss: 71.38014030456543\n",
            "[1,   231] loss: 85.19565153121948\n",
            "[1,   241] loss: 92.1034083366394\n",
            "[1,   251] loss: 99.01116466522217\n",
            "[1,   261] loss: 110.52409029006958\n",
            "[1,   271] loss: 94.40599346160889\n",
            "[1,   281] loss: 82.89306592941284\n",
            "[1,   291] loss: 96.70857858657837\n",
            "[1,   301] loss: 78.28789567947388\n",
            "[1,   311] loss: 92.10340785980225\n",
            "[1,   321] loss: 99.01116371154785\n",
            "[1,   331] loss: 82.89306640625\n",
            "[1,   341] loss: 101.31374979019165\n",
            "[1,   351] loss: 108.22150659561157\n",
            "[1,   361] loss: 82.89306735992432\n",
            "[1,   371] loss: 110.5240912437439\n",
            "[1,   381] loss: 80.59048175811768\n",
            "[1,   391] loss: 101.31374979019165\n",
            "[1,   401] loss: 96.70857858657837\n",
            "[1,   411] loss: 96.70857858657837\n",
            "[1,   421] loss: 85.1956524848938\n",
            "[1,   431] loss: 78.2878966331482\n",
            "[1,   441] loss: 94.40599250793457\n",
            "[1,   451] loss: 101.31375074386597\n",
            "[1,   461] loss: 115.12926197052002\n",
            "[1,   471] loss: 94.40599346160889\n",
            "[1,   481] loss: 87.49823713302612\n",
            "[1,   491] loss: 110.5240912437439\n",
            "[1,   501] loss: 87.49823808670044\n",
            "[2,     1] loss: 6.907755374908447\n",
            "[2,    11] loss: 105.91892004013062\n",
            "[2,    21] loss: 82.89306640625\n",
            "[2,    31] loss: 96.70857810974121\n",
            "[2,    41] loss: 99.01116371154785\n",
            "[2,    51] loss: 99.01116371154785\n",
            "[2,    61] loss: 87.49823713302612\n",
            "[2,    71] loss: 112.82667636871338\n",
            "[2,    81] loss: 110.52409029006958\n",
            "[2,    91] loss: 108.22150707244873\n",
            "[2,   101] loss: 96.70857763290405\n",
            "[2,   111] loss: 96.70857810974121\n",
            "[2,   121] loss: 99.01116275787354\n",
            "[2,   131] loss: 99.01116418838501\n",
            "[2,   141] loss: 89.80082321166992\n",
            "[2,   151] loss: 85.19565057754517\n",
            "[2,   161] loss: 82.89306592941284\n",
            "[2,   171] loss: 87.49823760986328\n",
            "[2,   181] loss: 73.6827244758606\n",
            "[2,   191] loss: 94.40599250793457\n",
            "[2,   201] loss: 101.31375026702881\n",
            "[2,   211] loss: 94.40599203109741\n",
            "[2,   221] loss: 92.10340785980225\n",
            "[2,   231] loss: 73.68272590637207\n",
            "[2,   241] loss: 71.38014030456543\n",
            "[2,   251] loss: 119.7344331741333\n",
            "[2,   261] loss: 96.70857810974121\n",
            "[2,   271] loss: 85.19565200805664\n",
            "[2,   281] loss: 108.22150611877441\n",
            "[2,   291] loss: 78.28789567947388\n",
            "[2,   301] loss: 99.0111632347107\n",
            "[2,   311] loss: 117.43184661865234\n",
            "[2,   321] loss: 85.19565200805664\n",
            "[2,   331] loss: 75.98531103134155\n",
            "[2,   341] loss: 99.01116514205933\n",
            "[2,   351] loss: 110.52409172058105\n",
            "[2,   361] loss: 80.59048128128052\n",
            "[2,   371] loss: 85.19565200805664\n",
            "[2,   381] loss: 96.70857810974121\n",
            "[2,   391] loss: 101.31374931335449\n",
            "[2,   401] loss: 108.22150611877441\n",
            "[2,   411] loss: 103.6163330078125\n",
            "[2,   421] loss: 96.70857858657837\n",
            "[2,   431] loss: 105.91892004013062\n",
            "[2,   441] loss: 94.40599393844604\n",
            "[2,   451] loss: 82.89306497573853\n",
            "[2,   461] loss: 112.82667684555054\n",
            "[2,   471] loss: 73.68272495269775\n",
            "[2,   481] loss: 82.89306592941284\n",
            "[2,   491] loss: 108.22150659561157\n",
            "[2,   501] loss: 87.49823665618896\n",
            "[3,     1] loss: 11.51292610168457\n",
            "[3,    11] loss: 94.40599203109741\n",
            "[3,    21] loss: 75.98531007766724\n",
            "[3,    31] loss: 96.70857810974121\n",
            "[3,    41] loss: 82.89306592941284\n",
            "[3,    51] loss: 96.70857906341553\n",
            "[3,    61] loss: 85.19565057754517\n",
            "[3,    71] loss: 89.80082321166992\n",
            "[3,    81] loss: 92.10340642929077\n",
            "[3,    91] loss: 101.31374883651733\n",
            "[3,   101] loss: 94.40599393844604\n",
            "[3,   111] loss: 80.59048175811768\n",
            "[3,   121] loss: 82.89306640625\n",
            "[3,   131] loss: 85.1956524848938\n",
            "[3,   141] loss: 103.61633586883545\n",
            "[3,   151] loss: 101.31374979019165\n",
            "[3,   161] loss: 78.28789567947388\n",
            "[3,   171] loss: 103.61633586883545\n",
            "[3,   181] loss: 101.31374931335449\n",
            "[3,   191] loss: 92.10340785980225\n",
            "[3,   201] loss: 110.52409076690674\n",
            "[3,   211] loss: 112.82667541503906\n",
            "[3,   221] loss: 75.98530960083008\n",
            "[3,   231] loss: 99.01116418838501\n",
            "[3,   241] loss: 92.10340785980225\n",
            "[3,   251] loss: 99.01116371154785\n",
            "[3,   261] loss: 80.59048128128052\n",
            "[3,   271] loss: 71.38013935089111\n",
            "[3,   281] loss: 101.31374883651733\n",
            "[3,   291] loss: 112.82667589187622\n",
            "[3,   301] loss: 89.80082273483276\n",
            "[3,   311] loss: 78.28789472579956\n",
            "[3,   321] loss: 126.64218997955322\n",
            "[3,   331] loss: 89.80082273483276\n",
            "[3,   341] loss: 96.70858001708984\n",
            "[3,   351] loss: 66.77496910095215\n",
            "[3,   361] loss: 96.70857858657837\n",
            "[3,   371] loss: 82.89306592941284\n",
            "[3,   381] loss: 92.10340881347656\n",
            "[3,   391] loss: 94.40599346160889\n",
            "[3,   401] loss: 82.89306592941284\n",
            "[3,   411] loss: 96.70857954025269\n",
            "[3,   421] loss: 108.22150611877441\n",
            "[3,   431] loss: 105.91891956329346\n",
            "[3,   441] loss: 108.2215051651001\n",
            "[3,   451] loss: 94.40599250793457\n",
            "[3,   461] loss: 108.22150468826294\n",
            "[3,   471] loss: 101.31374931335449\n",
            "[3,   481] loss: 115.12926197052002\n",
            "[3,   491] loss: 112.82667589187622\n",
            "[3,   501] loss: 94.40599298477173\n",
            "[4,     1] loss: 18.420682907104492\n",
            "[4,    11] loss: 94.40599203109741\n",
            "[4,    21] loss: 87.49823760986328\n",
            "[4,    31] loss: 73.68272399902344\n",
            "[4,    41] loss: 94.40599298477173\n",
            "[4,    51] loss: 99.01116466522217\n",
            "[4,    61] loss: 96.70857906341553\n",
            "[4,    71] loss: 99.01116371154785\n",
            "[4,    81] loss: 94.40599346160889\n",
            "[4,    91] loss: 105.91891860961914\n",
            "[4,   101] loss: 92.10340881347656\n",
            "[4,   111] loss: 78.28789520263672\n",
            "[4,   121] loss: 92.1034083366394\n",
            "[4,   131] loss: 87.49823760986328\n",
            "[4,   141] loss: 99.01116371154785\n",
            "[4,   151] loss: 89.80082178115845\n",
            "[4,   161] loss: 78.28789472579956\n",
            "[4,   171] loss: 96.70857858657837\n",
            "[4,   181] loss: 105.91891956329346\n",
            "[4,   191] loss: 92.1034083366394\n",
            "[4,   201] loss: 92.10340785980225\n",
            "[4,   211] loss: 75.98531007766724\n",
            "[4,   221] loss: 103.61633539199829\n",
            "[4,   231] loss: 105.91892004013062\n",
            "[4,   241] loss: 92.10340785980225\n",
            "[4,   251] loss: 87.49823760986328\n",
            "[4,   261] loss: 94.40599250793457\n",
            "[4,   271] loss: 75.98530960083008\n",
            "[4,   281] loss: 94.40599346160889\n",
            "[4,   291] loss: 101.31374835968018\n",
            "[4,   301] loss: 115.12926197052002\n",
            "[4,   311] loss: 96.70857858657837\n",
            "[4,   321] loss: 103.61633491516113\n",
            "[4,   331] loss: 85.1956524848938\n",
            "[4,   341] loss: 117.4318470954895\n",
            "[4,   351] loss: 71.38014078140259\n",
            "[4,   361] loss: 89.80082321166992\n",
            "[4,   371] loss: 94.40599298477173\n",
            "[4,   381] loss: 92.10340738296509\n",
            "[4,   391] loss: 82.89306640625\n",
            "[4,   401] loss: 108.22150611877441\n",
            "[4,   411] loss: 101.31374835968018\n",
            "[4,   421] loss: 103.61633443832397\n",
            "[4,   431] loss: 112.82667636871338\n",
            "[4,   441] loss: 78.28789567947388\n",
            "[4,   451] loss: 110.52409076690674\n",
            "[4,   461] loss: 105.91892099380493\n",
            "[4,   471] loss: 94.40599203109741\n",
            "[4,   481] loss: 89.8008222579956\n",
            "[4,   491] loss: 110.5240912437439\n",
            "[4,   501] loss: 75.98530960083008\n",
            "[5,     1] loss: 13.815511703491211\n",
            "[5,    11] loss: 103.61633491516113\n",
            "[5,    21] loss: 101.31374835968018\n",
            "[5,    31] loss: 78.28789472579956\n",
            "[5,    41] loss: 108.2215051651001\n",
            "[5,    51] loss: 85.19565105438232\n",
            "[5,    61] loss: 89.80082321166992\n",
            "[5,    71] loss: 85.19565200805664\n",
            "[5,    81] loss: 108.2215051651001\n",
            "[5,    91] loss: 103.61633396148682\n",
            "[5,   101] loss: 87.49823808670044\n",
            "[5,   111] loss: 105.91892099380493\n",
            "[5,   121] loss: 115.12926149368286\n",
            "[5,   131] loss: 85.19565200805664\n",
            "[5,   141] loss: 85.1956524848938\n",
            "[5,   151] loss: 89.80082130432129\n",
            "[5,   161] loss: 105.91892051696777\n",
            "[5,   171] loss: 78.28789472579956\n",
            "[5,   181] loss: 82.89306640625\n",
            "[5,   191] loss: 99.0111632347107\n",
            "[5,   201] loss: 103.61633539199829\n",
            "[5,   211] loss: 85.19565153121948\n",
            "[5,   221] loss: 85.19565057754517\n",
            "[5,   231] loss: 103.61633682250977\n",
            "[5,   241] loss: 101.31374931335449\n",
            "[5,   251] loss: 78.28789520263672\n",
            "[5,   261] loss: 87.49823760986328\n",
            "[5,   271] loss: 105.91892099380493\n",
            "[5,   281] loss: 92.10340785980225\n",
            "[5,   291] loss: 105.9189190864563\n",
            "[5,   301] loss: 89.80082273483276\n",
            "[5,   311] loss: 94.40599298477173\n",
            "[5,   321] loss: 89.80082273483276\n",
            "[5,   331] loss: 96.70857858657837\n",
            "[5,   341] loss: 99.01116371154785\n",
            "[5,   351] loss: 89.80082321166992\n",
            "[5,   361] loss: 89.80082273483276\n",
            "[5,   371] loss: 82.89306640625\n",
            "[5,   381] loss: 110.52409029006958\n",
            "[5,   391] loss: 126.64218950271606\n",
            "[5,   401] loss: 92.1034083366394\n",
            "[5,   411] loss: 92.10340690612793\n",
            "[5,   421] loss: 96.70857858657837\n",
            "[5,   431] loss: 82.89306831359863\n",
            "[5,   441] loss: 89.80082082748413\n",
            "[5,   451] loss: 80.59048175811768\n",
            "[5,   461] loss: 89.8008222579956\n",
            "[5,   471] loss: 101.31374979019165\n",
            "[5,   481] loss: 103.61633491516113\n",
            "[5,   491] loss: 101.31374835968018\n",
            "[5,   501] loss: 96.70857906341553\n",
            "[6,     1] loss: 6.907755374908447\n",
            "[6,    11] loss: 101.31374883651733\n",
            "[6,    21] loss: 78.28789567947388\n",
            "[6,    31] loss: 108.22150468826294\n",
            "[6,    41] loss: 124.33960437774658\n",
            "[6,    51] loss: 96.70857954025269\n",
            "[6,    61] loss: 85.19565153121948\n",
            "[6,    71] loss: 99.01116371154785\n",
            "[6,    81] loss: 80.59048128128052\n",
            "[6,    91] loss: 87.4982361793518\n",
            "[6,   101] loss: 87.49823665618896\n",
            "[6,   111] loss: 105.91892004013062\n",
            "[6,   121] loss: 85.19565153121948\n",
            "[6,   131] loss: 101.31374883651733\n",
            "[6,   141] loss: 103.61633348464966\n",
            "[6,   151] loss: 96.7085771560669\n",
            "[6,   161] loss: 82.89306592941284\n",
            "[6,   171] loss: 117.43184757232666\n",
            "[6,   181] loss: 99.01116466522217\n",
            "[6,   191] loss: 94.40599298477173\n",
            "[6,   201] loss: 96.70857858657837\n",
            "[6,   211] loss: 96.70857858657837\n",
            "[6,   221] loss: 94.40599250793457\n",
            "[6,   231] loss: 92.10340642929077\n",
            "[6,   241] loss: 105.91892051696777\n",
            "[6,   251] loss: 89.80082130432129\n",
            "[6,   261] loss: 101.31374931335449\n",
            "[6,   271] loss: 87.49823665618896\n",
            "[6,   281] loss: 78.28789567947388\n",
            "[6,   291] loss: 94.40599298477173\n",
            "[6,   301] loss: 89.80082273483276\n",
            "[6,   311] loss: 96.70857954025269\n",
            "[6,   321] loss: 89.80082321166992\n",
            "[6,   331] loss: 101.31374883651733\n",
            "[6,   341] loss: 80.59048080444336\n",
            "[6,   351] loss: 87.49823808670044\n",
            "[6,   361] loss: 85.19565153121948\n",
            "[6,   371] loss: 115.12926197052002\n",
            "[6,   381] loss: 99.01116466522217\n",
            "[6,   391] loss: 101.31375074386597\n",
            "[6,   401] loss: 92.10340785980225\n",
            "[6,   411] loss: 85.1956524848938\n",
            "[6,   421] loss: 101.31375122070312\n",
            "[6,   431] loss: 96.70857763290405\n",
            "[6,   441] loss: 87.49823665618896\n",
            "[6,   451] loss: 92.10340785980225\n",
            "[6,   461] loss: 89.80082273483276\n",
            "[6,   471] loss: 71.38013887405396\n",
            "[6,   481] loss: 99.01116418838501\n",
            "[6,   491] loss: 108.22150611877441\n",
            "[6,   501] loss: 103.61633539199829\n",
            "[7,     1] loss: 9.21034049987793\n",
            "[7,    11] loss: 89.80082273483276\n",
            "[7,    21] loss: 101.31374835968018\n",
            "[7,    31] loss: 87.4982361793518\n",
            "[7,    41] loss: 80.5904803276062\n",
            "[7,    51] loss: 115.1292610168457\n",
            "[7,    61] loss: 85.19565200805664\n",
            "[7,    71] loss: 89.80082273483276\n",
            "[7,    81] loss: 94.40599346160889\n",
            "[7,    91] loss: 71.38014078140259\n",
            "[7,   101] loss: 89.80082368850708\n",
            "[7,   111] loss: 128.9447751045227\n",
            "[7,   121] loss: 99.01116418838501\n",
            "[7,   131] loss: 85.19565105438232\n",
            "[7,   141] loss: 89.8008222579956\n",
            "[7,   151] loss: 105.91892051696777\n",
            "[7,   161] loss: 105.91891956329346\n",
            "[7,   171] loss: 75.98530960083008\n",
            "[7,   181] loss: 101.31374835968018\n",
            "[7,   191] loss: 69.07755517959595\n",
            "[7,   201] loss: 99.01116371154785\n",
            "[7,   211] loss: 99.01116371154785\n",
            "[7,   221] loss: 85.19565105438232\n",
            "[7,   231] loss: 73.68272542953491\n",
            "[7,   241] loss: 103.61633491516113\n",
            "[7,   251] loss: 85.19565153121948\n",
            "[7,   261] loss: 117.4318470954895\n",
            "[7,   271] loss: 101.31374979019165\n",
            "[7,   281] loss: 110.5240912437439\n",
            "[7,   291] loss: 99.0111632347107\n",
            "[7,   301] loss: 119.73443269729614\n",
            "[7,   311] loss: 122.0370192527771\n",
            "[7,   321] loss: 108.22150564193726\n",
            "[7,   331] loss: 78.28789520263672\n",
            "[7,   341] loss: 103.61633491516113\n",
            "[7,   351] loss: 87.49823570251465\n",
            "[7,   361] loss: 78.28789472579956\n",
            "[7,   371] loss: 117.43184804916382\n",
            "[7,   381] loss: 87.49823665618896\n",
            "[7,   391] loss: 80.59048080444336\n",
            "[7,   401] loss: 115.12926149368286\n",
            "[7,   411] loss: 82.89306545257568\n",
            "[7,   421] loss: 101.31374883651733\n",
            "[7,   431] loss: 75.98531007766724\n",
            "[7,   441] loss: 80.59048128128052\n",
            "[7,   451] loss: 103.61633443832397\n",
            "[7,   461] loss: 103.61633443832397\n",
            "[7,   471] loss: 62.169798374176025\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b4b883132ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# clear gradients for this training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# cnn output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8f97fa22d5e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;31m#print(x.size())            # torch.Size([50, 32, 38])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         return F.max_pool1d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m     93\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# apply the model to the test data"
      ],
      "metadata": {
        "id": "UVSipUZwos-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "net.load_state_dict(torch.load(\"./content\"))\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for i, (b_x, b_y) in enumerate(test_loader):\n",
        "        b_x = Variable(b_x).type(torch.FloatTensor)\n",
        "        #print(b_x.shape)\n",
        "        #b_y = b_y.type(torch.LongTensor)\n",
        "        # calculate outputs by running the network\n",
        "        outputs = net(b_x)\n",
        "        #print(outputs[0].shape)\n",
        "        #print(outputs[1].shape)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs[0], 1)\n",
        "        total += b_y.size(0)\n",
        "        correct += (predicted == b_y).sum().item()           \n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
      ],
      "metadata": {
        "id": "XjjcoqJLoqDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}